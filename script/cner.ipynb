{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /scratch/local/jieba.cache\n",
      "Loading model cost 0.650 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import pickle\n",
    "import math\n",
    "import jieba\n",
    "jieba.initialize()\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.crf import crf_log_likelihood\n",
    "from tensorflow.contrib.crf import viterbi_decode\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import concat, placeholder, reduce_sum, Variable, expand_dims, reduce_mean\n",
    "from tensorflow import abs, get_variable, variable_scope, sign, reshape, cast, squeeze, shape\n",
    "from tensorflow.nn import embedding_lookup as embed\n",
    "from tensorflow.nn import dropout, atrous_conv2d, conv2d, bias_add, relu, xw_plus_b\n",
    "from collections import defaultdict, namedtuple\n",
    "from keras.models import Model as Model_init\n",
    "from keras.layers import  LSTM, Bidirectional, Input, Embedding, Concatenate, Dropout\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss as loss\n",
    "from keras_contrib.metrics import crf_accuracy as accuracy\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Choose a Model!\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "print('Please Choose a Model!')\n",
    "# The tensorflow will fill up the GPU memery during a training process of one model, and it will not release the memery after training.\n",
    "# After executing the pipeline for one model. Users need to select 'Kernel'-----'Restart and Clean Output' to release the memery manually.\n",
    "# Then, users can test the pipeline for another model\n",
    "restart_flag =False # a flag to display whether the kernel should be manually 'restart and clean output' for switching between two models\n",
    "\n",
    "# If any of these two commands is changed, Please restart the kernel!!! ( Menu ---- Kernel ---- Restart&Run All)\n",
    "# selected_model = 'Bi-lstm' # uncomment this line, the designed BI-lstm model will be loaded\n",
    "\n",
    "selected_model = False # uncommentting this line will load the Iterated Dilated Convolutions model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Raw Data preprocessing\n",
    "\n",
    "    1) Load data from file. \n",
    "    2) Convert IOB tagging into IOBES tagging. \n",
    "    3) Split data into training data, testing data and evaluation data.\n",
    "    4) Creating item-to-sequence and sequence-to-item dictionaries.\n",
    "    5) Convert chinese characters and tags into sequence.\n",
    "    6) Divide data into batches with fixed length and padding samples with 0 to maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['因', 'O'], ['此', 'O'], ['，', 'O'], ['这', 'O'], ['次', 'O'], ['政', 'O'], ['府', 'O'], ['危', 'O'], ['机', 'O'], ['终', 'O'], ['于', 'O'], ['得', 'O'], ['到', 'O'], ['化', 'O'], ['解', 'O'], ['，', 'O'], ['对', 'O'], ['俄', 'B-LOC'], ['罗', 'I-LOC'], ['斯', 'I-LOC'], ['来', 'O'], ['说', 'O'], ['是', 'O'], ['值', 'O'], ['得', 'O'], ['庆', 'O'], ['幸', 'O'], ['的', 'O'], ['。', 'O']]\n",
      "[['因', 'O'], ['此', 'O'], ['，', 'O'], ['这', 'O'], ['次', 'O'], ['政', 'O'], ['府', 'O'], ['危', 'O'], ['机', 'O'], ['终', 'O'], ['于', 'O'], ['得', 'O'], ['到', 'O'], ['化', 'O'], ['解', 'O'], ['，', 'O'], ['对', 'O'], ['俄', 'B-LOC'], ['罗', 'I-LOC'], ['斯', 'E-LOC'], ['来', 'O'], ['说', 'O'], ['是', 'O'], ['值', 'O'], ['得', 'O'], ['庆', 'O'], ['幸', 'O'], ['的', 'O'], ['。', 'O']]\n",
      "The number of sentences of trainning data is 19472\n",
      "The number of sentences of testing data is 5007\n",
      "The number of sentences of development data is 3339\n",
      "The number of unique Chinese characters is: 4277\n",
      "The number of unique tag characters is: 13\n",
      "[['因', '此', '，', '这', '次', '政', '府', '危', '机', '终', '于', '得', '到', '化', '解', '，', '对', '俄', '罗', '斯', '来', '说', '是', '值', '得', '庆', '幸', '的', '。'], [209, 187, 2, 22, 133, 63, 247, 642, 117, 619, 57, 107, 42, 103, 229, 2, 38, 663, 415, 206, 43, 87, 11, 552, 107, 782, 1130, 3, 4], [1, 3, 0, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 0, 0, 1, 2, 3, 1, 3, 0, 1, 3, 1, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "The number of steps per epoch is 974\n"
     ]
    }
   ],
   "source": [
    "# read sentences from file\n",
    "def load_data(file_path):\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for line in codecs.open(file_path, 'r', 'utf8'):\n",
    "        line = line.rstrip() # Remove any white spaces at the end of the string\n",
    "        if not line:\n",
    "            if len(sent) > 0: # a line with \"\\n\" is used for spliting sentences\n",
    "                sentences.append(sent)\n",
    "                sent = []\n",
    "        else:\n",
    "            word_tag = line.split() # split word and tag\n",
    "            if len(word_tag) == 2:\n",
    "                sent.append(word_tag)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# convert IOB tags to IOBES tags\n",
    "def convert_to_iobes_tags(sentences):   \n",
    "    for index, sent in enumerate(sentences):\n",
    "        iob_tags = [word_tag[-1] for word_tag in sent] # obtain iob tags of a sentence\n",
    "        iobes_tags = [] # iobes tags\n",
    "        for i, tag in enumerate(iob_tags): \n",
    "            if tag == 'O': # O tag is unchanged\n",
    "                iobes_tags.append(tag)\n",
    "            elif tag.split('-')[0] == 'B':  # B to S if an entity only includes a single word\n",
    "                if i + 1 < len(iob_tags) and iob_tags[i + 1].split('-')[0] == 'I':\n",
    "                    iobes_tags.append(tag)\n",
    "                else:\n",
    "                    iobes_tags.append(tag.replace('B-', 'S-'))\n",
    "            elif tag.split('-')[0] == 'I':  # E is used for the last item of an entity (words > 2)\n",
    "                if i + 1 < len(iob_tags) and iob_tags[i + 1].split('-')[0] == 'I':\n",
    "                    iobes_tags.append(tag)\n",
    "                else:\n",
    "                    iobes_tags.append(tag.replace('I-', 'E-'))\n",
    "            else:\n",
    "                print('ERROR: INVALID IOB TAGGING!')  \n",
    "        for word, iobes_tag in zip(sent, iobes_tags): # replace tags\n",
    "            word[-1] = iobes_tag\n",
    "            \n",
    "            \n",
    "# split sentences into train, test, dev\n",
    "def split_data(sentences):\n",
    "    train_div = int(len(sentences) * 0.7);  # train set divide number\n",
    "    train_sentences = sentences[:train_div]\n",
    "    remaining_sentences = sentences[train_div:] \n",
    "    test_div = int(len(remaining_sentences)* 0.6)  # test set divide number\n",
    "    test_sentences = remaining_sentences[:test_div]\n",
    "    dev_sentences = remaining_sentences[test_div:]\n",
    "    return train_sentences, test_sentences, dev_sentences\n",
    "\n",
    "\n",
    "# creating dictionaries from unique chinese characters to unique id\n",
    "def create_char_id_convert_dict(sentences):\n",
    "    char_dict = {} # a dictionary of the frequency of unique chinese characters\n",
    "    chinese_chars = [[word[0] for word in sent] for sent in sentences] # get words from tupe word_tag\n",
    "    for chars in chinese_chars: # get frequency of unique chinese characters\n",
    "        for char in chars:\n",
    "            if char not in char_dict:\n",
    "                char_dict[char] = 1\n",
    "            else:\n",
    "                char_dict[char] += 1\n",
    "    char_dict[\"<PAD>\"] = 99999 # spacial word for padding, and intial a largest frequency\n",
    "    char_dict['<UNK>'] = 99998 # spacial word for unkonwn, and intial a second largest frequency\n",
    "    # sort characters by frequency (highest to samllest)\n",
    "    sorted_char_dict = sorted(char_dict.items(), key=lambda x: (-x[1], x[0])) \n",
    "    # create two dictionaries: find char by id, or find id by char\n",
    "    id_to_char = {index: value[0] for index, value in enumerate(sorted_char_dict)} \n",
    "    char_to_id = {value: key for key, value in id_to_char.items()}\n",
    "    return id_to_char, char_to_id\n",
    "\n",
    "\n",
    "# creating dictionaries from unique tag to unique id\n",
    "def create_tag_id_convert_dict(sentences):\n",
    "    tag_dict = {} # a dictionary of the frequency of tags\n",
    "    tags = [[word[1] for word in sent] for sent in sentences]\n",
    "    for tag in tags: # get frequency of unique chinese characters\n",
    "        for t in tag:\n",
    "            if t not in tag_dict:\n",
    "                tag_dict[t] = 1\n",
    "            else:\n",
    "                tag_dict[t] += 1\n",
    "    # sort characters by frequency (highest to samllest)\n",
    "    sorted_tag_dict = sorted(tag_dict.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # create two dictionaries: find tag by id, or find id by tag\n",
    "    id_to_tag = {index: value[0] for index, value in enumerate(sorted_tag_dict)} \n",
    "    tag_to_id = {value: key for key, value in id_to_tag.items()}\n",
    "    return id_to_tag, tag_to_id\n",
    "\n",
    "\n",
    "# Generated formated data for training\n",
    "def get_formated_data(sentences, char_to_id, tag_to_id):\n",
    "    formated_data = []\n",
    "    for sent in sentences:\n",
    "        sent_chars = [word[0] for word in sent] # get chinese chars\n",
    "        # convert chars to id\n",
    "        chars_id = [char_to_id[char if char in char_to_id else '<UNK>'] for char in sent_chars] \n",
    "        joined_sent = \"\".join(sent_chars) # joined all the chars into a sentence\n",
    "\n",
    "        # Tokenize sent with Jieba to get chinese phrase feature (the start, inside, and end of a phrase)\n",
    "        phrase_feature = []\n",
    "        for token in jieba.cut(joined_sent):\n",
    "            if len(token) == 1: # phrase_feature is 0 if a phase only has one Chinese character\n",
    "                phrase_feature.append(0)\n",
    "            else:\n",
    "                phrase_list = [2] * len(token) # phrase_feature of middle characters in a phase is 2\n",
    "                phrase_list[0] = 1 # phrase_feature of start character in a phase is 1\n",
    "                phrase_list[-1] = 3 # phrase_feature of end character in a phase is 3\n",
    "                phrase_feature.extend(phrase_list)\n",
    "\n",
    "        tags_id = [tag_to_id[word[-1]] for word in sent] # convert tags to id\n",
    "        formated_data.append([sent_chars, chars_id, phrase_feature, tags_id]) # formated data\n",
    "    return formated_data\n",
    "\n",
    "\n",
    "# Divide data into batches and padding each sample\n",
    "def generate_batch_data_with_padding(data, bcount, model=False, cut_length=510):\n",
    "    batches = []\n",
    "    GT = []\n",
    "    batch_count = int(math.ceil(len(data)/ bcount)) # calulate number of batches\n",
    "    # sorted list based on the length of sentences(short to long)\n",
    "    sorted_len_data = sorted(train_data, key=lambda x: len(x[0]))\n",
    "    for i in range(batch_count):\n",
    "        batch = sorted_len_data[(i * bcount) : ((i + 1) * bcount)] # divided data into batches with fixed length\n",
    "        pad_sentsents = [] # sentsents after padding\n",
    "        pad_chars = [] # chinese characters after padding\n",
    "        pad_phrases = [] # pahrase features after padding\n",
    "        pad_tags = [] # tags after padding\n",
    "        max_length = max([len(sample[0]) for sample in batch]) # find the max length of sentence in batch\n",
    "\n",
    "        for sample in batch:\n",
    "            sent, char, phrase, tag = sample \n",
    "            pad_array = [0] * (max_length - len(sent)) # padding with 0 based on the max length\n",
    "            pad_sentsents.append(sent + pad_array) \n",
    "            pad_chars.append(char + pad_array)\n",
    "            pad_phrases.append(phrase + pad_array)\n",
    "            pad_tags.append(tag + pad_array)\n",
    "            \n",
    "        batches.append([pad_sentsents, pad_chars, pad_phrases, pad_tags]) # get batch data\n",
    "    if model == False:\n",
    "        return batches\n",
    "    elif model == 'lstm':\n",
    "        fixed_max_length = cut_length #the length should be the same as the lstm model input dimention\n",
    "        pad_sentsents = [] # reinitialize, sentsents after padding\n",
    "        pad_chars = [] # reinitialize, chinese characters after padding\n",
    "        pad_phrases = [] # reinitialize, pahrase features after padding\n",
    "        pad_tags = [] # reinitialize, tags after padding\n",
    "        for sample in data:\n",
    "            sent, char, phrase, tag = sample \n",
    "            if len(sent) >= fixed_max_length:\n",
    "                pad_sentsents.append(sent[0:fixed_max_length])\n",
    "                pad_chars.append(char[0:fixed_max_length])\n",
    "                pad_phrases.append(phrase[0:fixed_max_length])\n",
    "                pad_tags.append(tag[0:fixed_max_length])\n",
    "            else:\n",
    "                pad_array = [0] * (fixed_max_length - len(sent))\n",
    "                pad_sentsents.append(sent + pad_array) \n",
    "                pad_chars.append(char + pad_array)\n",
    "                pad_phrases.append(phrase + pad_array)\n",
    "                pad_tags.append(tag + pad_array)\n",
    "            \n",
    "        pad_chars = np.array(pad_chars)\n",
    "        pad_phrases = np.array(pad_phrases)\n",
    "        pad_tags = np.array(pad_tags)\n",
    "        groundT = np.expand_dims(pad_tags, 2)\n",
    "        return [pad_chars,pad_phrases], groundT\n",
    "    else:\n",
    "        print('Please Enter the Correct Model')\n",
    "# data processing\n",
    "folder_patch = \"./dataset/\"  # dataset folder\n",
    "data_path = folder_patch + \"data.txt\" # data path\n",
    "\n",
    "sentences = load_data(data_path) # load data\n",
    "print(sentences[0]) \n",
    "\n",
    "convert_to_iobes_tags(sentences) # convert to iobes tags\n",
    "print(sentences[0]) \n",
    "\n",
    "train_sentences, test_sentences, dev_sentences = split_data(sentences) # split data \n",
    "print(\"The number of sentences of trainning data is\", len(train_sentences))\n",
    "print(\"The number of sentences of testing data is\", len(test_sentences))\n",
    "print(\"The number of sentences of development data is\", len(dev_sentences))\n",
    "\n",
    "# creates chinese characters and senquence convertion dictionaries\n",
    "id_to_char, char_to_id = create_char_id_convert_dict(train_sentences) \n",
    "# creates tags and senquence convertion dictionaries\n",
    "id_to_tag, tag_to_id = create_tag_id_convert_dict(train_sentences)\n",
    "print(\"The number of unique Chinese characters is:\", len(char_to_id))\n",
    "print(\"The number of unique tag characters is:\", len(tag_to_id))\n",
    "\n",
    "train_data = get_formated_data(train_sentences, char_to_id, tag_to_id) # formated training data\n",
    "test_data = get_formated_data(test_sentences, char_to_id, tag_to_id) # formated testing data\n",
    "dev_data = get_formated_data(dev_sentences, char_to_id, tag_to_id) # formated edata\n",
    "print(train_data[0])\n",
    "\n",
    "with open(folder_patch + 'dict.pkl', \"wb\") as out_file:  # dump data for eveluation \n",
    "    pickle.dump([char_to_id, id_to_char, tag_to_id, id_to_tag], out_file)\n",
    "    \n",
    "# Hyper Parameters:\n",
    "learning_rate = 0.001\n",
    "channel_char = 128 #embedding output dimention for char\n",
    "channel_phrase =20 #embedding output dimention for phrase\n",
    "channel_lstm = 256 #input dimention for Bi-lstm\n",
    "len_tags = len(tag_to_id)\n",
    "len_char = len(char_to_id)\n",
    "\n",
    "# generate batches with padding\n",
    "if selected_model == 'Bi-lstm':\n",
    "    train_data_lstm,train_labels = generate_batch_data_with_padding(train_data, 20, 'lstm',channel_lstm) \n",
    "    evl_data_lstm, evl_labels = generate_batch_data_with_padding(dev_data, 20, 'lstm',channel_lstm)\n",
    "    test_data_lstm, test_labels = generate_batch_data_with_padding(test_data, 20, 'lstm',channel_lstm)\n",
    "#     print('The shape of Ground Truth Numpy Array:', train_labels.shape,evl_labels.shape,test_labels.shape,)\n",
    "else:\n",
    "    train_batch_data = generate_batch_data_with_padding(train_data, 20) \n",
    "    dev_batch_data = generate_batch_data_with_padding(dev_data, 100)\n",
    "    test_batch_data = generate_batch_data_with_padding(test_data, 100)\n",
    "    \n",
    "    epoch_iterations = len(train_batch_data) # set the iterations per epoch\n",
    "    print(\"The number of steps per epoch is\", epoch_iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dependences of Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-lstm model\n",
    "# Because we take advantage of two types of features: cn_chars, cn_phrase, there will be two input layers\n",
    "def Bilstm():\n",
    "    crf = CRF(len(tag_to_id), sparse_target=True) # define the crf layer at first\n",
    "    char_input = Input(shape=(channel_lstm,), name=\"Input-CN-Char\") # the shape must be the max size of a sentence, it can be changed for long sentence. However, the too many neurons will slow down the model dramatically\n",
    "    char_emb = Embedding(len(char_to_id),output_dim=channel_char,trainable=False,mask_zero=True)(char_input) \n",
    "    phrase_input = Input(shape=(channel_lstm,), name=\"Input-CN-Phrase\")# the shape must be the max size of a sentence\n",
    "    phrase_emb = Embedding(input_dim=4,output_dim=20,trainable=False,mask_zero=True)(phrase_input) \n",
    "    # concatenate them to makes a single vector\n",
    "    merged = Concatenate(axis=-1)([char_emb, phrase_emb])\n",
    "#     dropout = Dropout(0.5)(merged) #prevent from overfitting # if encounter severe overfitting, uncomment this line and change the input layer of the next layer\n",
    "    lstm = Bidirectional(LSTM(100, return_sequences=True))(merged)\n",
    "    dropout = Dropout(0.5)(lstm) #prevent from overfitting\n",
    "    CRF_layer = crf(dropout)\n",
    "    model = Model_init(inputs=[char_input, phrase_input], outputs=[CRF_layer]) #explicate the input list.\n",
    "    model.summary() # plot the configure\n",
    "    plot_model(model, to_file='BI-lstm model.png') #output the figure of model structure\n",
    "    return model\n",
    "\n",
    "class Model(object): # <Fast and Accurate Entity Recognition with Iterated Dilated Convolutions>\n",
    "    def __init__(self):\n",
    "        self.__main_setup() # model initializing\n",
    "        \n",
    "    def __main_setup(self):\n",
    "        self.__hyper() #set up hyperparameters\n",
    "        self.__placeholder() #build tensor holder\n",
    "        self.__parameters() #initializing\n",
    "        self.__layers() #create model\n",
    "        self.__opt() #optimizer\n",
    "        \n",
    "    def __layers(self):\n",
    "        self.__embedding() #embedding layers\n",
    "        self.__dilated() # iterated dilated cnn \n",
    "        self.__loss() \n",
    "        \n",
    "    def __hyper(self):\n",
    "        self.learningR = learning_rate #learning rate \n",
    "        self.channel_char = channel_char  # char embedding dimention\n",
    "        self.channel_phrase = channel_phrase # phrase embedding dimention\n",
    "        self.len_tags = len_tags # number of tags\n",
    "        self.len_chars = len_char #unique Chinese char\n",
    "        self.output_channel = 0\n",
    "        \n",
    "    def __placeholder(self):\n",
    "        self.gt = placeholder(dtype=tf.int32) #GT\n",
    "        self.f1_evaluate = Variable(dtype=tf.float32,initial_value=0.0, trainable=False) #best f1 score for evaluate data\n",
    "        self.f1_test = Variable(dtype=tf.float32,initial_value=0.0, trainable=False) #for test data\n",
    "        self.whole_steps = Variable(dtype=tf.int32,initial_value=0, trainable=False) #steps for training process\n",
    "        self.cn_char = placeholder(dtype=tf.int32) #input sentence\n",
    "        self.cn_phrase = placeholder(dtype=tf.int32) #nput Chinese phrase features\n",
    "        self.dropout = placeholder(dtype=tf.float32) #dropout\n",
    "        \n",
    "    def __parameters(self):\n",
    "        self.output_channel = 0\n",
    "        self.len_phrase = 4  #phrase features 0,1,2,3\n",
    "        length = reduce_sum(sign(abs(self.cn_char)), reduction_indices=1)\n",
    "        self.lengths = cast(length, tf.int32)\n",
    "        self.batch_size = shape(self.cn_char)[0] #batch_size\n",
    "        self.num_steps = shape(self.cn_char)[-1] #num_steps: total chars in each sentenc\n",
    "        self.layers = [1,1,2] #based on the paper, there will be 2 types of dilated rates\n",
    "        self.flag_drop = 0.5  #prevent from overfitting\n",
    "        self.channel_cnn = 100 # cnn kernels numbers \n",
    "        self.minor = -1000.0\n",
    "        self.model_training = True\n",
    "        if self.model_training == False:\n",
    "            self.flag_drop = 1.0 \n",
    "        self.filters = 3 \n",
    "        self.iterations = 4 #iterated \n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        self.channel_embedding = self.channel_char + self.channel_phrase  # char channels + phrase channels\n",
    "        \n",
    "    def __embedding(self): # initializing for two features\n",
    "        char_embeddings = get_variable('Embeddings_1',shape=[self.len_chars, self.channel_char],initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None),\n",
    "            dtype=tf.float32,trainable=True)\n",
    "        feature_1 = embed(char_embeddings, self.cn_char)\n",
    "        phrase_embeddings = get_variable('Embeddings_2',shape=[self.len_phrase, self.channel_phrase],initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None),\n",
    "            dtype=tf.float32,trainable=True)\n",
    "        feature_2 = embed(phrase_embeddings, self.cn_phrase)\n",
    "        self.embedding_cns = concat([feature_1,feature_2], axis=-1)\n",
    "        self.embedding_cns = dropout(self.embedding_cns, self.dropout)#apply dropout \n",
    "\n",
    "    def __dilated(self):# Dilated Convolutions Networks\n",
    "        nets_input = expand_dims(self.embedding_cns, 1)   \n",
    "        initialed_weight = get_variable(\"kernel\",shape=[1, self.filters, self.channel_embedding,self.channel_cnn],initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "        nets_input = conv2d(nets_input, initialed_weight, strides=[1, 1, 1, 1],  padding=\"SAME\",name=\"nets_input\")\n",
    "        output = []\n",
    "        channels = 0\n",
    "        for j in range(self.iterations):  \n",
    "            for i in range(len(self.layers)):# many dilated cnns can cover almost all the features\n",
    "                dilated_rate = self.layers[i]\n",
    "                if i == (len(self.layers) - 1):\n",
    "                    last_layer = True\n",
    "                else:\n",
    "                    last_layer = False\n",
    "                with variable_scope(\"DilatedConv%d\" % i, reuse=tf.AUTO_REUSE):\n",
    "                    weights = get_variable(name='Weights',shape=[1, self.filters, self.channel_cnn,self.channel_cnn], initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "                    biases = get_variable(name='Biases',shape=[self.channel_cnn]) \n",
    "                    c = atrous_conv2d(nets_input,weights, rate=dilated_rate, padding=\"SAME\") # dilated convolution\n",
    "                    c = bias_add(c, biases)\n",
    "                    c = relu(c)\n",
    "                    if last_layer:\n",
    "                        channels += self.channel_cnn\n",
    "                        output.append(c)\n",
    "                    nets_input = c\n",
    "        output_last = concat(values=output,axis=3) # merge the output of 4 last layers\n",
    "        output_last = dropout(output_last, self.flag_drop) #add dropout layer \n",
    "#             drop dimention: the dimention which contians only one data\n",
    "        output_last = squeeze(output_last, [1])\n",
    "        output_last = reshape(output_last, [-1, channels]) # final features done\n",
    "        self.output_channel = channels\n",
    "        weight = get_variable(\"Weight\", shape=[self.output_channel, self.len_tags],dtype=tf.float32, initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "        bias = get_variable(\"Bias\",  initializer=tf.constant(0.0001, shape=[self.len_tags]))\n",
    "#                    matmul(x, w) + b.\n",
    "        result = xw_plus_b(output_last, weight, bias)\n",
    "        self.result =  reshape(result, [-1, self.num_steps, self.len_tags])  # num_steps: total chars in each sentenc, len_tags: number of tags\n",
    "\n",
    "    def __loss(self):\n",
    "        # pad units \n",
    "        initial_units = concat([self.minor*tf.ones(shape=[self.batch_size, 1, self.len_tags]), tf.zeros(shape=[self.batch_size, 1, 1])], axis=-1)\n",
    "        pad_units = cast(self.minor*tf.ones([self.batch_size, self.num_steps, 1]), tf.float32)\n",
    "        temp = concat([self.result, pad_units], axis=-1)\n",
    "        temp = concat([initial_units, temp], axis=1)\n",
    "        gt = concat([cast(self.len_tags*tf.ones([self.batch_size, 1]), tf.int32), self.gt], axis=-1)\n",
    "        self.transition = get_variable(\"transit\",shape=[self.len_tags + 1, self.len_tags + 1],initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "        likelihood, self.transition = crf_log_likelihood(inputs=temp,tag_indices=gt,transition_params=self.transition,sequence_lengths=self.lengths+1)\n",
    "        self.error = reduce_mean(likelihood*(-1))\n",
    "            \n",
    "    def __opt(self):\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learningR)\n",
    "        gradients = self.optimizer.compute_gradients(self.error) \n",
    "        limited_gradients = [[tf.clip_by_value(gra, -4, 4), va] for gra, va in gradients] # avoid gradient explosion\n",
    "        self.optimize = self.optimizer.apply_gradients(limited_gradients, self.whole_steps)\n",
    "            \n",
    "    def evaluate(self, sess, batch_data, id_to_tag):\n",
    "        transition = self.transition.eval()\n",
    "        report = []\n",
    "        for batch in batch_data:\n",
    "            cn_sentences = batch[0]\n",
    "            tags = batch[-1] #true tag\n",
    "            lengths, scores = self.each_step(sess, False, batch)\n",
    "            batch_paths = self.viterbi(scores, lengths, transition)\n",
    "            for i in range(len(cn_sentences)):\n",
    "                output = []\n",
    "                sentence = cn_sentences[i][:lengths[i]]\n",
    "                gt = convert_iobes_to_iob_tags([id_to_tag[int(x)] for x in tags[i][:lengths[i]]])\n",
    "                predict = convert_iobes_to_iob_tags([id_to_tag[int(x)] for x in batch_paths[i][:lengths[i]]])\n",
    "                for cn_char, gt, predict in zip(sentence, gt, predict):\n",
    "                    output.append(\" \".join([cn_char, gt, predict]))\n",
    "                report.append(output)\n",
    "        return report\n",
    "    \n",
    "    def viterbi(self, units, lengths, array): # viterbi Algorithm\n",
    "        paths = []\n",
    "        begin = np.asarray([[self.minor]*self.len_tags +[0]])\n",
    "        for val, temp_len in zip(units, lengths):\n",
    "            val = val[:temp_len]\n",
    "            pad = np.ones([temp_len, 1])*(self.minor)\n",
    "            units = np.concatenate([val, pad], axis=1)\n",
    "            units = np.concatenate([begin, units], axis=0)\n",
    "            path, _ = viterbi_decode(units, array)\n",
    "            paths.append(path[1:])\n",
    "        return paths\n",
    "    \n",
    "    def each_step(self, sess, training, batch):\n",
    "        _, cn_char, cn_phrase, tags = batch\n",
    "        temp_dict = {self.cn_char: np.asarray(cn_char),self.cn_phrase: np.asarray(cn_phrase), self.dropout: 1.0}\n",
    "        if training:\n",
    "            temp_dict[self.gt] = np.asarray(tags) #GT\n",
    "            temp_dict[self.dropout] = 0.5\n",
    "            whole_steps, error, _ = sess.run([self.whole_steps, self.error, self.optimize], temp_dict)\n",
    "            return whole_steps, error\n",
    "        else:\n",
    "            lengths, units = sess.run([self.lengths, self.result], temp_dict)\n",
    "            return lengths, units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert IOBES tags to IOB tags\n",
    "def convert_iobes_to_iob_tags(tags):\n",
    "    iob_tags = []\n",
    "    for index, tag in enumerate(tags):\n",
    "        t = tag.split('-')[0]        \n",
    "        if t == 'S': iob_tags.append(tag.replace('S-', 'B-'))\n",
    "        elif t == 'E': iob_tags.append(tag.replace('E-', 'I-'))\n",
    "        else: iob_tags.append(tag)          \n",
    "    return iob_tags\n",
    "\n",
    "def string_tag(data, id_to_tag, is_groundtruth=False ,): # convert tag num to strings\n",
    "    tags = []\n",
    "    if is_groundtruth == False:\n",
    "        for dim_0 in data:\n",
    "            temp = []\n",
    "            for dim_1 in dim_0:\n",
    "                temp.append(id_to_tag[np.asscalar(np.array([dim_1]))]) #np variable should be convert to scalar!\n",
    "            tags.append(temp)\n",
    "        return tags\n",
    "    else:       \n",
    "        for dim_0 in data:\n",
    "            temp = []\n",
    "            for dim_1 in dim_0:\n",
    "                index = dim_1.argmax() # obtain index\n",
    "                temp.append(id_to_tag[index]) #convert to string tag\n",
    "            tags.append(temp)\n",
    "        return tags\n",
    "\n",
    "# check if a phrase ended between the previous and current character\n",
    "def check_phrase_end_tag(prev_tag, cur_tag, prev_type, cur_type):\n",
    "    is_end = False\n",
    "\n",
    "    if prev_tag == 'E': is_end = True\n",
    "    if prev_tag == 'S': is_end = True\n",
    "        \n",
    "    if prev_tag == 'B' and (cur_tag == 'B' or cur_tag == 'S' or cur_tag == 'O'): \n",
    "        is_end = True  \n",
    "    if prev_tag == 'I' and (cur_tag == 'B' or cur_tag == 'S' or cur_tag == 'O'): \n",
    "        is_end = True\n",
    "        \n",
    "    if prev_tag != 'O' and prev_tag != '.' and prev_type != cur_type: \n",
    "        is_end = True\n",
    "\n",
    "    return is_end\n",
    "\n",
    "\n",
    "# check if a phrase started between the previous and current character\n",
    "def check_phrase_start_tag(prev_tag, cur_tag, prev_type, cur_type):\n",
    "    is_start = False\n",
    "\n",
    "    if cur_tag == 'B': chunk_start = True\n",
    "    if cur_tag == 'S': chunk_start = True\n",
    "    \n",
    "    if prev_tag == 'E' and (cur_tag == 'E' or cur_tag == 'I'): \n",
    "        is_start = True\n",
    "    if prev_tag == 'S' and (cur_tag == 'E' or cur_tag == 'I'): \n",
    "        is_start = True\n",
    "    if prev_tag == 'O' and (cur_tag == 'E' or cur_tag == 'I'): \n",
    "        is_start = True\n",
    "    \n",
    "    if cur_tag != 'O' and cur_tag != '.' and prev_type != cur_type: \n",
    "        is_start = True\n",
    "\n",
    "    return is_start\n",
    "\n",
    "\n",
    "# calculate the precision, recall and f-score\n",
    "def get_metrics(correct_count, predict_count, total_count):\n",
    "    TP = correct_count \n",
    "    FP = predict_count - correct_count\n",
    "    FN = total_count - correct_count\n",
    "\n",
    "    prec = 0 if (TP + FP == 0) else (1. * TP) / (TP + FP)  # precision\n",
    "    recall = 0 if (TP + FN == 0) else (1. * TP) / (TP + FN)  # recall \n",
    "    fscore = 0 if (prec + recall == 0) else (2 * prec * recall / (prec + recall)) # f-score\n",
    "\n",
    "    Metrics = namedtuple('Metrics', 'tp fp fn prec rec fscore')\n",
    "    return Metrics(TP, FP, FN, prec, recall, fscore)\n",
    "\n",
    "\n",
    "# parse tag into IOBES tags and enetity type\n",
    "def parse_tages(tag):\n",
    "    matched = re.match(r'^([^-]*)-(.*)$', tag)\n",
    "    return matched.groups() if matched else (tag, '')\n",
    "\n",
    "\n",
    "# print precsion, recall and f-score with format\n",
    "def print_report(parse_results, correct_entities, found_c_entities, found_g_entities):    \n",
    "    parsed_report = []        \n",
    "    metrics = get_metrics(parse_results[0], parse_results[3], parse_results[2])\n",
    "    \n",
    "    # all the found entities\n",
    "    cg_entities = list(found_c_entities) + list(found_g_entities)    \n",
    "    uniq_tags = set([e for e in cg_entities])  # unique tags\n",
    "\n",
    "    # get metrics includes precsion, recall and f-score\n",
    "    entity_metrics = {}\n",
    "    for tag in uniq_tags:\n",
    "        entity_metrics[tag] = get_metrics(correct_entities[tag], found_g_entities[tag], found_c_entities[tag])\n",
    "     \n",
    "    # print total tokens and phrases count\n",
    "    result_line = []\n",
    "    result_line.append('Total tokens is %d and total is phrases %d\\n' % (parse_results[4], parse_results[2]))\n",
    "    result_line.append('Found: %d phrases, correct: %d.\\n' % (parse_results[3], parse_results[0]))\n",
    "    parsed_report.append(\"\".join(result_line))\n",
    "\n",
    "    # formated result lines \n",
    "    if parse_results[4] > 0:\n",
    "        result_line = []\n",
    "        result_line.append(\"Accuracy:%6.2f%%, \" % (100. * parse_results[1] / parse_results[4]))\n",
    "        result_line.append(\"Precision:%6.2f%%, \" % (100.* metrics.prec))\n",
    "        result_line.append(\"Recall:%6.2f%%, \" % (100. * metrics.rec))\n",
    "        result_line.append(\"Fscore:%6.2f\\n\" % (100. * metrics.fscore))\n",
    "        parsed_report.append(\"\".join(result_line))\n",
    "\n",
    "    for index, metric in sorted(entity_metrics.items()):\n",
    "        result_line = []\n",
    "        result_line.append('%17s: ' % index)\n",
    "        result_line.append('Precision:%6.2f%%, ' % (100. * metric.prec))\n",
    "        result_line.append('Recall:%6.2f%%, ' % (100. * metric.rec))\n",
    "        result_line.append('Fscore:%6.2f\\n' % (100. * metric.fscore))\n",
    "        parsed_report.append(\"\".join(result_line))\n",
    "    \n",
    "    return parsed_report\n",
    "\n",
    "\n",
    "# parsed the reports\n",
    "def parse_report(file_name):\n",
    "\n",
    "    is_correct = False        # if current chunk is correct\n",
    "    \n",
    "    prev_ctag = 'O'           # previous correct tag\n",
    "    prev_ctag_entity = ''     # previous correct entity (LOC, ORG, PER)\n",
    "    prev_gtag = 'O'           # previous guessed tag\n",
    "    prev_gtag_entity = ''     # previous guessed entity (LOC, ORG, PER)\n",
    "    \n",
    "    # 0: correct entity number, 1: correct tag number, 2: number of phrases  \n",
    "    # 3: number of guessed phrases 4: number of tokens\n",
    "    results = [0, 0, 0, 0, 0]\n",
    "\n",
    "    correct_entities = defaultdict(int)\n",
    "    found_c_entities = defaultdict(int)\n",
    "    found_g_entities = defaultdict(int)\n",
    "\n",
    "    with codecs.open(file_name, \"r\") as file:   # read file\n",
    "        for line in file:\n",
    "            features = line.split() # features list per line  \n",
    "            if len(features) == 0: \n",
    "                features = ['-X-', 'O', 'O'] # for white space\n",
    "\n",
    "            cur_gtag, cur_gtag_entity = parse_tages(features.pop())  # parse predicted tag\n",
    "            cur_ctag, cur_ctag_entity = parse_tages(features.pop())  # parse correct tag\n",
    "            chinese_char = features.pop(0)  # chinese character \n",
    "\n",
    "            # check if the phrase is ended between the previous and current character\n",
    "            is_end_correct = check_phrase_end_tag(prev_ctag, cur_ctag, prev_ctag_entity, cur_ctag_entity)\n",
    "            is_end_guessed = check_phrase_end_tag(prev_gtag, cur_gtag, prev_gtag_entity, cur_gtag_entity)\n",
    "\n",
    "            # check if the phrase is started between the previous and current character\n",
    "            is_start_correct = check_phrase_start_tag(prev_ctag, cur_ctag, prev_ctag_entity, cur_ctag_entity)\n",
    "            is_start_guessed = check_phrase_start_tag(prev_gtag, cur_gtag, prev_gtag_entity, cur_gtag_entity)\n",
    "\n",
    "            if is_correct:\n",
    "                if (is_end_correct and is_end_guessed and prev_gtag_entity == prev_ctag_entity):\n",
    "                    is_correct = False\n",
    "                    results[0] += 1\n",
    "                    correct_entities[prev_ctag_entity] += 1\n",
    "\n",
    "                elif (is_end_correct != is_end_guessed or cur_gtag_entity != cur_ctag_entity):\n",
    "                    is_correct = False\n",
    "\n",
    "            if is_start_correct and is_start_guessed and cur_gtag_entity == cur_ctag_entity:\n",
    "                is_correct = True\n",
    "\n",
    "            if is_start_correct:\n",
    "                results[2] += 1\n",
    "                found_c_entities[cur_ctag_entity] += 1\n",
    "            if is_start_guessed:\n",
    "                results[3] += 1\n",
    "                found_g_entities[cur_gtag_entity] += 1\n",
    "            \n",
    "            if chinese_char != '-X-':  # not empty character\n",
    "                if cur_ctag == cur_gtag and cur_gtag_entity == cur_ctag_entity:\n",
    "                    results[1] += 1\n",
    "                results[4] += 1\n",
    "            \n",
    "            # get previous tags \n",
    "            prev_gtag = cur_gtag\n",
    "            prev_ctag = cur_ctag\n",
    "            prev_gtag_entity = cur_gtag_entity\n",
    "            prev_ctag_entity = cur_ctag_entity\n",
    "\n",
    "        if is_correct:\n",
    "            results[0] += 1\n",
    "            correct_entities[prev_ctag_entity] += 1\n",
    "\n",
    "    # get parsed report,includes accuracy, precsion, recall and f-score\n",
    "    parsed_report = print_report(results, correct_entities, found_c_entities, found_g_entities)   \n",
    "    return parsed_report\n",
    "\n",
    "# write predict result and parse report\n",
    "def evaluate_report(train_results, file_path):\n",
    "    # file name\n",
    "    file_name = os.path.join(file_path, \"predict_result.txt\") \n",
    "    # write file\n",
    "    with open(file_name, \"w\") as outfile:\n",
    "        write_context = []\n",
    "        # write line by line\n",
    "        for chunk in train_results:\n",
    "            for line in chunk:\n",
    "                write_context.append(line + \"\\n\")\n",
    "            write_context.append(\"\\n\")\n",
    "        outfile.writelines(write_context)\n",
    "    # parse report\n",
    "    result_lines = parse_report(file_name)\n",
    "    return result_lines\n",
    "\n",
    "# evalute data\n",
    "def evaluate(tf_sess, model, data, id_to_tag):\n",
    "    predict_results = model.evaluate(tf_sess, data, id_to_tag)\n",
    "    parsed_lines = evaluate_report(predict_results, folder_patch)\n",
    "    for line in parsed_lines:\n",
    "        print(line)\n",
    "    f1 = float(parsed_lines[1].strip().split()[-1])\n",
    "    f1_test = model.f1_evaluate.eval()\n",
    "    if f1 > f1_test:\n",
    "        tf.assign(model.f1_evaluate, f1).eval()\n",
    "        print(\"Best f1 score: {:>.3f}\".format(f1))\n",
    "    return f1 > f1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larry5/.conda/envs/project/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Num:1 Step Num:100 of 974, Model Loss:11.783861\n",
      "Epoch Num:1 Step Num:200 of 974, Model Loss: 8.198160\n",
      "Epoch Num:1 Step Num:300 of 974, Model Loss: 7.557155\n",
      "Epoch Num:1 Step Num:400 of 974, Model Loss: 6.960884\n",
      "Epoch Num:1 Step Num:500 of 974, Model Loss: 7.400844\n",
      "Epoch Num:1 Step Num:600 of 974, Model Loss: 7.345351\n",
      "Epoch Num:1 Step Num:700 of 974, Model Loss: 7.335250\n",
      "Epoch Num:1 Step Num:800 of 974, Model Loss: 7.713682\n",
      "Epoch Num:1 Step Num:900 of 974, Model Loss: 7.926855\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 1788 phrases, correct: 1216.\n",
      "\n",
      "Accuracy: 95.44%, Precision: 68.01%, Recall: 56.32%, Fscore: 61.62\n",
      "\n",
      "              LOC: Precision: 57.56%, Recall: 69.13%, Fscore: 62.81\n",
      "\n",
      "              ORG: Precision: 80.48%, Recall: 43.86%, Fscore: 56.78\n",
      "\n",
      "              PER: Precision: 89.14%, Recall: 49.68%, Fscore: 63.80\n",
      "\n",
      "Best f1 score: 61.620\n",
      "Epoch Num:2 Step Num:26 of 974, Model Loss:10.767795\n",
      "Epoch Num:2 Step Num:126 of 974, Model Loss: 2.340820\n",
      "Epoch Num:2 Step Num:226 of 974, Model Loss: 2.296874\n",
      "Epoch Num:2 Step Num:326 of 974, Model Loss: 2.609559\n",
      "Epoch Num:2 Step Num:426 of 974, Model Loss: 2.796995\n",
      "Epoch Num:2 Step Num:526 of 974, Model Loss: 3.018409\n",
      "Epoch Num:2 Step Num:626 of 974, Model Loss: 3.354306\n",
      "Epoch Num:2 Step Num:726 of 974, Model Loss: 3.952556\n",
      "Epoch Num:2 Step Num:826 of 974, Model Loss: 3.950431\n",
      "Epoch Num:2 Step Num:926 of 974, Model Loss: 4.882520\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 1891 phrases, correct: 1565.\n",
      "\n",
      "Accuracy: 97.21%, Precision: 82.76%, Recall: 72.49%, Fscore: 77.28\n",
      "\n",
      "              LOC: Precision: 77.13%, Recall: 77.72%, Fscore: 77.42\n",
      "\n",
      "              ORG: Precision: 90.57%, Recall: 59.74%, Fscore: 71.99\n",
      "\n",
      "              PER: Precision: 86.45%, Recall: 77.23%, Fscore: 81.58\n",
      "\n",
      "Best f1 score: 77.280\n",
      "Epoch Num:3 Step Num:52 of 974, Model Loss: 5.089668\n",
      "Epoch Num:3 Step Num:152 of 974, Model Loss: 1.283387\n",
      "Epoch Num:3 Step Num:252 of 974, Model Loss: 1.551242\n",
      "Epoch Num:3 Step Num:352 of 974, Model Loss: 1.658453\n",
      "Epoch Num:3 Step Num:452 of 974, Model Loss: 1.997549\n",
      "Epoch Num:3 Step Num:552 of 974, Model Loss: 2.087638\n",
      "Epoch Num:3 Step Num:652 of 974, Model Loss: 2.319744\n",
      "Epoch Num:3 Step Num:752 of 974, Model Loss: 2.775274\n",
      "Epoch Num:3 Step Num:852 of 974, Model Loss: 2.901343\n",
      "Epoch Num:3 Step Num:952 of 974, Model Loss: 3.887314\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 2092 phrases, correct: 1738.\n",
      "\n",
      "Accuracy: 97.64%, Precision: 83.08%, Recall: 80.50%, Fscore: 81.77\n",
      "\n",
      "              LOC: Precision: 79.75%, Recall: 84.78%, Fscore: 82.19\n",
      "\n",
      "              ORG: Precision: 84.22%, Recall: 72.50%, Fscore: 77.92\n",
      "\n",
      "              PER: Precision: 87.59%, Recall: 82.01%, Fscore: 84.70\n",
      "\n",
      "Best f1 score: 81.770\n",
      "Epoch Num:4 Step Num:78 of 974, Model Loss: 2.566514\n",
      "Epoch Num:4 Step Num:178 of 974, Model Loss: 0.957505\n",
      "Epoch Num:4 Step Num:278 of 974, Model Loss: 1.182704\n",
      "Epoch Num:4 Step Num:378 of 974, Model Loss: 1.233991\n",
      "Epoch Num:4 Step Num:478 of 974, Model Loss: 1.422535\n",
      "Epoch Num:4 Step Num:578 of 974, Model Loss: 1.630625\n",
      "Epoch Num:4 Step Num:678 of 974, Model Loss: 1.854850\n",
      "Epoch Num:4 Step Num:778 of 974, Model Loss: 2.186527\n",
      "Epoch Num:4 Step Num:878 of 974, Model Loss: 2.471709\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 2223 phrases, correct: 1832.\n",
      "\n",
      "Accuracy: 97.50%, Precision: 82.41%, Recall: 84.85%, Fscore: 83.61\n",
      "\n",
      "              LOC: Precision: 84.86%, Recall: 84.67%, Fscore: 84.77\n",
      "\n",
      "              ORG: Precision: 73.95%, Recall: 80.85%, Fscore: 77.25\n",
      "\n",
      "              PER: Precision: 87.76%, Recall: 89.01%, Fscore: 88.38\n",
      "\n",
      "Best f1 score: 83.610\n"
     ]
    }
   ],
   "source": [
    "# Iterated Dilated Convolutions model\n",
    "steps_check = 100\n",
    "if selected_model !='Bi-lstm':\n",
    "    if restart_flag == False:\n",
    "        restart_flag = True\n",
    "        with tf.Session() as sess:\n",
    "            model = Model()\n",
    "            loss_holder = []\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(4):\n",
    "                for batch in train_batch_data:\n",
    "                    step, temp_loss = model.each_step(sess, True, batch)\n",
    "                    loss_holder.append(temp_loss)\n",
    "                    if step % steps_check == 0:\n",
    "                        Epoch = step // epoch_iterations + 1\n",
    "                        print(\"Epoch Num:{} Step Num:{} of {}, \"\"Model Loss:{:>9.6f}\".format(Epoch, step % epoch_iterations, epoch_iterations, np.mean(loss_holder)))\n",
    "                        loss_holder = []\n",
    "\n",
    "                evaluate(sess, model, dev_batch_data, id_to_tag)\n",
    "    else:\n",
    "        print('Do not train both models in one time. The GPU memory has been filled up by the last model.  Please restart the kernel to clean the GPU and re-run the codes to train the second model!' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BI-lstm model:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-CN-Char (InputLayer)      (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-CN-Phrase (InputLayer)    (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 256, 128)     547456      Input-CN-Char[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 256, 20)      80          Input-CN-Phrase[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256, 148)     0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256, 200)     199200      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256, 200)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, 256, 13)      2808        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 749,544\n",
      "Trainable params: 202,008\n",
      "Non-trainable params: 547,536\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 76s 4ms/step - loss: 23.4803 - crf_accuracy: 0.7670 - val_loss: 23.1865 - val_crf_accuracy: 0.8682\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 0 phrases, correct: 0.\n",
      "\n",
      "Accuracy: 81.10%, Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "              LOC: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "              ORG: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 0.000\n",
      "Epoch 2/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 73s 4ms/step - loss: 23.1032 - crf_accuracy: 0.8572 - val_loss: 23.1364 - val_crf_accuracy: 0.8682\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 0 phrases, correct: 0.\n",
      "\n",
      "Accuracy: 81.10%, Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "              LOC: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "              ORG: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 0.000\n",
      "Epoch 3/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 73s 4ms/step - loss: 23.0479 - crf_accuracy: 0.8569 - val_loss: 23.0872 - val_crf_accuracy: 0.8685\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 20 phrases, correct: 9.\n",
      "\n",
      "Accuracy: 81.16%, Precision: 45.00%, Recall:  1.43%, Fscore:  2.76\n",
      "\n",
      "              LOC: Precision: 15.38%, Recall:  1.09%, Fscore:  2.04\n",
      "\n",
      "              ORG: Precision:100.00%, Recall:  2.86%, Fscore:  5.56\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 2.760\n",
      "Epoch 4/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 73s 4ms/step - loss: 22.9892 - crf_accuracy: 0.8581 - val_loss: 23.0351 - val_crf_accuracy: 0.8693\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 20 phrases, correct: 16.\n",
      "\n",
      "Accuracy: 81.13%, Precision: 80.00%, Recall:  2.54%, Fscore:  4.92\n",
      "\n",
      "              LOC: Precision: 25.00%, Recall:  0.55%, Fscore:  1.07\n",
      "\n",
      "              ORG: Precision: 93.75%, Recall:  6.12%, Fscore: 11.49\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 4.920\n",
      "Epoch 5/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 73s 4ms/step - loss: 22.9408 - crf_accuracy: 0.8604 - val_loss: 22.9934 - val_crf_accuracy: 0.8730\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 80 phrases, correct: 49.\n",
      "\n",
      "Accuracy: 82.03%, Precision: 61.25%, Recall:  7.77%, Fscore: 13.78\n",
      "\n",
      "              LOC: Precision: 28.57%, Recall:  3.28%, Fscore:  5.88\n",
      "\n",
      "              ORG: Precision: 72.88%, Recall: 17.55%, Fscore: 28.29\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 13.780\n",
      "Epoch 6/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 73s 4ms/step - loss: 22.9028 - crf_accuracy: 0.8632 - val_loss: 22.9623 - val_crf_accuracy: 0.8756\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 103 phrases, correct: 69.\n",
      "\n",
      "Accuracy: 82.99%, Precision: 66.99%, Recall: 10.94%, Fscore: 18.80\n",
      "\n",
      "              LOC: Precision: 50.00%, Recall: 10.38%, Fscore: 17.19\n",
      "\n",
      "              ORG: Precision: 76.92%, Recall: 20.41%, Fscore: 32.26\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 18.800\n",
      "Epoch 7/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 73s 4ms/step - loss: 22.8672 - crf_accuracy: 0.8666 - val_loss: 22.9314 - val_crf_accuracy: 0.8785\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 236 phrases, correct: 132.\n",
      "\n",
      "Accuracy: 83.65%, Precision: 55.93%, Recall: 20.92%, Fscore: 30.45\n",
      "\n",
      "              LOC: Precision: 68.97%, Recall: 10.93%, Fscore: 18.87\n",
      "\n",
      "              ORG: Precision: 54.11%, Recall: 45.71%, Fscore: 49.56\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 30.450\n",
      "Epoch 8/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 73s 4ms/step - loss: 22.8302 - crf_accuracy: 0.8698 - val_loss: 22.8909 - val_crf_accuracy: 0.8837\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 229 phrases, correct: 142.\n",
      "\n",
      "Accuracy: 84.31%, Precision: 62.01%, Recall: 22.50%, Fscore: 33.02\n",
      "\n",
      "              LOC: Precision: 64.00%, Recall: 17.49%, Fscore: 27.47\n",
      "\n",
      "              ORG: Precision: 61.45%, Recall: 44.90%, Fscore: 51.89\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 33.020\n",
      "Epoch 9/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 73s 4ms/step - loss: 22.7938 - crf_accuracy: 0.8736 - val_loss: 22.8603 - val_crf_accuracy: 0.8852\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 206 phrases, correct: 135.\n",
      "\n",
      "Accuracy: 84.46%, Precision: 65.53%, Recall: 21.39%, Fscore: 32.26\n",
      "\n",
      "              LOC: Precision: 63.16%, Recall: 19.67%, Fscore: 30.00\n",
      "\n",
      "              ORG: Precision: 66.89%, Recall: 40.41%, Fscore: 50.38\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 32.260\n",
      "Epoch 10/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 73s 4ms/step - loss: 22.7663 - crf_accuracy: 0.8761 - val_loss: 22.8367 - val_crf_accuracy: 0.8865\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 215 phrases, correct: 145.\n",
      "\n",
      "Accuracy: 84.79%, Precision: 67.44%, Recall: 22.98%, Fscore: 34.28\n",
      "\n",
      "              LOC: Precision: 62.50%, Recall: 21.86%, Fscore: 32.39\n",
      "\n",
      "              ORG: Precision: 71.13%, Recall: 41.22%, Fscore: 52.20\n",
      "\n",
      "              PER: Precision: 44.44%, Recall:  1.97%, Fscore:  3.77\n",
      "\n",
      "Best F1 score: 34.280\n",
      "Epoch 11/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 73s 4ms/step - loss: 22.7400 - crf_accuracy: 0.8805 - val_loss: 22.8111 - val_crf_accuracy: 0.8921\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 271 phrases, correct: 187.\n",
      "\n",
      "Accuracy: 85.83%, Precision: 69.00%, Recall: 29.64%, Fscore: 41.46\n",
      "\n",
      "              LOC: Precision: 61.76%, Recall: 34.43%, Fscore: 44.21\n",
      "\n",
      "              ORG: Precision: 77.78%, Recall: 42.86%, Fscore: 55.26\n",
      "\n",
      "              PER: Precision: 55.88%, Recall:  9.36%, Fscore: 16.03\n",
      "\n",
      "Best F1 score: 41.460\n",
      "Epoch 12/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 73s 4ms/step - loss: 22.7182 - crf_accuracy: 0.8842 - val_loss: 22.7947 - val_crf_accuracy: 0.8933\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 228 phrases, correct: 167.\n",
      "\n",
      "Accuracy: 85.44%, Precision: 73.25%, Recall: 26.47%, Fscore: 38.88\n",
      "\n",
      "              LOC: Precision: 75.47%, Recall: 21.86%, Fscore: 33.90\n",
      "\n",
      "              ORG: Precision: 73.89%, Recall: 47.35%, Fscore: 57.71\n",
      "\n",
      "              PER: Precision: 61.11%, Recall:  5.42%, Fscore:  9.95\n",
      "\n",
      "Best F1 score: 38.880\n",
      "Epoch 13/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 73s 4ms/step - loss: 22.7005 - crf_accuracy: 0.8875 - val_loss: 22.7763 - val_crf_accuracy: 0.9007\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 359 phrases, correct: 247.\n",
      "\n",
      "Accuracy: 87.09%, Precision: 68.80%, Recall: 39.14%, Fscore: 49.90\n",
      "\n",
      "              LOC: Precision: 63.21%, Recall: 36.61%, Fscore: 46.37\n",
      "\n",
      "              ORG: Precision: 71.96%, Recall: 55.51%, Fscore: 62.67\n",
      "\n",
      "              PER: Precision: 68.75%, Recall: 21.67%, Fscore: 32.96\n",
      "\n",
      "Best F1 score: 49.900\n",
      "Epoch 14/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.6849 - crf_accuracy: 0.8896 - val_loss: 22.7644 - val_crf_accuracy: 0.9006\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 277 phrases, correct: 216.\n",
      "\n",
      "Accuracy: 87.03%, Precision: 77.98%, Recall: 34.23%, Fscore: 47.58\n",
      "\n",
      "              LOC: Precision: 69.89%, Recall: 35.52%, Fscore: 47.10\n",
      "\n",
      "              ORG: Precision: 86.36%, Recall: 46.53%, Fscore: 60.48\n",
      "\n",
      "              PER: Precision: 71.15%, Recall: 18.23%, Fscore: 29.02\n",
      "\n",
      "Best F1 score: 47.580\n",
      "Epoch 15/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.6721 - crf_accuracy: 0.8922 - val_loss: 22.7519 - val_crf_accuracy: 0.9046\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 351 phrases, correct: 263.\n",
      "\n",
      "Accuracy: 87.84%, Precision: 74.93%, Recall: 41.68%, Fscore: 53.56\n",
      "\n",
      "              LOC: Precision: 68.10%, Recall: 43.17%, Fscore: 52.84\n",
      "\n",
      "              ORG: Precision: 81.58%, Recall: 50.61%, Fscore: 62.47\n",
      "\n",
      "              PER: Precision: 72.29%, Recall: 29.56%, Fscore: 41.96\n",
      "\n",
      "Best F1 score: 53.560\n",
      "Epoch 16/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.6600 - crf_accuracy: 0.8954 - val_loss: 22.7416 - val_crf_accuracy: 0.9057\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 356 phrases, correct: 265.\n",
      "\n",
      "Accuracy: 87.90%, Precision: 74.44%, Recall: 42.00%, Fscore: 53.70\n",
      "\n",
      "              LOC: Precision: 66.67%, Recall: 46.99%, Fscore: 55.13\n",
      "\n",
      "              ORG: Precision: 79.38%, Recall: 51.84%, Fscore: 62.72\n",
      "\n",
      "              PER: Precision: 77.61%, Recall: 25.62%, Fscore: 38.52\n",
      "\n",
      "Best F1 score: 53.700\n",
      "Epoch 17/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.6505 - crf_accuracy: 0.8971 - val_loss: 22.7321 - val_crf_accuracy: 0.9081\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 356 phrases, correct: 275.\n",
      "\n",
      "Accuracy: 88.26%, Precision: 77.25%, Recall: 43.58%, Fscore: 55.72\n",
      "\n",
      "              LOC: Precision: 73.79%, Recall: 41.53%, Fscore: 53.15\n",
      "\n",
      "              ORG: Precision: 80.49%, Recall: 53.88%, Fscore: 64.55\n",
      "\n",
      "              PER: Precision: 75.28%, Recall: 33.00%, Fscore: 45.89\n",
      "\n",
      "Best F1 score: 55.720\n",
      "Epoch 18/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.6398 - crf_accuracy: 0.8997 - val_loss: 22.7242 - val_crf_accuracy: 0.9093\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 359 phrases, correct: 280.\n",
      "\n",
      "Accuracy: 88.44%, Precision: 77.99%, Recall: 44.37%, Fscore: 56.57\n",
      "\n",
      "              LOC: Precision: 75.24%, Recall: 43.17%, Fscore: 54.86\n",
      "\n",
      "              ORG: Precision: 80.24%, Recall: 54.69%, Fscore: 65.05\n",
      "\n",
      "              PER: Precision: 77.01%, Recall: 33.00%, Fscore: 46.21\n",
      "\n",
      "Best F1 score: 56.570\n",
      "Epoch 19/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.6327 - crf_accuracy: 0.9005 - val_loss: 22.7178 - val_crf_accuracy: 0.9102\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 367 phrases, correct: 286.\n",
      "\n",
      "Accuracy: 88.65%, Precision: 77.93%, Recall: 45.32%, Fscore: 57.31\n",
      "\n",
      "              LOC: Precision: 70.83%, Recall: 46.45%, Fscore: 56.11\n",
      "\n",
      "              ORG: Precision: 82.10%, Recall: 54.29%, Fscore: 65.36\n",
      "\n",
      "              PER: Precision: 80.00%, Recall: 33.50%, Fscore: 47.22\n",
      "\n",
      "Best F1 score: 57.310\n",
      "Epoch 20/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.6245 - crf_accuracy: 0.9026 - val_loss: 22.7116 - val_crf_accuracy: 0.9113\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 396 phrases, correct: 307.\n",
      "\n",
      "Accuracy: 89.07%, Precision: 77.53%, Recall: 48.65%, Fscore: 59.79\n",
      "\n",
      "              LOC: Precision: 79.17%, Recall: 41.53%, Fscore: 54.48\n",
      "\n",
      "              ORG: Precision: 80.79%, Recall: 58.37%, Fscore: 67.77\n",
      "\n",
      "              PER: Precision: 71.54%, Recall: 43.35%, Fscore: 53.99\n",
      "\n",
      "Best F1 score: 59.790\n",
      "Epoch 21/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.6178 - crf_accuracy: 0.9039 - val_loss: 22.7073 - val_crf_accuracy: 0.9108\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 344 phrases, correct: 287.\n",
      "\n",
      "Accuracy: 88.95%, Precision: 83.43%, Recall: 45.48%, Fscore: 58.87\n",
      "\n",
      "              LOC: Precision: 86.08%, Recall: 37.16%, Fscore: 51.91\n",
      "\n",
      "              ORG: Precision: 83.63%, Recall: 58.37%, Fscore: 68.75\n",
      "\n",
      "              PER: Precision: 80.85%, Recall: 37.44%, Fscore: 51.18\n",
      "\n",
      "Best F1 score: 58.870\n",
      "Epoch 22/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.6116 - crf_accuracy: 0.9058 - val_loss: 22.7034 - val_crf_accuracy: 0.9106\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 325 phrases, correct: 272.\n",
      "\n",
      "Accuracy: 88.62%, Precision: 83.69%, Recall: 43.11%, Fscore: 56.90\n",
      "\n",
      "              LOC: Precision: 80.72%, Recall: 36.61%, Fscore: 50.38\n",
      "\n",
      "              ORG: Precision: 85.37%, Recall: 57.14%, Fscore: 68.46\n",
      "\n",
      "              PER: Precision: 83.33%, Recall: 32.02%, Fscore: 46.26\n",
      "\n",
      "Best F1 score: 56.900\n",
      "Epoch 23/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.6059 - crf_accuracy: 0.9063 - val_loss: 22.6960 - val_crf_accuracy: 0.9155\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 383 phrases, correct: 305.\n",
      "\n",
      "Accuracy: 89.22%, Precision: 79.63%, Recall: 48.34%, Fscore: 60.16\n",
      "\n",
      "              LOC: Precision: 85.90%, Recall: 36.61%, Fscore: 51.34\n",
      "\n",
      "              ORG: Precision: 79.47%, Recall: 61.63%, Fscore: 69.43\n",
      "\n",
      "              PER: Precision: 75.65%, Recall: 42.86%, Fscore: 54.72\n",
      "\n",
      "Best F1 score: 60.160\n",
      "Epoch 24/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.6001 - crf_accuracy: 0.9084 - val_loss: 22.6912 - val_crf_accuracy: 0.9155\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 345 phrases, correct: 295.\n",
      "\n",
      "Accuracy: 89.34%, Precision: 85.51%, Recall: 46.75%, Fscore: 60.45\n",
      "\n",
      "              LOC: Precision: 81.91%, Recall: 42.08%, Fscore: 55.60\n",
      "\n",
      "              ORG: Precision: 88.12%, Recall: 57.55%, Fscore: 69.63\n",
      "\n",
      "              PER: Precision: 84.62%, Recall: 37.93%, Fscore: 52.38\n",
      "\n",
      "Best F1 score: 60.450\n",
      "Epoch 25/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.5950 - crf_accuracy: 0.9098 - val_loss: 22.6890 - val_crf_accuracy: 0.9185\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 440 phrases, correct: 338.\n",
      "\n",
      "Accuracy: 89.70%, Precision: 76.82%, Recall: 53.57%, Fscore: 63.12\n",
      "\n",
      "              LOC: Precision: 76.79%, Recall: 46.99%, Fscore: 58.31\n",
      "\n",
      "              ORG: Precision: 76.42%, Recall: 66.12%, Fscore: 70.90\n",
      "\n",
      "              PER: Precision: 77.59%, Recall: 44.33%, Fscore: 56.43\n",
      "\n",
      "Best F1 score: 63.120\n",
      "Epoch 26/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.5915 - crf_accuracy: 0.9098 - val_loss: 22.6819 - val_crf_accuracy: 0.9210\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 420 phrases, correct: 334.\n",
      "\n",
      "Accuracy: 89.91%, Precision: 79.52%, Recall: 52.93%, Fscore: 63.56\n",
      "\n",
      "              LOC: Precision: 74.81%, Recall: 53.55%, Fscore: 62.42\n",
      "\n",
      "              ORG: Precision: 84.36%, Recall: 61.63%, Fscore: 71.23\n",
      "\n",
      "              PER: Precision: 77.27%, Recall: 41.87%, Fscore: 54.31\n",
      "\n",
      "Best F1 score: 63.560\n",
      "Epoch 27/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.5855 - crf_accuracy: 0.9114 - val_loss: 22.6798 - val_crf_accuracy: 0.9182\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 353 phrases, correct: 296.\n",
      "\n",
      "Accuracy: 89.28%, Precision: 83.85%, Recall: 46.91%, Fscore: 60.16\n",
      "\n",
      "              LOC: Precision: 83.72%, Recall: 39.34%, Fscore: 53.53\n",
      "\n",
      "              ORG: Precision: 85.03%, Recall: 57.96%, Fscore: 68.93\n",
      "\n",
      "              PER: Precision: 82.00%, Recall: 40.39%, Fscore: 54.13\n",
      "\n",
      "Best F1 score: 60.160\n",
      "Epoch 28/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.5824 - crf_accuracy: 0.9123 - val_loss: 22.6777 - val_crf_accuracy: 0.9180\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 372 phrases, correct: 312.\n",
      "\n",
      "Accuracy: 89.70%, Precision: 83.87%, Recall: 49.45%, Fscore: 62.21\n",
      "\n",
      "              LOC: Precision: 79.25%, Recall: 45.90%, Fscore: 58.13\n",
      "\n",
      "              ORG: Precision: 85.80%, Recall: 59.18%, Fscore: 70.05\n",
      "\n",
      "              PER: Precision: 85.57%, Recall: 40.89%, Fscore: 55.33\n",
      "\n",
      "Best F1 score: 62.210\n",
      "Epoch 29/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.5776 - crf_accuracy: 0.9138 - val_loss: 22.6729 - val_crf_accuracy: 0.9203\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 376 phrases, correct: 316.\n",
      "\n",
      "Accuracy: 89.79%, Precision: 84.04%, Recall: 50.08%, Fscore: 62.76\n",
      "\n",
      "              LOC: Precision: 87.21%, Recall: 40.98%, Fscore: 55.76\n",
      "\n",
      "              ORG: Precision: 83.42%, Recall: 63.67%, Fscore: 72.22\n",
      "\n",
      "              PER: Precision: 82.52%, Recall: 41.87%, Fscore: 55.56\n",
      "\n",
      "Best F1 score: 62.760\n",
      "Epoch 30/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.5739 - crf_accuracy: 0.9149 - val_loss: 22.6695 - val_crf_accuracy: 0.9239\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 437 phrases, correct: 352.\n",
      "\n",
      "Accuracy: 90.42%, Precision: 80.55%, Recall: 55.78%, Fscore: 65.92\n",
      "\n",
      "              LOC: Precision: 74.82%, Recall: 56.83%, Fscore: 64.60\n",
      "\n",
      "              ORG: Precision: 82.56%, Recall: 65.71%, Fscore: 73.18\n",
      "\n",
      "              PER: Precision: 84.47%, Recall: 42.86%, Fscore: 56.86\n",
      "\n",
      "Best F1 score: 65.920\n",
      "Epoch 31/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.5701 - crf_accuracy: 0.9157 - val_loss: 22.6672 - val_crf_accuracy: 0.9249\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 429 phrases, correct: 354.\n",
      "\n",
      "Accuracy: 90.63%, Precision: 82.52%, Recall: 56.10%, Fscore: 66.79\n",
      "\n",
      "              LOC: Precision: 77.10%, Recall: 55.19%, Fscore: 64.33\n",
      "\n",
      "              ORG: Precision: 84.74%, Recall: 65.71%, Fscore: 74.02\n",
      "\n",
      "              PER: Precision: 85.19%, Recall: 45.32%, Fscore: 59.16\n",
      "\n",
      "Best F1 score: 66.790\n",
      "Epoch 32/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.5673 - crf_accuracy: 0.9165 - val_loss: 22.6624 - val_crf_accuracy: 0.9258\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 442 phrases, correct: 357.\n",
      "\n",
      "Accuracy: 90.54%, Precision: 80.77%, Recall: 56.58%, Fscore: 66.54\n",
      "\n",
      "              LOC: Precision: 79.83%, Recall: 51.91%, Fscore: 62.91\n",
      "\n",
      "              ORG: Precision: 80.29%, Recall: 68.16%, Fscore: 73.73\n",
      "\n",
      "              PER: Precision: 82.61%, Recall: 46.80%, Fscore: 59.75\n",
      "\n",
      "Best F1 score: 66.540\n",
      "Epoch 33/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.5639 - crf_accuracy: 0.9179 - val_loss: 22.6648 - val_crf_accuracy: 0.9227\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 475 phrases, correct: 374.\n",
      "\n",
      "Accuracy: 90.78%, Precision: 78.74%, Recall: 59.27%, Fscore: 67.63\n",
      "\n",
      "              LOC: Precision: 82.65%, Recall: 44.26%, Fscore: 57.65\n",
      "\n",
      "              ORG: Precision: 80.73%, Recall: 71.84%, Fscore: 76.03\n",
      "\n",
      "              PER: Precision: 73.58%, Recall: 57.64%, Fscore: 64.64\n",
      "\n",
      "Best F1 score: 67.630\n",
      "Epoch 34/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.5624 - crf_accuracy: 0.9172 - val_loss: 22.6603 - val_crf_accuracy: 0.9223\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 376 phrases, correct: 325.\n",
      "\n",
      "Accuracy: 90.21%, Precision: 86.44%, Recall: 51.51%, Fscore: 64.55\n",
      "\n",
      "              LOC: Precision: 86.46%, Recall: 45.36%, Fscore: 59.50\n",
      "\n",
      "              ORG: Precision: 90.36%, Recall: 61.22%, Fscore: 72.99\n",
      "\n",
      "              PER: Precision: 80.70%, Recall: 45.32%, Fscore: 58.04\n",
      "\n",
      "Best F1 score: 64.550\n",
      "Epoch 35/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.5590 - crf_accuracy: 0.9182 - val_loss: 22.6564 - val_crf_accuracy: 0.9261\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 450 phrases, correct: 359.\n",
      "\n",
      "Accuracy: 90.57%, Precision: 79.78%, Recall: 56.89%, Fscore: 66.42\n",
      "\n",
      "              LOC: Precision: 84.00%, Recall: 45.90%, Fscore: 59.36\n",
      "\n",
      "              ORG: Precision: 77.53%, Recall: 71.84%, Fscore: 74.58\n",
      "\n",
      "              PER: Precision: 80.49%, Recall: 48.77%, Fscore: 60.74\n",
      "\n",
      "Best F1 score: 66.420\n",
      "Epoch 36/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.5557 - crf_accuracy: 0.9202 - val_loss: 22.6559 - val_crf_accuracy: 0.9247\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 388 phrases, correct: 337.\n",
      "\n",
      "Accuracy: 90.48%, Precision: 86.86%, Recall: 53.41%, Fscore: 66.14\n",
      "\n",
      "              LOC: Precision: 85.71%, Recall: 49.18%, Fscore: 62.50\n",
      "\n",
      "              ORG: Precision: 86.67%, Recall: 63.67%, Fscore: 73.41\n",
      "\n",
      "              PER: Precision: 88.35%, Recall: 44.83%, Fscore: 59.48\n",
      "\n",
      "Best F1 score: 66.140\n",
      "Epoch 37/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.5528 - crf_accuracy: 0.9210 - val_loss: 22.6514 - val_crf_accuracy: 0.9283\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 460 phrases, correct: 378.\n",
      "\n",
      "Accuracy: 91.14%, Precision: 82.17%, Recall: 59.90%, Fscore: 69.29\n",
      "\n",
      "              LOC: Precision: 79.69%, Recall: 55.74%, Fscore: 65.59\n",
      "\n",
      "              ORG: Precision: 81.57%, Recall: 72.24%, Fscore: 76.62\n",
      "\n",
      "              PER: Precision: 86.09%, Recall: 48.77%, Fscore: 62.26\n",
      "\n",
      "Best F1 score: 69.290\n",
      "Epoch 38/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.5506 - crf_accuracy: 0.9211 - val_loss: 22.6547 - val_crf_accuracy: 0.9228\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 401 phrases, correct: 345.\n",
      "\n",
      "Accuracy: 90.78%, Precision: 86.03%, Recall: 54.68%, Fscore: 66.86\n",
      "\n",
      "              LOC: Precision: 86.60%, Recall: 45.90%, Fscore: 60.00\n",
      "\n",
      "              ORG: Precision: 88.40%, Recall: 65.31%, Fscore: 75.12\n",
      "\n",
      "              PER: Precision: 82.11%, Recall: 49.75%, Fscore: 61.96\n",
      "\n",
      "Best F1 score: 66.860\n",
      "Epoch 39/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.5481 - crf_accuracy: 0.9221 - val_loss: 22.6476 - val_crf_accuracy: 0.9289\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 423 phrases, correct: 362.\n",
      "\n",
      "Accuracy: 91.05%, Precision: 85.58%, Recall: 57.37%, Fscore: 68.69\n",
      "\n",
      "              LOC: Precision: 83.74%, Recall: 56.28%, Fscore: 67.32\n",
      "\n",
      "              ORG: Precision: 85.28%, Recall: 68.57%, Fscore: 76.02\n",
      "\n",
      "              PER: Precision: 88.35%, Recall: 44.83%, Fscore: 59.48\n",
      "\n",
      "Best F1 score: 68.690\n",
      "Epoch 40/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.5450 - crf_accuracy: 0.9234 - val_loss: 22.6457 - val_crf_accuracy: 0.9294\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 447 phrases, correct: 383.\n",
      "\n",
      "Accuracy: 91.70%, Precision: 85.68%, Recall: 60.70%, Fscore: 71.06\n",
      "\n",
      "              LOC: Precision: 81.12%, Recall: 63.39%, Fscore: 71.17\n",
      "\n",
      "              ORG: Precision: 88.24%, Recall: 67.35%, Fscore: 76.39\n",
      "\n",
      "              PER: Precision: 87.18%, Recall: 50.25%, Fscore: 63.75\n",
      "\n",
      "Best F1 score: 71.060\n",
      "Epoch 41/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.5434 - crf_accuracy: 0.9244 - val_loss: 22.6429 - val_crf_accuracy: 0.9321\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 471 phrases, correct: 390.\n",
      "\n",
      "Accuracy: 91.52%, Precision: 82.80%, Recall: 61.81%, Fscore: 70.78\n",
      "\n",
      "              LOC: Precision: 80.60%, Recall: 59.02%, Fscore: 68.14\n",
      "\n",
      "              ORG: Precision: 82.30%, Recall: 70.20%, Fscore: 75.77\n",
      "\n",
      "              PER: Precision: 85.94%, Recall: 54.19%, Fscore: 66.47\n",
      "\n",
      "Best F1 score: 70.780\n",
      "Epoch 42/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.5405 - crf_accuracy: 0.9250 - val_loss: 22.6417 - val_crf_accuracy: 0.9321\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 454 phrases, correct: 384.\n",
      "\n",
      "Accuracy: 91.52%, Precision: 84.58%, Recall: 60.86%, Fscore: 70.78\n",
      "\n",
      "              LOC: Precision: 78.95%, Recall: 65.57%, Fscore: 71.64\n",
      "\n",
      "              ORG: Precision: 86.29%, Recall: 69.39%, Fscore: 76.92\n",
      "\n",
      "              PER: Precision: 89.52%, Recall: 46.31%, Fscore: 61.04\n",
      "\n",
      "Best F1 score: 70.780\n",
      "Epoch 43/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.5383 - crf_accuracy: 0.9254 - val_loss: 22.6395 - val_crf_accuracy: 0.9331\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 447 phrases, correct: 378.\n",
      "\n",
      "Accuracy: 91.43%, Precision: 84.56%, Recall: 59.90%, Fscore: 70.13\n",
      "\n",
      "              LOC: Precision: 84.38%, Recall: 59.02%, Fscore: 69.45\n",
      "\n",
      "              ORG: Precision: 82.69%, Recall: 70.20%, Fscore: 75.94\n",
      "\n",
      "              PER: Precision: 88.29%, Recall: 48.28%, Fscore: 62.42\n",
      "\n",
      "Best F1 score: 70.130\n",
      "Epoch 44/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.5376 - crf_accuracy: 0.9257 - val_loss: 22.6433 - val_crf_accuracy: 0.9280\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 419 phrases, correct: 364.\n",
      "\n",
      "Accuracy: 91.31%, Precision: 86.87%, Recall: 57.69%, Fscore: 69.33\n",
      "\n",
      "              LOC: Precision: 85.34%, Recall: 54.10%, Fscore: 66.22\n",
      "\n",
      "              ORG: Precision: 89.07%, Recall: 66.53%, Fscore: 76.17\n",
      "\n",
      "              PER: Precision: 85.00%, Recall: 50.25%, Fscore: 63.16\n",
      "\n",
      "Best F1 score: 69.330\n",
      "Epoch 45/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5347 - crf_accuracy: 0.9274 - val_loss: 22.6366 - val_crf_accuracy: 0.9332\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 439 phrases, correct: 379.\n",
      "\n",
      "Accuracy: 91.61%, Precision: 86.33%, Recall: 60.06%, Fscore: 70.84\n",
      "\n",
      "              LOC: Precision: 83.08%, Recall: 59.02%, Fscore: 69.01\n",
      "\n",
      "              ORG: Precision: 88.95%, Recall: 65.71%, Fscore: 75.59\n",
      "\n",
      "              PER: Precision: 85.94%, Recall: 54.19%, Fscore: 66.47\n",
      "\n",
      "Best F1 score: 70.840\n",
      "Epoch 46/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5321 - crf_accuracy: 0.9288 - val_loss: 22.6348 - val_crf_accuracy: 0.9346\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 466 phrases, correct: 396.\n",
      "\n",
      "Accuracy: 91.82%, Precision: 84.98%, Recall: 62.76%, Fscore: 72.20\n",
      "\n",
      "              LOC: Precision: 81.51%, Recall: 65.03%, Fscore: 72.34\n",
      "\n",
      "              ORG: Precision: 84.73%, Recall: 70.20%, Fscore: 76.79\n",
      "\n",
      "              PER: Precision: 89.74%, Recall: 51.72%, Fscore: 65.63\n",
      "\n",
      "Best F1 score: 72.200\n",
      "Epoch 47/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5302 - crf_accuracy: 0.9292 - val_loss: 22.6477 - val_crf_accuracy: 0.9184\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 540 phrases, correct: 408.\n",
      "\n",
      "Accuracy: 91.25%, Precision: 75.56%, Recall: 64.66%, Fscore: 69.68\n",
      "\n",
      "              LOC: Precision: 78.10%, Recall: 58.47%, Fscore: 66.88\n",
      "\n",
      "              ORG: Precision: 69.42%, Recall: 78.78%, Fscore: 73.80\n",
      "\n",
      "              PER: Precision: 86.40%, Recall: 53.20%, Fscore: 65.85\n",
      "\n",
      "Best F1 score: 69.680\n",
      "Epoch 48/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5299 - crf_accuracy: 0.9284 - val_loss: 22.6352 - val_crf_accuracy: 0.9311\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 435 phrases, correct: 383.\n",
      "\n",
      "Accuracy: 91.88%, Precision: 88.05%, Recall: 60.70%, Fscore: 71.86\n",
      "\n",
      "              LOC: Precision: 86.18%, Recall: 57.92%, Fscore: 69.28\n",
      "\n",
      "              ORG: Precision: 89.36%, Recall: 68.57%, Fscore: 77.60\n",
      "\n",
      "              PER: Precision: 87.90%, Recall: 53.69%, Fscore: 66.67\n",
      "\n",
      "Best F1 score: 71.860\n",
      "Epoch 49/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5273 - crf_accuracy: 0.9299 - val_loss: 22.6321 - val_crf_accuracy: 0.9339\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 429 phrases, correct: 378.\n",
      "\n",
      "Accuracy: 91.76%, Precision: 88.11%, Recall: 59.90%, Fscore: 71.32\n",
      "\n",
      "              LOC: Precision: 83.57%, Recall: 63.93%, Fscore: 72.45\n",
      "\n",
      "              ORG: Precision: 90.16%, Recall: 67.35%, Fscore: 77.10\n",
      "\n",
      "              PER: Precision: 90.57%, Recall: 47.29%, Fscore: 62.14\n",
      "\n",
      "Best F1 score: 71.320\n",
      "Epoch 50/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5255 - crf_accuracy: 0.9307 - val_loss: 22.6320 - val_crf_accuracy: 0.9324\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 483 phrases, correct: 401.\n",
      "\n",
      "Accuracy: 92.03%, Precision: 83.02%, Recall: 63.55%, Fscore: 71.99\n",
      "\n",
      "              LOC: Precision: 88.39%, Recall: 54.10%, Fscore: 67.12\n",
      "\n",
      "              ORG: Precision: 77.64%, Recall: 77.96%, Fscore: 77.80\n",
      "\n",
      "              PER: Precision: 88.80%, Recall: 54.68%, Fscore: 67.68\n",
      "\n",
      "Best F1 score: 71.990\n",
      "Epoch 51/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.5246 - crf_accuracy: 0.9310 - val_loss: 22.6303 - val_crf_accuracy: 0.9337\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 434 phrases, correct: 383.\n",
      "\n",
      "Accuracy: 91.91%, Precision: 88.25%, Recall: 60.70%, Fscore: 71.92\n",
      "\n",
      "              LOC: Precision: 86.89%, Recall: 57.92%, Fscore: 69.51\n",
      "\n",
      "              ORG: Precision: 88.08%, Recall: 69.39%, Fscore: 77.63\n",
      "\n",
      "              PER: Precision: 89.92%, Recall: 52.71%, Fscore: 66.46\n",
      "\n",
      "Best F1 score: 71.920\n",
      "Epoch 52/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5214 - crf_accuracy: 0.9332 - val_loss: 22.6258 - val_crf_accuracy: 0.9376\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 453 phrases, correct: 402.\n",
      "\n",
      "Accuracy: 92.48%, Precision: 88.74%, Recall: 63.71%, Fscore: 74.17\n",
      "\n",
      "              LOC: Precision: 86.23%, Recall: 65.03%, Fscore: 74.14\n",
      "\n",
      "              ORG: Precision: 89.12%, Recall: 70.20%, Fscore: 78.54\n",
      "\n",
      "              PER: Precision: 90.98%, Recall: 54.68%, Fscore: 68.31\n",
      "\n",
      "Best F1 score: 74.170\n",
      "Epoch 53/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5199 - crf_accuracy: 0.9334 - val_loss: 22.6276 - val_crf_accuracy: 0.9350\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 442 phrases, correct: 394.\n",
      "\n",
      "Accuracy: 92.27%, Precision: 89.14%, Recall: 62.44%, Fscore: 73.44\n",
      "\n",
      "              LOC: Precision: 86.57%, Recall: 63.39%, Fscore: 73.19\n",
      "\n",
      "              ORG: Precision: 90.81%, Recall: 68.57%, Fscore: 78.14\n",
      "\n",
      "              PER: Precision: 89.43%, Recall: 54.19%, Fscore: 67.48\n",
      "\n",
      "Best F1 score: 73.440\n",
      "Epoch 54/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5188 - crf_accuracy: 0.9338 - val_loss: 22.6235 - val_crf_accuracy: 0.9385\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 465 phrases, correct: 406.\n",
      "\n",
      "Accuracy: 92.48%, Precision: 87.31%, Recall: 64.34%, Fscore: 74.09\n",
      "\n",
      "              LOC: Precision: 85.40%, Recall: 63.93%, Fscore: 73.12\n",
      "\n",
      "              ORG: Precision: 87.62%, Recall: 72.24%, Fscore: 79.19\n",
      "\n",
      "              PER: Precision: 88.89%, Recall: 55.17%, Fscore: 68.09\n",
      "\n",
      "Best F1 score: 74.090\n",
      "Epoch 55/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5162 - crf_accuracy: 0.9351 - val_loss: 22.6236 - val_crf_accuracy: 0.9382\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 446 phrases, correct: 401.\n",
      "\n",
      "Accuracy: 92.60%, Precision: 89.91%, Recall: 63.55%, Fscore: 74.47\n",
      "\n",
      "              LOC: Precision: 87.31%, Recall: 63.93%, Fscore: 73.82\n",
      "\n",
      "              ORG: Precision: 91.05%, Recall: 70.61%, Fscore: 79.54\n",
      "\n",
      "              PER: Precision: 90.98%, Recall: 54.68%, Fscore: 68.31\n",
      "\n",
      "Best F1 score: 74.470\n",
      "Epoch 56/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5152 - crf_accuracy: 0.9356 - val_loss: 22.6271 - val_crf_accuracy: 0.9349\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 526 phrases, correct: 429.\n",
      "\n",
      "Accuracy: 92.42%, Precision: 81.56%, Recall: 67.99%, Fscore: 74.16\n",
      "\n",
      "              LOC: Precision: 83.10%, Recall: 64.48%, Fscore: 72.62\n",
      "\n",
      "              ORG: Precision: 77.24%, Recall: 77.55%, Fscore: 77.39\n",
      "\n",
      "              PER: Precision: 87.68%, Recall: 59.61%, Fscore: 70.97\n",
      "\n",
      "Best F1 score: 74.160\n",
      "Epoch 57/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5141 - crf_accuracy: 0.9358 - val_loss: 22.6247 - val_crf_accuracy: 0.9368\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 443 phrases, correct: 388.\n",
      "\n",
      "Accuracy: 92.15%, Precision: 87.58%, Recall: 61.49%, Fscore: 72.25\n",
      "\n",
      "              LOC: Precision: 79.11%, Recall: 68.31%, Fscore: 73.31\n",
      "\n",
      "              ORG: Precision: 91.48%, Recall: 65.71%, Fscore: 76.48\n",
      "\n",
      "              PER: Precision: 93.58%, Recall: 50.25%, Fscore: 65.38\n",
      "\n",
      "Best F1 score: 72.250\n",
      "Epoch 58/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5122 - crf_accuracy: 0.9367 - val_loss: 22.6203 - val_crf_accuracy: 0.9400\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 458 phrases, correct: 403.\n",
      "\n",
      "Accuracy: 92.48%, Precision: 87.99%, Recall: 63.87%, Fscore: 74.01\n",
      "\n",
      "              LOC: Precision: 85.61%, Recall: 65.03%, Fscore: 73.91\n",
      "\n",
      "              ORG: Precision: 86.83%, Recall: 72.65%, Fscore: 79.11\n",
      "\n",
      "              PER: Precision: 92.98%, Recall: 52.22%, Fscore: 66.88\n",
      "\n",
      "Best F1 score: 74.010\n",
      "Epoch 59/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5103 - crf_accuracy: 0.9375 - val_loss: 22.6173 - val_crf_accuracy: 0.9419\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 490 phrases, correct: 428.\n",
      "\n",
      "Accuracy: 93.08%, Precision: 87.35%, Recall: 67.83%, Fscore: 76.36\n",
      "\n",
      "              LOC: Precision: 81.37%, Recall: 71.58%, Fscore: 76.16\n",
      "\n",
      "              ORG: Precision: 89.50%, Recall: 73.06%, Fscore: 80.45\n",
      "\n",
      "              PER: Precision: 91.47%, Recall: 58.13%, Fscore: 71.08\n",
      "\n",
      "Best F1 score: 76.360\n",
      "Epoch 60/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5090 - crf_accuracy: 0.9383 - val_loss: 22.6163 - val_crf_accuracy: 0.9421\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 472 phrases, correct: 415.\n",
      "\n",
      "Accuracy: 92.93%, Precision: 87.92%, Recall: 65.77%, Fscore: 75.25\n",
      "\n",
      "              LOC: Precision: 82.99%, Recall: 66.67%, Fscore: 73.94\n",
      "\n",
      "              ORG: Precision: 91.98%, Recall: 70.20%, Fscore: 79.63\n",
      "\n",
      "              PER: Precision: 87.68%, Recall: 59.61%, Fscore: 70.97\n",
      "\n",
      "Best F1 score: 75.250\n",
      "Epoch 61/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5081 - crf_accuracy: 0.9387 - val_loss: 22.6161 - val_crf_accuracy: 0.9408\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 516 phrases, correct: 432.\n",
      "\n",
      "Accuracy: 92.75%, Precision: 83.72%, Recall: 68.46%, Fscore: 75.33\n",
      "\n",
      "              LOC: Precision: 81.17%, Recall: 68.31%, Fscore: 74.18\n",
      "\n",
      "              ORG: Precision: 82.10%, Recall: 76.73%, Fscore: 79.32\n",
      "\n",
      "              PER: Precision: 89.47%, Recall: 58.62%, Fscore: 70.83\n",
      "\n",
      "Best F1 score: 75.330\n",
      "Epoch 62/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5069 - crf_accuracy: 0.9390 - val_loss: 22.6160 - val_crf_accuracy: 0.9421\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 476 phrases, correct: 414.\n",
      "\n",
      "Accuracy: 92.84%, Precision: 86.97%, Recall: 65.61%, Fscore: 74.80\n",
      "\n",
      "              LOC: Precision: 78.57%, Recall: 72.13%, Fscore: 75.21\n",
      "\n",
      "              ORG: Precision: 89.64%, Recall: 70.61%, Fscore: 79.00\n",
      "\n",
      "              PER: Precision: 94.78%, Recall: 53.69%, Fscore: 68.55\n",
      "\n",
      "Best F1 score: 74.800\n",
      "Epoch 63/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5048 - crf_accuracy: 0.9401 - val_loss: 22.6138 - val_crf_accuracy: 0.9439\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 500 phrases, correct: 427.\n",
      "\n",
      "Accuracy: 92.93%, Precision: 85.40%, Recall: 67.67%, Fscore: 75.51\n",
      "\n",
      "              LOC: Precision: 81.94%, Recall: 69.40%, Fscore: 75.15\n",
      "\n",
      "              ORG: Precision: 83.33%, Recall: 75.51%, Fscore: 79.23\n",
      "\n",
      "              PER: Precision: 93.50%, Recall: 56.65%, Fscore: 70.55\n",
      "\n",
      "Best F1 score: 75.510\n",
      "Epoch 64/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5033 - crf_accuracy: 0.9410 - val_loss: 22.6123 - val_crf_accuracy: 0.9435\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 495 phrases, correct: 428.\n",
      "\n",
      "Accuracy: 93.08%, Precision: 86.46%, Recall: 67.83%, Fscore: 76.02\n",
      "\n",
      "              LOC: Precision: 80.49%, Recall: 72.13%, Fscore: 76.08\n",
      "\n",
      "              ORG: Precision: 88.56%, Recall: 72.65%, Fscore: 79.82\n",
      "\n",
      "              PER: Precision: 90.77%, Recall: 58.13%, Fscore: 70.87\n",
      "\n",
      "Best F1 score: 76.020\n",
      "Epoch 65/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5019 - crf_accuracy: 0.9416 - val_loss: 22.6166 - val_crf_accuracy: 0.9397\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 458 phrases, correct: 406.\n",
      "\n",
      "Accuracy: 92.69%, Precision: 88.65%, Recall: 64.34%, Fscore: 74.56\n",
      "\n",
      "              LOC: Precision: 83.67%, Recall: 67.21%, Fscore: 74.55\n",
      "\n",
      "              ORG: Precision: 89.53%, Recall: 69.80%, Fscore: 78.44\n",
      "\n",
      "              PER: Precision: 93.33%, Recall: 55.17%, Fscore: 69.35\n",
      "\n",
      "Best F1 score: 74.560\n",
      "Epoch 66/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5009 - crf_accuracy: 0.9422 - val_loss: 22.6113 - val_crf_accuracy: 0.9440\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 507 phrases, correct: 434.\n",
      "\n",
      "Accuracy: 93.14%, Precision: 85.60%, Recall: 68.78%, Fscore: 76.27\n",
      "\n",
      "              LOC: Precision: 79.41%, Recall: 73.77%, Fscore: 76.49\n",
      "\n",
      "              ORG: Precision: 87.75%, Recall: 73.06%, Fscore: 79.73\n",
      "\n",
      "              PER: Precision: 90.23%, Recall: 59.11%, Fscore: 71.43\n",
      "\n",
      "Best F1 score: 76.270\n",
      "Epoch 67/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.5000 - crf_accuracy: 0.9430 - val_loss: 22.6161 - val_crf_accuracy: 0.9403\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 456 phrases, correct: 413.\n",
      "\n",
      "Accuracy: 92.99%, Precision: 90.57%, Recall: 65.45%, Fscore: 75.99\n",
      "\n",
      "              LOC: Precision: 87.14%, Recall: 66.67%, Fscore: 75.54\n",
      "\n",
      "              ORG: Precision: 90.58%, Recall: 70.61%, Fscore: 79.36\n",
      "\n",
      "              PER: Precision: 94.40%, Recall: 58.13%, Fscore: 71.95\n",
      "\n",
      "Best F1 score: 75.990\n",
      "Epoch 68/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.4988 - crf_accuracy: 0.9433 - val_loss: 22.6096 - val_crf_accuracy: 0.9461\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 493 phrases, correct: 430.\n",
      "\n",
      "Accuracy: 93.23%, Precision: 87.22%, Recall: 68.15%, Fscore: 76.51\n",
      "\n",
      "              LOC: Precision: 83.33%, Recall: 71.04%, Fscore: 76.70\n",
      "\n",
      "              ORG: Precision: 86.32%, Recall: 74.69%, Fscore: 80.09\n",
      "\n",
      "              PER: Precision: 93.60%, Recall: 57.64%, Fscore: 71.34\n",
      "\n",
      "Best F1 score: 76.510\n",
      "Epoch 69/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.4970 - crf_accuracy: 0.9441 - val_loss: 22.6108 - val_crf_accuracy: 0.9440\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 494 phrases, correct: 428.\n",
      "\n",
      "Accuracy: 93.17%, Precision: 86.64%, Recall: 67.83%, Fscore: 76.09\n",
      "\n",
      "              LOC: Precision: 80.72%, Recall: 73.22%, Fscore: 76.79\n",
      "\n",
      "              ORG: Precision: 86.89%, Recall: 73.06%, Fscore: 79.38\n",
      "\n",
      "              PER: Precision: 94.26%, Recall: 56.65%, Fscore: 70.77\n",
      "\n",
      "Best F1 score: 76.090\n",
      "Epoch 70/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.4966 - crf_accuracy: 0.9440 - val_loss: 22.6093 - val_crf_accuracy: 0.9438\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 512 phrases, correct: 437.\n",
      "\n",
      "Accuracy: 93.23%, Precision: 85.35%, Recall: 69.26%, Fscore: 76.47\n",
      "\n",
      "              LOC: Precision: 85.71%, Recall: 65.57%, Fscore: 74.30\n",
      "\n",
      "              ORG: Precision: 82.35%, Recall: 80.00%, Fscore: 81.16\n",
      "\n",
      "              PER: Precision: 90.30%, Recall: 59.61%, Fscore: 71.81\n",
      "\n",
      "Best F1 score: 76.470\n",
      "Epoch 71/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.4948 - crf_accuracy: 0.9452 - val_loss: 22.6077 - val_crf_accuracy: 0.9458\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 479 phrases, correct: 416.\n",
      "\n",
      "Accuracy: 92.84%, Precision: 86.85%, Recall: 65.93%, Fscore: 74.95\n",
      "\n",
      "              LOC: Precision: 84.83%, Recall: 67.21%, Fscore: 75.00\n",
      "\n",
      "              ORG: Precision: 84.26%, Recall: 74.29%, Fscore: 78.96\n",
      "\n",
      "              PER: Precision: 94.07%, Recall: 54.68%, Fscore: 69.16\n",
      "\n",
      "Best F1 score: 74.950\n",
      "Epoch 72/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 73s 4ms/step - loss: 22.4939 - crf_accuracy: 0.9452 - val_loss: 22.6124 - val_crf_accuracy: 0.9425\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 449 phrases, correct: 405.\n",
      "\n",
      "Accuracy: 92.84%, Precision: 90.20%, Recall: 64.18%, Fscore: 75.00\n",
      "\n",
      "              LOC: Precision: 84.56%, Recall: 68.85%, Fscore: 75.90\n",
      "\n",
      "              ORG: Precision: 92.22%, Recall: 67.76%, Fscore: 78.12\n",
      "\n",
      "              PER: Precision: 94.17%, Recall: 55.67%, Fscore: 69.97\n",
      "\n",
      "Best F1 score: 75.000\n",
      "Epoch 73/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.4929 - crf_accuracy: 0.9463 - val_loss: 22.6097 - val_crf_accuracy: 0.9444\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 489 phrases, correct: 427.\n",
      "\n",
      "Accuracy: 93.20%, Precision: 87.32%, Recall: 67.67%, Fscore: 76.25\n",
      "\n",
      "              LOC: Precision: 85.62%, Recall: 68.31%, Fscore: 75.99\n",
      "\n",
      "              ORG: Precision: 86.79%, Recall: 75.10%, Fscore: 80.53\n",
      "\n",
      "              PER: Precision: 90.08%, Recall: 58.13%, Fscore: 70.66\n",
      "\n",
      "Best F1 score: 76.250\n",
      "Epoch 74/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.4916 - crf_accuracy: 0.9470 - val_loss: 22.6077 - val_crf_accuracy: 0.9458\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 478 phrases, correct: 416.\n",
      "\n",
      "Accuracy: 92.90%, Precision: 87.03%, Recall: 65.93%, Fscore: 75.02\n",
      "\n",
      "              LOC: Precision: 80.12%, Recall: 74.86%, Fscore: 77.40\n",
      "\n",
      "              ORG: Precision: 88.54%, Recall: 69.39%, Fscore: 77.80\n",
      "\n",
      "              PER: Precision: 94.78%, Recall: 53.69%, Fscore: 68.55\n",
      "\n",
      "Best F1 score: 75.020\n",
      "Epoch 75/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.4901 - crf_accuracy: 0.9477 - val_loss: 22.6047 - val_crf_accuracy: 0.9465\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 508 phrases, correct: 444.\n",
      "\n",
      "Accuracy: 93.62%, Precision: 87.40%, Recall: 70.36%, Fscore: 77.96\n",
      "\n",
      "              LOC: Precision: 84.42%, Recall: 71.04%, Fscore: 77.15\n",
      "\n",
      "              ORG: Precision: 88.02%, Recall: 77.96%, Fscore: 82.68\n",
      "\n",
      "              PER: Precision: 89.78%, Recall: 60.59%, Fscore: 72.35\n",
      "\n",
      "Best F1 score: 77.960\n",
      "Epoch 76/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.4895 - crf_accuracy: 0.9475 - val_loss: 22.6062 - val_crf_accuracy: 0.9462\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 491 phrases, correct: 433.\n",
      "\n",
      "Accuracy: 93.44%, Precision: 88.19%, Recall: 68.62%, Fscore: 77.18\n",
      "\n",
      "              LOC: Precision: 83.65%, Recall: 72.68%, Fscore: 77.78\n",
      "\n",
      "              ORG: Precision: 87.56%, Recall: 74.69%, Fscore: 80.62\n",
      "\n",
      "              PER: Precision: 95.12%, Recall: 57.64%, Fscore: 71.78\n",
      "\n",
      "Best F1 score: 77.180\n",
      "Epoch 77/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.4886 - crf_accuracy: 0.9484 - val_loss: 22.6043 - val_crf_accuracy: 0.9465\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 532 phrases, correct: 451.\n",
      "\n",
      "Accuracy: 93.50%, Precision: 84.77%, Recall: 71.47%, Fscore: 77.56\n",
      "\n",
      "              LOC: Precision: 84.67%, Recall: 69.40%, Fscore: 76.28\n",
      "\n",
      "              ORG: Precision: 82.35%, Recall: 80.00%, Fscore: 81.16\n",
      "\n",
      "              PER: Precision: 88.89%, Recall: 63.05%, Fscore: 73.78\n",
      "\n",
      "Best F1 score: 77.560\n",
      "Epoch 78/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.4874 - crf_accuracy: 0.9487 - val_loss: 22.6048 - val_crf_accuracy: 0.9471\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 479 phrases, correct: 423.\n",
      "\n",
      "Accuracy: 93.29%, Precision: 88.31%, Recall: 67.04%, Fscore: 76.22\n",
      "\n",
      "              LOC: Precision: 81.82%, Recall: 73.77%, Fscore: 77.59\n",
      "\n",
      "              ORG: Precision: 89.74%, Recall: 71.43%, Fscore: 79.55\n",
      "\n",
      "              PER: Precision: 94.96%, Recall: 55.67%, Fscore: 70.19\n",
      "\n",
      "Best F1 score: 76.220\n",
      "Epoch 79/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.4870 - crf_accuracy: 0.9493 - val_loss: 22.6028 - val_crf_accuracy: 0.9469\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 538 phrases, correct: 451.\n",
      "\n",
      "Accuracy: 93.41%, Precision: 83.83%, Recall: 71.47%, Fscore: 77.16\n",
      "\n",
      "              LOC: Precision: 79.53%, Recall: 74.32%, Fscore: 76.84\n",
      "\n",
      "              ORG: Precision: 82.33%, Recall: 77.96%, Fscore: 80.08\n",
      "\n",
      "              PER: Precision: 91.85%, Recall: 61.08%, Fscore: 73.37\n",
      "\n",
      "Best F1 score: 77.160\n",
      "Epoch 80/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.4862 - crf_accuracy: 0.9498 - val_loss: 22.6085 - val_crf_accuracy: 0.9459\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 485 phrases, correct: 437.\n",
      "\n",
      "Accuracy: 93.68%, Precision: 90.10%, Recall: 69.26%, Fscore: 78.32\n",
      "\n",
      "              LOC: Precision: 86.93%, Recall: 72.68%, Fscore: 79.17\n",
      "\n",
      "              ORG: Precision: 89.47%, Recall: 76.33%, Fscore: 82.38\n",
      "\n",
      "              PER: Precision: 95.12%, Recall: 57.64%, Fscore: 71.78\n",
      "\n",
      "Best F1 score: 78.320\n",
      "Epoch 81/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 72s 4ms/step - loss: 22.4854 - crf_accuracy: 0.9495 - val_loss: 22.6027 - val_crf_accuracy: 0.9486\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 527 phrases, correct: 452.\n",
      "\n",
      "Accuracy: 93.62%, Precision: 85.77%, Recall: 71.63%, Fscore: 78.07\n",
      "\n",
      "              LOC: Precision: 84.87%, Recall: 70.49%, Fscore: 77.01\n",
      "\n",
      "              ORG: Precision: 83.48%, Recall: 78.37%, Fscore: 80.84\n",
      "\n",
      "              PER: Precision: 90.34%, Recall: 64.53%, Fscore: 75.29\n",
      "\n",
      "Best F1 score: 78.070\n",
      "Epoch 82/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.4839 - crf_accuracy: 0.9507 - val_loss: 22.6038 - val_crf_accuracy: 0.9475\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 502 phrases, correct: 441.\n",
      "\n",
      "Accuracy: 93.59%, Precision: 87.85%, Recall: 69.89%, Fscore: 77.85\n",
      "\n",
      "              LOC: Precision: 84.42%, Recall: 71.04%, Fscore: 77.15\n",
      "\n",
      "              ORG: Precision: 86.18%, Recall: 76.33%, Fscore: 80.95\n",
      "\n",
      "              PER: Precision: 94.66%, Recall: 61.08%, Fscore: 74.25\n",
      "\n",
      "Best F1 score: 77.850\n",
      "Epoch 83/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.4828 - crf_accuracy: 0.9513 - val_loss: 22.6027 - val_crf_accuracy: 0.9482\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 529 phrases, correct: 446.\n",
      "\n",
      "Accuracy: 93.41%, Precision: 84.31%, Recall: 70.68%, Fscore: 76.90\n",
      "\n",
      "              LOC: Precision: 78.57%, Recall: 78.14%, Fscore: 78.36\n",
      "\n",
      "              ORG: Precision: 83.41%, Recall: 75.92%, Fscore: 79.49\n",
      "\n",
      "              PER: Precision: 94.35%, Recall: 57.64%, Fscore: 71.56\n",
      "\n",
      "Best F1 score: 76.900\n",
      "Epoch 84/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.4828 - crf_accuracy: 0.9506 - val_loss: 22.6008 - val_crf_accuracy: 0.9491\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 518 phrases, correct: 453.\n",
      "\n",
      "Accuracy: 94.01%, Precision: 87.45%, Recall: 71.79%, Fscore: 78.85\n",
      "\n",
      "              LOC: Precision: 83.03%, Recall: 74.86%, Fscore: 78.74\n",
      "\n",
      "              ORG: Precision: 89.66%, Recall: 74.29%, Fscore: 81.25\n",
      "\n",
      "              PER: Precision: 89.33%, Recall: 66.01%, Fscore: 75.92\n",
      "\n",
      "Best F1 score: 78.850\n",
      "Epoch 85/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.4816 - crf_accuracy: 0.9521 - val_loss: 22.6006 - val_crf_accuracy: 0.9500\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 551 phrases, correct: 462.\n",
      "\n",
      "Accuracy: 93.62%, Precision: 83.85%, Recall: 73.22%, Fscore: 78.17\n",
      "\n",
      "              LOC: Precision: 81.33%, Recall: 73.77%, Fscore: 77.36\n",
      "\n",
      "              ORG: Precision: 82.83%, Recall: 78.78%, Fscore: 80.75\n",
      "\n",
      "              PER: Precision: 88.16%, Recall: 66.01%, Fscore: 75.49\n",
      "\n",
      "Best F1 score: 78.170\n",
      "Epoch 86/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.4808 - crf_accuracy: 0.9523 - val_loss: 22.5998 - val_crf_accuracy: 0.9485\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 542 phrases, correct: 461.\n",
      "\n",
      "Accuracy: 93.83%, Precision: 85.06%, Recall: 73.06%, Fscore: 78.60\n",
      "\n",
      "              LOC: Precision: 82.42%, Recall: 74.32%, Fscore: 78.16\n",
      "\n",
      "              ORG: Precision: 83.48%, Recall: 78.37%, Fscore: 80.84\n",
      "\n",
      "              PER: Precision: 90.48%, Recall: 65.52%, Fscore: 76.00\n",
      "\n",
      "Best F1 score: 78.600\n",
      "Epoch 87/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.4792 - crf_accuracy: 0.9534 - val_loss: 22.6002 - val_crf_accuracy: 0.9506\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 546 phrases, correct: 466.\n",
      "\n",
      "Accuracy: 93.89%, Precision: 85.35%, Recall: 73.85%, Fscore: 79.18\n",
      "\n",
      "              LOC: Precision: 81.44%, Recall: 74.32%, Fscore: 77.71\n",
      "\n",
      "              ORG: Precision: 83.76%, Recall: 80.00%, Fscore: 81.84\n",
      "\n",
      "              PER: Precision: 92.41%, Recall: 66.01%, Fscore: 77.01\n",
      "\n",
      "Best F1 score: 79.180\n",
      "Epoch 88/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.4788 - crf_accuracy: 0.9533 - val_loss: 22.6012 - val_crf_accuracy: 0.9502\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 515 phrases, correct: 447.\n",
      "\n",
      "Accuracy: 93.68%, Precision: 86.80%, Recall: 70.84%, Fscore: 78.01\n",
      "\n",
      "              LOC: Precision: 84.38%, Recall: 73.77%, Fscore: 78.72\n",
      "\n",
      "              ORG: Precision: 84.21%, Recall: 78.37%, Fscore: 81.18\n",
      "\n",
      "              PER: Precision: 94.49%, Recall: 59.11%, Fscore: 72.73\n",
      "\n",
      "Best F1 score: 78.010\n",
      "Epoch 89/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.4774 - crf_accuracy: 0.9543 - val_loss: 22.6060 - val_crf_accuracy: 0.9464\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 487 phrases, correct: 434.\n",
      "\n",
      "Accuracy: 93.65%, Precision: 89.12%, Recall: 68.78%, Fscore: 77.64\n",
      "\n",
      "              LOC: Precision: 83.03%, Recall: 74.86%, Fscore: 78.74\n",
      "\n",
      "              ORG: Precision: 90.77%, Recall: 72.24%, Fscore: 80.45\n",
      "\n",
      "              PER: Precision: 94.49%, Recall: 59.11%, Fscore: 72.73\n",
      "\n",
      "Best F1 score: 77.640\n",
      "Epoch 90/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.4784 - crf_accuracy: 0.9529 - val_loss: 22.6005 - val_crf_accuracy: 0.9456\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 545 phrases, correct: 463.\n",
      "\n",
      "Accuracy: 93.89%, Precision: 84.95%, Recall: 73.38%, Fscore: 78.74\n",
      "\n",
      "              LOC: Precision: 83.65%, Recall: 72.68%, Fscore: 77.78\n",
      "\n",
      "              ORG: Precision: 81.27%, Recall: 83.27%, Fscore: 82.26\n",
      "\n",
      "              PER: Precision: 93.33%, Recall: 62.07%, Fscore: 74.56\n",
      "\n",
      "Best F1 score: 78.740\n",
      "Epoch 91/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.4762 - crf_accuracy: 0.9547 - val_loss: 22.5994 - val_crf_accuracy: 0.9486\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 547 phrases, correct: 467.\n",
      "\n",
      "Accuracy: 94.04%, Precision: 85.37%, Recall: 74.01%, Fscore: 79.29\n",
      "\n",
      "              LOC: Precision: 77.37%, Recall: 80.33%, Fscore: 78.82\n",
      "\n",
      "              ORG: Precision: 88.21%, Recall: 76.33%, Fscore: 81.84\n",
      "\n",
      "              PER: Precision: 91.72%, Recall: 65.52%, Fscore: 76.44\n",
      "\n",
      "Best F1 score: 79.290\n",
      "Epoch 92/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.4756 - crf_accuracy: 0.9552 - val_loss: 22.5984 - val_crf_accuracy: 0.9507\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 527 phrases, correct: 463.\n",
      "\n",
      "Accuracy: 94.19%, Precision: 87.86%, Recall: 73.38%, Fscore: 79.97\n",
      "\n",
      "              LOC: Precision: 86.54%, Recall: 73.77%, Fscore: 79.65\n",
      "\n",
      "              ORG: Precision: 86.49%, Recall: 78.37%, Fscore: 82.23\n",
      "\n",
      "              PER: Precision: 91.28%, Recall: 67.00%, Fscore: 77.27\n",
      "\n",
      "Best F1 score: 79.970\n",
      "Epoch 93/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.4744 - crf_accuracy: 0.9561 - val_loss: 22.5993 - val_crf_accuracy: 0.9504\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 537 phrases, correct: 463.\n",
      "\n",
      "Accuracy: 94.04%, Precision: 86.22%, Recall: 73.38%, Fscore: 79.28\n",
      "\n",
      "              LOC: Precision: 80.23%, Recall: 77.60%, Fscore: 78.89\n",
      "\n",
      "              ORG: Precision: 87.26%, Recall: 75.51%, Fscore: 80.96\n",
      "\n",
      "              PER: Precision: 91.89%, Recall: 67.00%, Fscore: 77.49\n",
      "\n",
      "Best F1 score: 79.280\n",
      "Epoch 94/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.4740 - crf_accuracy: 0.9564 - val_loss: 22.6028 - val_crf_accuracy: 0.9488\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 497 phrases, correct: 443.\n",
      "\n",
      "Accuracy: 93.83%, Precision: 89.13%, Recall: 70.21%, Fscore: 78.55\n",
      "\n",
      "              LOC: Precision: 82.84%, Recall: 76.50%, Fscore: 79.55\n",
      "\n",
      "              ORG: Precision: 91.24%, Recall: 72.24%, Fscore: 80.64\n",
      "\n",
      "              PER: Precision: 94.03%, Recall: 62.07%, Fscore: 74.78\n",
      "\n",
      "Best F1 score: 78.550\n",
      "Epoch 95/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.4733 - crf_accuracy: 0.9562 - val_loss: 22.5971 - val_crf_accuracy: 0.9520\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 537 phrases, correct: 460.\n",
      "\n",
      "Accuracy: 93.86%, Precision: 85.66%, Recall: 72.90%, Fscore: 78.77\n",
      "\n",
      "              LOC: Precision: 79.89%, Recall: 75.96%, Fscore: 77.87\n",
      "\n",
      "              ORG: Precision: 86.64%, Recall: 76.73%, Fscore: 81.39\n",
      "\n",
      "              PER: Precision: 91.10%, Recall: 65.52%, Fscore: 76.22\n",
      "\n",
      "Best F1 score: 78.770\n",
      "Epoch 96/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.4719 - crf_accuracy: 0.9577 - val_loss: 22.6009 - val_crf_accuracy: 0.9507\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 519 phrases, correct: 458.\n",
      "\n",
      "Accuracy: 94.19%, Precision: 88.25%, Recall: 72.58%, Fscore: 79.65\n",
      "\n",
      "              LOC: Precision: 87.92%, Recall: 71.58%, Fscore: 78.92\n",
      "\n",
      "              ORG: Precision: 85.84%, Recall: 79.18%, Fscore: 82.38\n",
      "\n",
      "              PER: Precision: 92.36%, Recall: 65.52%, Fscore: 76.66\n",
      "\n",
      "Best F1 score: 79.650\n",
      "Epoch 97/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.4728 - crf_accuracy: 0.9563 - val_loss: 22.5982 - val_crf_accuracy: 0.9506\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 545 phrases, correct: 466.\n",
      "\n",
      "Accuracy: 94.01%, Precision: 85.50%, Recall: 73.85%, Fscore: 79.25\n",
      "\n",
      "              LOC: Precision: 81.71%, Recall: 73.22%, Fscore: 77.23\n",
      "\n",
      "              ORG: Precision: 82.38%, Recall: 82.04%, Fscore: 82.21\n",
      "\n",
      "              PER: Precision: 95.62%, Recall: 64.53%, Fscore: 77.06\n",
      "\n",
      "Best F1 score: 79.250\n",
      "Epoch 98/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.4720 - crf_accuracy: 0.9571 - val_loss: 22.5978 - val_crf_accuracy: 0.9516\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 540 phrases, correct: 463.\n",
      "\n",
      "Accuracy: 93.98%, Precision: 85.74%, Recall: 73.38%, Fscore: 79.08\n",
      "\n",
      "              LOC: Precision: 78.98%, Recall: 75.96%, Fscore: 77.44\n",
      "\n",
      "              ORG: Precision: 87.21%, Recall: 77.96%, Fscore: 82.33\n",
      "\n",
      "              PER: Precision: 91.72%, Recall: 65.52%, Fscore: 76.44\n",
      "\n",
      "Best F1 score: 79.080\n",
      "Epoch 99/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 71s 4ms/step - loss: 22.4704 - crf_accuracy: 0.9583 - val_loss: 22.5984 - val_crf_accuracy: 0.9513\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 531 phrases, correct: 463.\n",
      "\n",
      "Accuracy: 94.22%, Precision: 87.19%, Recall: 73.38%, Fscore: 79.69\n",
      "\n",
      "              LOC: Precision: 82.93%, Recall: 74.32%, Fscore: 78.39\n",
      "\n",
      "              ORG: Precision: 88.21%, Recall: 76.33%, Fscore: 81.84\n",
      "\n",
      "              PER: Precision: 90.32%, Recall: 68.97%, Fscore: 78.21\n",
      "\n",
      "Best F1 score: 79.690\n",
      "Epoch 100/100\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 22.4699 - crf_accuracy: 0.9588 - val_loss: 22.5976 - val_crf_accuracy: 0.9519\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 533 phrases, correct: 464.\n",
      "\n",
      "Accuracy: 94.13%, Precision: 87.05%, Recall: 73.53%, Fscore: 79.73\n",
      "\n",
      "              LOC: Precision: 81.50%, Recall: 77.05%, Fscore: 79.21\n",
      "\n",
      "              ORG: Precision: 87.91%, Recall: 77.14%, Fscore: 82.17\n",
      "\n",
      "              PER: Precision: 92.41%, Recall: 66.01%, Fscore: 77.01\n",
      "\n",
      "Best F1 score: 79.730\n"
     ]
    }
   ],
   "source": [
    "# Bi-lstm model\n",
    "if selected_model =='Bi-lstm':\n",
    "    print('BI-lstm model:')\n",
    "    total_epoch = 100 \n",
    "    if restart_flag == False:\n",
    "        restart_flag = True\n",
    "        model= Bilstm()\n",
    "        model.compile(loss=loss, optimizer='adam', metrics=[accuracy])\n",
    "        if os.path.exists('Bi-lstm_weights-32epoch.h5') == True: #load pre-trained model so that you don`t need train from scratch\n",
    "            print('Loading pre-trained weights obtained from the designed Bilstm model pre-trained 32 epoch on Cheaha...')\n",
    "            model.load_weights('Bi-lstm_weights-32epoch.h5')\n",
    "            print('Loading complete!')\n",
    "        for i in range(total_epoch):\n",
    "            print('Epoch ' + str(i + 1) + '/' + str(total_epoch))\n",
    "            model.fit([train_data_lstm[0],train_data_lstm[1]], train_labels,epochs=1,shuffle=True, batch_size=256,validation_data=(evl_data_lstm, evl_labels))\n",
    "            y_pred = model.predict([evl_data_lstm[0],evl_data_lstm[1]], batch_size=128)\n",
    "            pred = string_tag(y_pred, id_to_tag, True)\n",
    "            ground_truth = string_tag(evl_labels, id_to_tag)\n",
    "            info = []\n",
    "            for i in range(len(pred)):\n",
    "                temp = []\n",
    "                padding = 'S' #padding information\n",
    "                ground_Truth = convert_iobes_to_iob_tags(ground_truth[i]) #tag num to strings\n",
    "                predictions = convert_iobes_to_iob_tags(pred[i]) #tag num to strings\n",
    "                for padding_char, gT, predict in zip(padding, ground_Truth, predictions):\n",
    "                    temp.append(\" \".join([padding_char, gT, predict]))\n",
    "                info.append(temp)\n",
    "            lines=evaluate_report(info,folder_patch)\n",
    "            for line in lines:\n",
    "                print(line)\n",
    "            F1 = float(lines[1].strip().split()[-1])\n",
    "            print(\"Best F1 score: {:>.3f}\".format(F1))\n",
    "        model.save_weights('Bi-lstm_weights-{}epoch.h5'.format(total_epoch)) # save the model after training\n",
    "    else:\n",
    "        print('Do not train both models in one time. The GPU memory has been filled up by the last model.  Please restart the kernel to clean the GPU and re-run the codes to train the second model!' )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-project]",
   "language": "python",
   "name": "conda-env-.conda-project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
