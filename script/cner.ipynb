{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import pickle\n",
    "import math\n",
    "import jieba\n",
    "jieba.initialize()\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.crf import crf_log_likelihood\n",
    "from tensorflow.contrib.crf import viterbi_decode\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import concat, placeholder, reduce_sum, Variable, expand_dims, reduce_mean\n",
    "from tensorflow import abs, get_variable, variable_scope, sign, reshape, cast, squeeze, shape\n",
    "from tensorflow.nn import embedding_lookup as embed\n",
    "from tensorflow.nn import dropout, atrous_conv2d, conv2d, bias_add, relu, xw_plus_b\n",
    "from collections import defaultdict, namedtuple\n",
    "from keras.models import Model as Model_init\n",
    "from keras.layers import  LSTM, Bidirectional, Input, Embedding, Concatenate, Dropout\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss as loss\n",
    "from keras_contrib.metrics import crf_accuracy as accuracy\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "# The tensorflow will fill up the GPU memery during a training process of one model, and it will not release the memery after training.\n",
    "# After executing the pipeline for one model. Users need to select 'Kernel'-----'Restart and Clean Output' to release the memery manually.\n",
    "# Then, users can test the pipeline for another model\n",
    "restart_flag = False # a flag to display whether the kernel should be manually 'restart and clean output' for switching between two models\n",
    "\n",
    "# If any of these two commands is changed, Please restart the kernel!!! ( Menu ---- Kernel ---- Restart&Run All)\n",
    "# selected_model = 'bi-lstm' # uncomment this line, the designed BI-lstm model will be loaded\n",
    "selected_model = 'id-cnn'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Raw Data preprocessing\n",
    "\n",
    "    1) Load data from file. \n",
    "    2) Convert IOB tagging into IOBES tagging. \n",
    "    3) Split data into training data, testing data and evaluation data.\n",
    "    4) Creating item-to-sequence and sequence-to-item dictionaries.\n",
    "    5) Convert chinese characters and tags into sequence.\n",
    "    6) Divide data into batches with fixed length and padding samples with 0 to maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['因', 'O'], ['此', 'O'], ['，', 'O'], ['这', 'O'], ['次', 'O'], ['政', 'O'], ['府', 'O'], ['危', 'O'], ['机', 'O'], ['终', 'O'], ['于', 'O'], ['得', 'O'], ['到', 'O'], ['化', 'O'], ['解', 'O'], ['，', 'O'], ['对', 'O'], ['俄', 'B-LOC'], ['罗', 'I-LOC'], ['斯', 'I-LOC'], ['来', 'O'], ['说', 'O'], ['是', 'O'], ['值', 'O'], ['得', 'O'], ['庆', 'O'], ['幸', 'O'], ['的', 'O'], ['。', 'O']]\n",
      "[['因', 'O'], ['此', 'O'], ['，', 'O'], ['这', 'O'], ['次', 'O'], ['政', 'O'], ['府', 'O'], ['危', 'O'], ['机', 'O'], ['终', 'O'], ['于', 'O'], ['得', 'O'], ['到', 'O'], ['化', 'O'], ['解', 'O'], ['，', 'O'], ['对', 'O'], ['俄', 'B-LOC'], ['罗', 'I-LOC'], ['斯', 'E-LOC'], ['来', 'O'], ['说', 'O'], ['是', 'O'], ['值', 'O'], ['得', 'O'], ['庆', 'O'], ['幸', 'O'], ['的', 'O'], ['。', 'O']]\n",
      "The number of sentences of trainning data is 19472\n",
      "The number of sentences of testing data is 5007\n",
      "The number of sentences of development data is 3339\n",
      "The number of unique Chinese characters is: 4277\n",
      "The number of unique tag characters is: 13\n",
      "[['因', '此', '，', '这', '次', '政', '府', '危', '机', '终', '于', '得', '到', '化', '解', '，', '对', '俄', '罗', '斯', '来', '说', '是', '值', '得', '庆', '幸', '的', '。'], [209, 187, 2, 22, 133, 63, 247, 642, 117, 619, 57, 107, 42, 103, 229, 2, 38, 663, 415, 206, 43, 87, 11, 552, 107, 782, 1130, 3, 4], [1, 3, 0, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 0, 0, 1, 2, 3, 1, 3, 0, 1, 3, 1, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "The number of steps per epoch is 974\n"
     ]
    }
   ],
   "source": [
    "# read sentences from file\n",
    "def load_data(file_path):\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for line in codecs.open(file_path, 'r', 'utf8'):\n",
    "        line = line.rstrip() # Remove any white spaces at the end of the string\n",
    "        if not line:\n",
    "            if len(sent) > 0: # a line with \"\\n\" is used for spliting sentences\n",
    "                sentences.append(sent)\n",
    "                sent = []\n",
    "        else:\n",
    "            word_tag = line.split() # split word and tag\n",
    "            if len(word_tag) == 2:\n",
    "                sent.append(word_tag)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# convert IOB tags to IOBES tags\n",
    "def convert_to_iobes_tags(sentences):   \n",
    "    for index, sent in enumerate(sentences):\n",
    "        iob_tags = [word_tag[-1] for word_tag in sent] # obtain iob tags of a sentence\n",
    "        iobes_tags = [] # iobes tags\n",
    "        for i, tag in enumerate(iob_tags): \n",
    "            if tag == 'O': # O tag is unchanged\n",
    "                iobes_tags.append(tag)\n",
    "            elif tag.split('-')[0] == 'B':  # B to S if an entity only includes a single word\n",
    "                if i + 1 < len(iob_tags) and iob_tags[i + 1].split('-')[0] == 'I':\n",
    "                    iobes_tags.append(tag)\n",
    "                else:\n",
    "                    iobes_tags.append(tag.replace('B-', 'S-'))\n",
    "            elif tag.split('-')[0] == 'I':  # E is used for the last item of an entity (words > 2)\n",
    "                if i + 1 < len(iob_tags) and iob_tags[i + 1].split('-')[0] == 'I':\n",
    "                    iobes_tags.append(tag)\n",
    "                else:\n",
    "                    iobes_tags.append(tag.replace('I-', 'E-'))\n",
    "            else:\n",
    "                print('ERROR: INVALID IOB TAGGING!')  \n",
    "        for word, iobes_tag in zip(sent, iobes_tags): # replace tags\n",
    "            word[-1] = iobes_tag\n",
    "            \n",
    "            \n",
    "# split sentences into train, test, dev\n",
    "def split_data(sentences):\n",
    "    train_div = int(len(sentences) * 0.7);  # train set divide number\n",
    "    train_sentences = sentences[:train_div]\n",
    "    remaining_sentences = sentences[train_div:] \n",
    "    test_div = int(len(remaining_sentences)* 0.6)  # test set divide number\n",
    "    test_sentences = remaining_sentences[:test_div]\n",
    "    dev_sentences = remaining_sentences[test_div:]\n",
    "    return train_sentences, test_sentences, dev_sentences\n",
    "\n",
    "\n",
    "# creating dictionaries from unique chinese characters to unique id\n",
    "def create_char_id_convert_dict(sentences):\n",
    "    char_dict = {} # a dictionary of the frequency of unique chinese characters\n",
    "    chinese_chars = [[word[0] for word in sent] for sent in sentences] # get words from tupe word_tag\n",
    "    for chars in chinese_chars: # get frequency of unique chinese characters\n",
    "        for char in chars:\n",
    "            if char not in char_dict:\n",
    "                char_dict[char] = 1\n",
    "            else:\n",
    "                char_dict[char] += 1\n",
    "    char_dict[\"<PAD>\"] = 99999 # spacial word for padding, and intial a largest frequency\n",
    "    char_dict['<UNK>'] = 99998 # spacial word for unkonwn, and intial a second largest frequency\n",
    "    # sort characters by frequency (highest to samllest)\n",
    "    sorted_char_dict = sorted(char_dict.items(), key=lambda x: (-x[1], x[0])) \n",
    "    # create two dictionaries: find char by id, or find id by char\n",
    "    id_to_char = {index: value[0] for index, value in enumerate(sorted_char_dict)} \n",
    "    char_to_id = {value: key for key, value in id_to_char.items()}\n",
    "    return id_to_char, char_to_id\n",
    "\n",
    "\n",
    "# creating dictionaries from unique tag to unique id\n",
    "def create_tag_id_convert_dict(sentences):\n",
    "    tag_dict = {} # a dictionary of the frequency of tags\n",
    "    tags = [[word[1] for word in sent] for sent in sentences]\n",
    "    for tag in tags: # get frequency of unique chinese characters\n",
    "        for t in tag:\n",
    "            if t not in tag_dict:\n",
    "                tag_dict[t] = 1\n",
    "            else:\n",
    "                tag_dict[t] += 1\n",
    "    # sort characters by frequency (highest to samllest)\n",
    "    sorted_tag_dict = sorted(tag_dict.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # create two dictionaries: find tag by id, or find id by tag\n",
    "    id_to_tag = {index: value[0] for index, value in enumerate(sorted_tag_dict)} \n",
    "    tag_to_id = {value: key for key, value in id_to_tag.items()}\n",
    "    return id_to_tag, tag_to_id\n",
    "\n",
    "\n",
    "# Generated formated data for training\n",
    "def get_formated_data(sentences, char_to_id, tag_to_id):\n",
    "    formated_data = []\n",
    "    for sent in sentences:\n",
    "        sent_chars = [word[0] for word in sent] # get chinese chars\n",
    "        # convert chars to id\n",
    "        chars_id = [char_to_id[char if char in char_to_id else '<UNK>'] for char in sent_chars] \n",
    "        joined_sent = \"\".join(sent_chars) # joined all the chars into a sentence\n",
    "\n",
    "        # Tokenize sent with Jieba to get chinese phrase feature (the start, inside, and end of a phrase)\n",
    "        phrase_feature = []\n",
    "        for token in jieba.cut(joined_sent):\n",
    "            if len(token) == 1: # phrase_feature is 0 if a phase only has one Chinese character\n",
    "                phrase_feature.append(0)\n",
    "            else:\n",
    "                phrase_list = [2] * len(token) # phrase_feature of middle characters in a phase is 2\n",
    "                phrase_list[0] = 1 # phrase_feature of start character in a phase is 1\n",
    "                phrase_list[-1] = 3 # phrase_feature of end character in a phase is 3\n",
    "                phrase_feature.extend(phrase_list)\n",
    "\n",
    "        tags_id = [tag_to_id[word[-1]] for word in sent] # convert tags to id\n",
    "        formated_data.append([sent_chars, chars_id, phrase_feature, tags_id]) # formated data\n",
    "    return formated_data\n",
    "\n",
    "\n",
    "# Divide data into batches and padding each sample\n",
    "def generate_batch_data_with_padding(data, bcount, model='id-cnn', cut_length=510):\n",
    "    GT = []\n",
    "    if model == 'id-cnn':\n",
    "        batches = []\n",
    "        batch_count = int(math.ceil(len(data)/ bcount)) # calulate number of batches\n",
    "        # sorted list based on the length of sentences(short to long)\n",
    "        sorted_len_data = sorted(train_data, key=lambda x: len(x[0]))\n",
    "        for i in range(batch_count):\n",
    "            batch = sorted_len_data[(i * bcount) : ((i + 1) * bcount)] # divided data into batches with fixed length\n",
    "            pad_sentsents = [] # sentsents after padding\n",
    "            pad_chars = [] # chinese characters after padding\n",
    "            pad_phrases = [] # pahrase features after padding\n",
    "            pad_tags = [] # tags after padding\n",
    "            max_length = max([len(sample[0]) for sample in batch]) # find the max length of sentence in batch\n",
    "            for sample in batch:\n",
    "                sent, char, phrase, tag = sample \n",
    "                pad_array = [0] * (max_length - len(sent)) # padding with 0 based on the max length\n",
    "                pad_sentsents.append(sent + pad_array) \n",
    "                pad_chars.append(char + pad_array)\n",
    "                pad_phrases.append(phrase + pad_array)\n",
    "                pad_tags.append(tag + pad_array)\n",
    "            batches.append([pad_sentsents, pad_chars, pad_phrases, pad_tags]) # get batch data\n",
    "        return batches\n",
    "    elif model == 'bi-lstm':\n",
    "        fixed_max_length = cut_length # the length should be the same as the lstm model input dimention\n",
    "        pad_sentsents = [] # reinitialize, sentsents after padding\n",
    "        pad_chars = [] # reinitialize, chinese characters after padding\n",
    "        pad_phrases = [] # reinitialize, pahrase features after padding\n",
    "        pad_tags = [] # reinitialize, tags after padding\n",
    "        for sample in data:\n",
    "            sent, char, phrase, tag = sample \n",
    "            if len(sent) >= fixed_max_length:\n",
    "                pad_sentsents.append(sent[0:fixed_max_length])\n",
    "                pad_chars.append(char[0:fixed_max_length])\n",
    "                pad_phrases.append(phrase[0:fixed_max_length])\n",
    "                pad_tags.append(tag[0:fixed_max_length])\n",
    "            else:\n",
    "                pad_array = [0] * (fixed_max_length - len(sent))\n",
    "                pad_sentsents.append(sent + pad_array) \n",
    "                pad_chars.append(char + pad_array)\n",
    "                pad_phrases.append(phrase + pad_array)\n",
    "                pad_tags.append(tag + pad_array)\n",
    "            \n",
    "        pad_chars = np.array(pad_chars)\n",
    "        pad_phrases = np.array(pad_phrases)\n",
    "        pad_tags = np.array(pad_tags)\n",
    "        groundT = np.expand_dims(pad_tags, 2)\n",
    "        return [pad_chars, pad_phrases], groundT\n",
    "    else:\n",
    "        print('ERROR: INVILID MODEL!\\n')\n",
    "        \n",
    "        \n",
    "# data processing\n",
    "folder_path = \"./dataset/\"  # dataset folder\n",
    "data_path = folder_path + \"data.txt\" # data path\n",
    "\n",
    "sentences = load_data(data_path) # load data\n",
    "print(sentences[0]) \n",
    "\n",
    "convert_to_iobes_tags(sentences) # convert to iobes tags\n",
    "print(sentences[0]) \n",
    "\n",
    "train_sentences, test_sentences, dev_sentences = split_data(sentences) # split data \n",
    "print(\"The number of sentences of trainning data is\", len(train_sentences))\n",
    "print(\"The number of sentences of testing data is\", len(test_sentences))\n",
    "print(\"The number of sentences of development data is\", len(dev_sentences))\n",
    "\n",
    "# creates chinese characters and senquence convertion dictionaries\n",
    "id_to_char, char_to_id = create_char_id_convert_dict(train_sentences) \n",
    "# creates tags and senquence convertion dictionaries\n",
    "id_to_tag, tag_to_id = create_tag_id_convert_dict(train_sentences)\n",
    "print(\"The number of unique Chinese characters is:\", len(char_to_id))\n",
    "print(\"The number of unique tag characters is:\", len(tag_to_id))\n",
    "\n",
    "train_data = get_formated_data(train_sentences, char_to_id, tag_to_id) # formated training data\n",
    "test_data = get_formated_data(test_sentences, char_to_id, tag_to_id) # formated testing data\n",
    "dev_data = get_formated_data(dev_sentences, char_to_id, tag_to_id) # formated edata\n",
    "print(train_data[0])\n",
    "\n",
    "with open(folder_path + 'dict.pkl', \"wb\") as out_file:  # dump data for eveluation \n",
    "    pickle.dump([char_to_id, id_to_char, tag_to_id, id_to_tag], out_file)\n",
    "    \n",
    "# Hyper Parameters:\n",
    "learning_rate = 0.001\n",
    "channel_char = 128 #embedding output dimention for char\n",
    "channel_phrase = 20 #embedding output dimention for phrase\n",
    "channel_lstm = 256 #input dimention for Bi-lstm\n",
    "len_tags = len(tag_to_id)\n",
    "len_char = len(char_to_id)\n",
    "\n",
    "# generate batches with padding\n",
    "if selected_model == 'bi-lstm':\n",
    "    train_data_lstm, train_labels = generate_batch_data_with_padding(train_data, 20, selected_model, channel_lstm) \n",
    "    evl_data_lstm, evl_labels = generate_batch_data_with_padding(dev_data, 20, selected_model, channel_lstm)\n",
    "    test_data_lstm, test_labels = generate_batch_data_with_padding(test_data, 20, selected_model, channel_lstm)\n",
    "elif selected_model == 'id-cnn':\n",
    "    train_batch_data = generate_batch_data_with_padding(train_data, 20) \n",
    "    dev_batch_data = generate_batch_data_with_padding(dev_data, 100)\n",
    "    test_batch_data = generate_batch_data_with_padding(test_data, 100)\n",
    "    \n",
    "    epoch_iterations = len(train_batch_data) # set the iterations per epoch\n",
    "    print(\"The number of steps per epoch is\", epoch_iterations)\n",
    "else:\n",
    "    print('ERROR: INVILID MODEL!\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dependences of Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-lstm model\n",
    "# Because we take advantage of two types of features: cn_chars, cn_phrase, there will be two input layers\n",
    "def Bilstm():\n",
    "    crf = CRF(len(tag_to_id), sparse_target=True) # define the crf layer at first\n",
    "    char_input = Input(shape=(channel_lstm,), name=\"Input-CN-Char\") # the shape must be the max size of a sentence, it can be changed for long sentence. However, the too many neurons will slow down the model dramatically\n",
    "    char_emb = Embedding(len(char_to_id),output_dim=channel_char,trainable=False,mask_zero=True)(char_input) \n",
    "    phrase_input = Input(shape=(channel_lstm,), name=\"Input-CN-Phrase\")# the shape must be the max size of a sentence\n",
    "    phrase_emb = Embedding(input_dim=4,output_dim=20,trainable=False,mask_zero=True)(phrase_input) \n",
    "    # concatenate them to makes a single vector\n",
    "    merged = Concatenate(axis=-1)([char_emb, phrase_emb])\n",
    "#     dropout = Dropout(0.5)(merged) #prevent from overfitting # if encounter severe overfitting, uncomment this line and change the input layer of the next layer\n",
    "    lstm = Bidirectional(LSTM(100, return_sequences=True))(merged)\n",
    "    dropout = Dropout(0.5)(lstm) #prevent from overfitting\n",
    "    CRF_layer = crf(dropout)\n",
    "    model = Model_init(inputs=[char_input, phrase_input], outputs=[CRF_layer]) #explicate the input list.\n",
    "    model.summary() # plot the configure\n",
    "    plot_model(model, to_file='BI-lstm model.png') #output the figure of model structure\n",
    "    return model\n",
    "\n",
    "class Model(object): # <Fast and Accurate Entity Recognition with Iterated Dilated Convolutions>\n",
    "    def __init__(self):\n",
    "        self.__main_setup() # model initializing\n",
    "        \n",
    "    def __main_setup(self):\n",
    "        self.__hyper() #set up hyperparameters\n",
    "        self.__placeholder() #build tensor holder\n",
    "        self.__parameters() #initializing\n",
    "        self.__layers() #create model\n",
    "        self.__opt() #optimizer\n",
    "        \n",
    "    def __layers(self):\n",
    "        self.__embedding() #embedding layers\n",
    "        self.__dilated() # iterated dilated cnn \n",
    "        self.__loss() \n",
    "        \n",
    "    def __hyper(self):\n",
    "        self.learningR = learning_rate #learning rate \n",
    "        self.channel_char = channel_char  # char embedding dimention\n",
    "        self.channel_phrase = channel_phrase # phrase embedding dimention\n",
    "        self.len_tags = len_tags # number of tags\n",
    "        self.len_chars = len_char #unique Chinese char\n",
    "        self.output_channel = 0\n",
    "        \n",
    "    def __placeholder(self):\n",
    "        self.gt = placeholder(dtype=tf.int32) #GT\n",
    "        self.f1_evaluate = Variable(dtype=tf.float32,initial_value=0.0, trainable=False) #best f1 score for evaluate data\n",
    "        self.f1_test = Variable(dtype=tf.float32,initial_value=0.0, trainable=False) #for test data\n",
    "        self.whole_steps = Variable(dtype=tf.int32,initial_value=0, trainable=False) #steps for training process\n",
    "        self.cn_char = placeholder(dtype=tf.int32) #input sentence\n",
    "        self.cn_phrase = placeholder(dtype=tf.int32) #nput Chinese phrase features\n",
    "        self.dropout = placeholder(dtype=tf.float32) #dropout\n",
    "        \n",
    "    def __parameters(self):\n",
    "        self.output_channel = 0\n",
    "        self.len_phrase = 4  #phrase features 0,1,2,3\n",
    "        length = reduce_sum(sign(abs(self.cn_char)), reduction_indices=1)\n",
    "        self.lengths = cast(length, tf.int32)\n",
    "        self.batch_size = shape(self.cn_char)[0] #batch_size\n",
    "        self.num_steps = shape(self.cn_char)[-1] #num_steps: total chars in each sentenc\n",
    "        self.layers = [1,1,2] #based on the paper, there will be 2 types of dilated rates\n",
    "        self.flag_drop = 0.5  #prevent from overfitting\n",
    "        self.channel_cnn = 100 # cnn kernels numbers \n",
    "        self.minor = -1000.0\n",
    "        self.model_training = True\n",
    "        if self.model_training == False:\n",
    "            self.flag_drop = 1.0 \n",
    "        self.filters = 3 \n",
    "        self.iterations = 4 #iterated \n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        self.channel_embedding = self.channel_char + self.channel_phrase  # char channels + phrase channels\n",
    "        \n",
    "    def __embedding(self): # initializing for two features\n",
    "        char_embeddings = get_variable('Embeddings_1',shape=[self.len_chars, self.channel_char],initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None),\n",
    "            dtype=tf.float32,trainable=True)\n",
    "        feature_1 = embed(char_embeddings, self.cn_char)\n",
    "        phrase_embeddings = get_variable('Embeddings_2',shape=[self.len_phrase, self.channel_phrase],initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None),\n",
    "            dtype=tf.float32,trainable=True)\n",
    "        feature_2 = embed(phrase_embeddings, self.cn_phrase)\n",
    "        self.embedding_cns = concat([feature_1,feature_2], axis=-1)\n",
    "        self.embedding_cns = dropout(self.embedding_cns, self.dropout)#apply dropout \n",
    "\n",
    "    def __dilated(self):# Dilated Convolutions Networks\n",
    "        nets_input = expand_dims(self.embedding_cns, 1)   \n",
    "        initialed_weight = get_variable(\"kernel\",shape=[1, self.filters, self.channel_embedding,self.channel_cnn],initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "        nets_input = conv2d(nets_input, initialed_weight, strides=[1, 1, 1, 1],  padding=\"SAME\",name=\"nets_input\")\n",
    "        output = []\n",
    "        channels = 0\n",
    "        for j in range(self.iterations):  \n",
    "            for i in range(len(self.layers)):# many dilated cnns can cover almost all the features\n",
    "                dilated_rate = self.layers[i]\n",
    "                if i == (len(self.layers) - 1):\n",
    "                    last_layer = True\n",
    "                else:\n",
    "                    last_layer = False\n",
    "                with variable_scope(\"DilatedConv%d\" % i, reuse=tf.AUTO_REUSE):\n",
    "                    weights = get_variable(name='Weights',shape=[1, self.filters, self.channel_cnn,self.channel_cnn], initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "                    biases = get_variable(name='Biases',shape=[self.channel_cnn]) \n",
    "                    c = atrous_conv2d(nets_input,weights, rate=dilated_rate, padding=\"SAME\") # dilated convolution\n",
    "                    c = bias_add(c, biases)\n",
    "                    c = relu(c)\n",
    "                    if last_layer:\n",
    "                        channels += self.channel_cnn\n",
    "                        output.append(c)\n",
    "                    nets_input = c\n",
    "        output_last = concat(values=output,axis=3) # merge the output of 4 last layers\n",
    "        output_last = dropout(output_last, self.flag_drop) #add dropout layer \n",
    "#             drop dimention: the dimention which contians only one data\n",
    "        output_last = squeeze(output_last, [1])\n",
    "        output_last = reshape(output_last, [-1, channels]) # final features done\n",
    "        self.output_channel = channels\n",
    "        weight = get_variable(\"Weight\", shape=[self.output_channel, self.len_tags],dtype=tf.float32, initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "        bias = get_variable(\"Bias\",  initializer=tf.constant(0.0001, shape=[self.len_tags]))\n",
    "#                    matmul(x, w) + b.\n",
    "        result = xw_plus_b(output_last, weight, bias)\n",
    "        self.result =  reshape(result, [-1, self.num_steps, self.len_tags])  # num_steps: total chars in each sentenc, len_tags: number of tags\n",
    "\n",
    "    def __loss(self):\n",
    "        # pad units \n",
    "        initial_units = concat([self.minor*tf.ones(shape=[self.batch_size, 1, self.len_tags]), tf.zeros(shape=[self.batch_size, 1, 1])], axis=-1)\n",
    "        pad_units = cast(self.minor*tf.ones([self.batch_size, self.num_steps, 1]), tf.float32)\n",
    "        temp = concat([self.result, pad_units], axis=-1)\n",
    "        temp = concat([initial_units, temp], axis=1)\n",
    "        gt = concat([cast(self.len_tags*tf.ones([self.batch_size, 1]), tf.int32), self.gt], axis=-1)\n",
    "        self.transition = get_variable(\"transit\",shape=[self.len_tags + 1, self.len_tags + 1],initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "        likelihood, self.transition = crf_log_likelihood(inputs=temp,tag_indices=gt,transition_params=self.transition,sequence_lengths=self.lengths+1)\n",
    "        self.error = reduce_mean(likelihood*(-1))\n",
    "            \n",
    "    def __opt(self):\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learningR)\n",
    "        gradients = self.optimizer.compute_gradients(self.error) \n",
    "        limited_gradients = [[tf.clip_by_value(gra, -4, 4), va] for gra, va in gradients] # avoid gradient explosion\n",
    "        self.optimize = self.optimizer.apply_gradients(limited_gradients, self.whole_steps)\n",
    "            \n",
    "    def evaluate(self, sess, batch_data, id_to_tag):\n",
    "        transition = self.transition.eval()\n",
    "        report = []\n",
    "        for batch in batch_data:\n",
    "            cn_sentences = batch[0]\n",
    "            tags = batch[-1] #true tag\n",
    "            lengths, scores = self.each_step(sess, False, batch)\n",
    "            batch_paths = self.viterbi(scores, lengths, transition)\n",
    "            for i in range(len(cn_sentences)):\n",
    "                output = []\n",
    "                sentence = cn_sentences[i][:lengths[i]]\n",
    "                gt = convert_iobes_to_iob_tags([id_to_tag[int(x)] for x in tags[i][:lengths[i]]])\n",
    "                predict = convert_iobes_to_iob_tags([id_to_tag[int(x)] for x in batch_paths[i][:lengths[i]]])\n",
    "                for cn_char, gt, predict in zip(sentence, gt, predict):\n",
    "                    output.append(\" \".join([cn_char, gt, predict]))\n",
    "                report.append(output)\n",
    "        return report\n",
    "    \n",
    "    def viterbi(self, units, lengths, array): # viterbi Algorithm\n",
    "        paths = []\n",
    "        begin = np.asarray([[self.minor]*self.len_tags +[0]])\n",
    "        for val, temp_len in zip(units, lengths):\n",
    "            val = val[:temp_len]\n",
    "            pad = np.ones([temp_len, 1])*(self.minor)\n",
    "            units = np.concatenate([val, pad], axis=1)\n",
    "            units = np.concatenate([begin, units], axis=0)\n",
    "            path, _ = viterbi_decode(units, array)\n",
    "            paths.append(path[1:])\n",
    "        return paths\n",
    "    \n",
    "    def each_step(self, sess, training, batch):\n",
    "        _, cn_char, cn_phrase, tags = batch\n",
    "        temp_dict = {self.cn_char: np.asarray(cn_char),self.cn_phrase: np.asarray(cn_phrase), self.dropout: 1.0}\n",
    "        if training:\n",
    "            temp_dict[self.gt] = np.asarray(tags) #GT\n",
    "            temp_dict[self.dropout] = 0.5\n",
    "            whole_steps, error, _ = sess.run([self.whole_steps, self.error, self.optimize], temp_dict)\n",
    "            return whole_steps, error\n",
    "        else:\n",
    "            lengths, units = sess.run([self.lengths, self.result], temp_dict)\n",
    "            return lengths, units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Evaluation\n",
    "\n",
    "- CRF loss function\n",
    "- Vierbi Alogrithm to decode the predict result\n",
    "- Convert the iobes tags to iob tags for predict result\n",
    "- Calculate the overall accracy, precision, recall and fcore \n",
    "- Calculate the precision, recall and fcore for each named-entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert IOBES tags to IOB tags\n",
    "def convert_iobes_to_iob_tags(tags):\n",
    "    iob_tags = []\n",
    "    for index, tag in enumerate(tags):\n",
    "        t = tag.split('-')[0]        \n",
    "        if t == 'S': iob_tags.append(tag.replace('S-', 'B-'))\n",
    "        elif t == 'E': iob_tags.append(tag.replace('E-', 'I-'))\n",
    "        else: iob_tags.append(tag)          \n",
    "    return iob_tags\n",
    "\n",
    "def string_tag(data, id_to_tag, is_groundtruth=False ,): # convert tag num to strings\n",
    "    tags = []\n",
    "    if is_groundtruth == False:\n",
    "        for dim_0 in data:\n",
    "            temp = []\n",
    "            for dim_1 in dim_0:\n",
    "                temp.append(id_to_tag[np.asscalar(np.array([dim_1]))]) #np variable should be convert to scalar!\n",
    "            tags.append(temp)\n",
    "        return tags\n",
    "    else:       \n",
    "        for dim_0 in data:\n",
    "            temp = []\n",
    "            for dim_1 in dim_0:\n",
    "                index = dim_1.argmax() # obtain index\n",
    "                temp.append(id_to_tag[index]) #convert to string tag\n",
    "            tags.append(temp)\n",
    "        return tags\n",
    "\n",
    "# check if a phrase ended between the previous and current character\n",
    "def check_phrase_end_tag(prev_tag, cur_tag, prev_type, cur_type):\n",
    "    is_end = False\n",
    "\n",
    "    if prev_tag == 'E': is_end = True\n",
    "    if prev_tag == 'S': is_end = True\n",
    "        \n",
    "    if prev_tag == 'B' and (cur_tag == 'B' or cur_tag == 'S' or cur_tag == 'O'): \n",
    "        is_end = True  \n",
    "    if prev_tag == 'I' and (cur_tag == 'B' or cur_tag == 'S' or cur_tag == 'O'): \n",
    "        is_end = True\n",
    "        \n",
    "    if prev_tag != 'O' and prev_tag != '.' and prev_type != cur_type: \n",
    "        is_end = True\n",
    "\n",
    "    return is_end\n",
    "\n",
    "\n",
    "# check if a phrase started between the previous and current character\n",
    "def check_phrase_start_tag(prev_tag, cur_tag, prev_type, cur_type):\n",
    "    is_start = False\n",
    "\n",
    "    if cur_tag == 'B': chunk_start = True\n",
    "    if cur_tag == 'S': chunk_start = True\n",
    "    \n",
    "    if prev_tag == 'E' and (cur_tag == 'E' or cur_tag == 'I'): \n",
    "        is_start = True\n",
    "    if prev_tag == 'S' and (cur_tag == 'E' or cur_tag == 'I'): \n",
    "        is_start = True\n",
    "    if prev_tag == 'O' and (cur_tag == 'E' or cur_tag == 'I'): \n",
    "        is_start = True\n",
    "    \n",
    "    if cur_tag != 'O' and cur_tag != '.' and prev_type != cur_type: \n",
    "        is_start = True\n",
    "\n",
    "    return is_start\n",
    "\n",
    "\n",
    "# calculate the precision, recall and f-score\n",
    "def get_metrics(correct_count, predict_count, total_count):\n",
    "    TP = correct_count \n",
    "    FP = predict_count - correct_count\n",
    "    FN = total_count - correct_count\n",
    "\n",
    "    prec = 0 if (TP + FP == 0) else (1. * TP) / (TP + FP)  # precision\n",
    "    recall = 0 if (TP + FN == 0) else (1. * TP) / (TP + FN)  # recall \n",
    "    fscore = 0 if (prec + recall == 0) else (2 * prec * recall / (prec + recall)) # f-score\n",
    "\n",
    "    Metrics = namedtuple('Metrics', 'tp fp fn prec rec fscore')\n",
    "    return Metrics(TP, FP, FN, prec, recall, fscore)\n",
    "\n",
    "\n",
    "# parse tag into IOBES tags and enetity type\n",
    "def parse_tages(tag):\n",
    "    matched = re.match(r'^([^-]*)-(.*)$', tag)\n",
    "    return matched.groups() if matched else (tag, '')\n",
    "\n",
    "\n",
    "# print precsion, recall and f-score with format\n",
    "def print_report(parse_results, correct_entities, found_c_entities, found_g_entities):    \n",
    "    parsed_report = []        \n",
    "    metrics = get_metrics(parse_results[0], parse_results[3], parse_results[2])\n",
    "    \n",
    "    # all the found entities\n",
    "    cg_entities = list(found_c_entities) + list(found_g_entities)    \n",
    "    uniq_tags = set([e for e in cg_entities])  # unique tags\n",
    "\n",
    "    # get metrics includes precsion, recall and f-score\n",
    "    entity_metrics = {}\n",
    "    for tag in uniq_tags:\n",
    "        entity_metrics[tag] = get_metrics(correct_entities[tag], found_g_entities[tag], found_c_entities[tag])\n",
    "     \n",
    "    # print total tokens and phrases count\n",
    "    result_line = []\n",
    "    result_line.append('Total tokens is %d and total is phrases %d\\n' % (parse_results[4], parse_results[2]))\n",
    "    result_line.append('Found: %d phrases, correct: %d.\\n' % (parse_results[3], parse_results[0]))\n",
    "    parsed_report.append(\"\".join(result_line))\n",
    "\n",
    "    # formated result lines \n",
    "    if parse_results[4] > 0:\n",
    "        result_line = []\n",
    "        result_line.append(\"Accuracy:%6.2f%%, \" % (100. * parse_results[1] / parse_results[4]))\n",
    "        result_line.append(\"Precision:%6.2f%%, \" % (100.* metrics.prec))\n",
    "        result_line.append(\"Recall:%6.2f%%, \" % (100. * metrics.rec))\n",
    "        result_line.append(\"Fscore:%6.2f\\n\" % (100. * metrics.fscore))\n",
    "        parsed_report.append(\"\".join(result_line))\n",
    "\n",
    "    for index, metric in sorted(entity_metrics.items()):\n",
    "        result_line = []\n",
    "        result_line.append('%17s: ' % index)\n",
    "        result_line.append('Precision:%6.2f%%, ' % (100. * metric.prec))\n",
    "        result_line.append('Recall:%6.2f%%, ' % (100. * metric.rec))\n",
    "        result_line.append('Fscore:%6.2f\\n' % (100. * metric.fscore))\n",
    "        parsed_report.append(\"\".join(result_line))\n",
    "    \n",
    "    return parsed_report\n",
    "\n",
    "\n",
    "# parsed the reports\n",
    "def parse_report(file_name):\n",
    "\n",
    "    is_correct = False        # if current chunk is correct\n",
    "    \n",
    "    prev_ctag = 'O'           # previous correct tag\n",
    "    prev_ctag_entity = ''     # previous correct entity (LOC, ORG, PER)\n",
    "    prev_gtag = 'O'           # previous guessed tag\n",
    "    prev_gtag_entity = ''     # previous guessed entity (LOC, ORG, PER)\n",
    "    \n",
    "    # 0: correct entity number, 1: correct tag number, 2: number of phrases  \n",
    "    # 3: number of guessed phrases 4: number of tokens\n",
    "    results = [0, 0, 0, 0, 0]\n",
    "\n",
    "    correct_entities = defaultdict(int)\n",
    "    found_c_entities = defaultdict(int)\n",
    "    found_g_entities = defaultdict(int)\n",
    "\n",
    "    with codecs.open(file_name, \"r\") as file:   # read file\n",
    "        for line in file:\n",
    "            features = line.split() # features list per line  \n",
    "            if len(features) == 0: \n",
    "                features = ['-X-', 'O', 'O'] # for white space\n",
    "\n",
    "            cur_gtag, cur_gtag_entity = parse_tages(features.pop())  # parse predicted tag\n",
    "            cur_ctag, cur_ctag_entity = parse_tages(features.pop())  # parse correct tag\n",
    "            chinese_char = features.pop(0)  # chinese character \n",
    "\n",
    "            # check if the phrase is ended between the previous and current character\n",
    "            is_end_correct = check_phrase_end_tag(prev_ctag, cur_ctag, prev_ctag_entity, cur_ctag_entity)\n",
    "            is_end_guessed = check_phrase_end_tag(prev_gtag, cur_gtag, prev_gtag_entity, cur_gtag_entity)\n",
    "\n",
    "            # check if the phrase is started between the previous and current character\n",
    "            is_start_correct = check_phrase_start_tag(prev_ctag, cur_ctag, prev_ctag_entity, cur_ctag_entity)\n",
    "            is_start_guessed = check_phrase_start_tag(prev_gtag, cur_gtag, prev_gtag_entity, cur_gtag_entity)\n",
    "\n",
    "            if is_correct:\n",
    "                if (is_end_correct and is_end_guessed and prev_gtag_entity == prev_ctag_entity):\n",
    "                    is_correct = False\n",
    "                    results[0] += 1\n",
    "                    correct_entities[prev_ctag_entity] += 1\n",
    "\n",
    "                elif (is_end_correct != is_end_guessed or cur_gtag_entity != cur_ctag_entity):\n",
    "                    is_correct = False\n",
    "\n",
    "            if is_start_correct and is_start_guessed and cur_gtag_entity == cur_ctag_entity:\n",
    "                is_correct = True\n",
    "\n",
    "            if is_start_correct:\n",
    "                results[2] += 1\n",
    "                found_c_entities[cur_ctag_entity] += 1\n",
    "            if is_start_guessed:\n",
    "                results[3] += 1\n",
    "                found_g_entities[cur_gtag_entity] += 1\n",
    "            \n",
    "            if chinese_char != '-X-':  # not empty character\n",
    "                if cur_ctag == cur_gtag and cur_gtag_entity == cur_ctag_entity:\n",
    "                    results[1] += 1\n",
    "                results[4] += 1\n",
    "            \n",
    "            # get previous tags \n",
    "            prev_gtag = cur_gtag\n",
    "            prev_ctag = cur_ctag\n",
    "            prev_gtag_entity = cur_gtag_entity\n",
    "            prev_ctag_entity = cur_ctag_entity\n",
    "\n",
    "        if is_correct:\n",
    "            results[0] += 1\n",
    "            correct_entities[prev_ctag_entity] += 1\n",
    "\n",
    "    # get parsed report,includes accuracy, precsion, recall and f-score\n",
    "    parsed_report = print_report(results, correct_entities, found_c_entities, found_g_entities)   \n",
    "    return parsed_report\n",
    "\n",
    "# write predict result and parse report\n",
    "def evaluate_report(train_results, file_path):\n",
    "    # file name\n",
    "    file_name = ''\n",
    "    if selected_model =='id-cnn':\n",
    "        file_name = os.path.join(file_path, \"id-cnn_predict_result.txt\") \n",
    "    elif selected_model == 'bi-lstm':\n",
    "        file_name = os.path.join(file_path, \"bi-lstm_predict_result.txt\") \n",
    "    else:\n",
    "        print('ERROR: INVILID MODEL!\\n')\n",
    "    # write file\n",
    "    with open(file_name, \"w\") as outfile:\n",
    "        write_context = []\n",
    "        # write line by line\n",
    "        for chunk in train_results:\n",
    "            for line in chunk:\n",
    "                write_context.append(line + \"\\n\")\n",
    "            write_context.append(\"\\n\")\n",
    "        outfile.writelines(write_context)\n",
    "    # parse report\n",
    "    result_lines = parse_report(file_name)\n",
    "    return result_lines\n",
    "\n",
    "# evalute data\n",
    "def evaluate(tf_sess, model, data, id_to_tag):\n",
    "    predict_results = model.evaluate(tf_sess, data, id_to_tag)\n",
    "    parsed_lines = evaluate_report(predict_results, folder_path)\n",
    "    for line in parsed_lines:\n",
    "        print(line)\n",
    "    f1 = float(parsed_lines[1].strip().split()[-1])\n",
    "    f1_test = model.f1_evaluate.eval()\n",
    "    if f1 > f1_test:\n",
    "        tf.assign(model.f1_evaluate, f1).eval()\n",
    "        print(\"Best f1 score: {:>.3f}\".format(f1))\n",
    "    return f1 > f1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results \n",
    "From the output information of the training process of each model, we can easily say that the training time of Iterated Dilated Convolutions model is much lower than that of Bi-lstm model.\n",
    "\n",
    "F1 score >0.80: Bi-lstm model takes at least 101 epochs, which cost at least 101x72 =7272s. However, the Iterated Dilated Convolutions model only needs to by trained on 4 epochs, which cost at most 130s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-28c4d9fe7d88>:80: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/kefan/.local/lib/python3.6/site-packages/tensorflow_core/contrib/crf/python/ops/crf.py:99: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/kefan/.local/lib/python3.6/site-packages/tensorflow_core/contrib/crf/python/ops/crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "Epoch: 1 Step: 100 of 974, Model Loss: 11.783496\n",
      "Epoch: 1 Step: 200 of 974, Model Loss:  8.320536\n",
      "Epoch: 1 Step: 300 of 974, Model Loss:  7.878644\n",
      "Epoch: 1 Step: 400 of 974, Model Loss:  7.067075\n",
      "Epoch: 1 Step: 500 of 974, Model Loss:  7.337442\n",
      "Epoch: 1 Step: 600 of 974, Model Loss:  7.370159\n",
      "Epoch: 1 Step: 700 of 974, Model Loss:  7.269576\n",
      "Epoch: 1 Step: 800 of 974, Model Loss:  7.506623\n",
      "Epoch: 1 Step: 900 of 974, Model Loss:  7.774714\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 1941 phrases, correct: 1276.\n",
      "\n",
      "Accuracy: 95.51%, Precision: 65.74%, Recall: 59.10%, Fscore: 62.24\n",
      "\n",
      "              LOC: Precision: 55.77%, Recall: 72.50%, Fscore: 63.04\n",
      "\n",
      "              ORG: Precision: 76.81%, Recall: 50.41%, Fscore: 60.87\n",
      "\n",
      "              PER: Precision: 87.50%, Recall: 47.93%, Fscore: 61.93\n",
      "\n",
      "Best f1 score: 62.240\n",
      "Epoch: 2 Step: 26 of 974, Model Loss: 10.548855\n",
      "Epoch: 2 Step: 126 of 974, Model Loss:  2.247391\n",
      "Epoch: 2 Step: 226 of 974, Model Loss:  2.310415\n",
      "Epoch: 2 Step: 326 of 974, Model Loss:  2.581856\n",
      "Epoch: 2 Step: 426 of 974, Model Loss:  2.771612\n",
      "Epoch: 2 Step: 526 of 974, Model Loss:  2.934578\n",
      "Epoch: 2 Step: 626 of 974, Model Loss:  3.290142\n",
      "Epoch: 2 Step: 726 of 974, Model Loss:  3.815366\n",
      "Epoch: 2 Step: 826 of 974, Model Loss:  3.961589\n",
      "Epoch: 2 Step: 926 of 974, Model Loss:  4.875295\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 2030 phrases, correct: 1595.\n",
      "\n",
      "Accuracy: 97.06%, Precision: 78.57%, Recall: 73.88%, Fscore: 76.15\n",
      "\n",
      "              LOC: Precision: 72.59%, Recall: 78.59%, Fscore: 75.47\n",
      "\n",
      "              ORG: Precision: 78.98%, Recall: 73.81%, Fscore: 76.31\n",
      "\n",
      "              PER: Precision: 90.93%, Recall: 67.04%, Fscore: 77.18\n",
      "\n",
      "Best f1 score: 76.150\n",
      "Epoch: 3 Step: 52 of 974, Model Loss:  4.800582\n",
      "Epoch: 3 Step: 152 of 974, Model Loss:  1.274827\n",
      "Epoch: 3 Step: 252 of 974, Model Loss:  1.557956\n",
      "Epoch: 3 Step: 352 of 974, Model Loss:  1.658494\n",
      "Epoch: 3 Step: 452 of 974, Model Loss:  1.975019\n",
      "Epoch: 3 Step: 552 of 974, Model Loss:  2.067803\n",
      "Epoch: 3 Step: 652 of 974, Model Loss:  2.299200\n",
      "Epoch: 3 Step: 752 of 974, Model Loss:  2.747910\n",
      "Epoch: 3 Step: 852 of 974, Model Loss:  2.976309\n",
      "Epoch: 3 Step: 952 of 974, Model Loss:  3.829373\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 2185 phrases, correct: 1780.\n",
      "\n",
      "Accuracy: 97.40%, Precision: 81.46%, Recall: 82.45%, Fscore: 81.95\n",
      "\n",
      "              LOC: Precision: 82.21%, Recall: 84.89%, Fscore: 83.53\n",
      "\n",
      "              ORG: Precision: 76.44%, Recall: 75.94%, Fscore: 76.19\n",
      "\n",
      "              PER: Precision: 85.19%, Recall: 85.19%, Fscore: 85.19\n",
      "\n",
      "Best f1 score: 81.950\n",
      "Epoch: 4 Step: 78 of 974, Model Loss:  2.460541\n",
      "Epoch: 4 Step: 178 of 974, Model Loss:  0.926907\n",
      "Epoch: 4 Step: 278 of 974, Model Loss:  1.171008\n",
      "Epoch: 4 Step: 378 of 974, Model Loss:  1.257589\n",
      "Epoch: 4 Step: 478 of 974, Model Loss:  1.478896\n",
      "Epoch: 4 Step: 578 of 974, Model Loss:  1.600496\n",
      "Epoch: 4 Step: 678 of 974, Model Loss:  1.903269\n",
      "Epoch: 4 Step: 778 of 974, Model Loss:  2.096939\n",
      "Epoch: 4 Step: 878 of 974, Model Loss:  2.493623\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 2185 phrases, correct: 1842.\n",
      "\n",
      "Accuracy: 97.74%, Precision: 84.30%, Recall: 85.32%, Fscore: 84.81\n",
      "\n",
      "              LOC: Precision: 87.17%, Recall: 84.89%, Fscore: 86.01\n",
      "\n",
      "              ORG: Precision: 76.29%, Recall: 82.16%, Fscore: 79.12\n",
      "\n",
      "              PER: Precision: 88.59%, Recall: 89.01%, Fscore: 88.80\n",
      "\n",
      "Best f1 score: 84.810\n",
      "Epoch: 5 Step: 4 of 974, Model Loss:  3.748037\n",
      "Epoch: 5 Step: 104 of 974, Model Loss:  0.810987\n",
      "Epoch: 5 Step: 204 of 974, Model Loss:  0.837227\n",
      "Epoch: 5 Step: 304 of 974, Model Loss:  0.968022\n",
      "Epoch: 5 Step: 404 of 974, Model Loss:  1.032144\n",
      "Epoch: 5 Step: 504 of 974, Model Loss:  1.235540\n",
      "Epoch: 5 Step: 604 of 974, Model Loss:  1.381186\n",
      "Epoch: 5 Step: 704 of 974, Model Loss:  1.678402\n",
      "Epoch: 5 Step: 804 of 974, Model Loss:  1.877780\n",
      "Epoch: 5 Step: 904 of 974, Model Loss:  2.295924\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 2113 phrases, correct: 1875.\n",
      "\n",
      "Accuracy: 98.41%, Precision: 88.74%, Recall: 86.85%, Fscore: 87.78\n",
      "\n",
      "              LOC: Precision: 87.17%, Recall: 88.59%, Fscore: 87.87\n",
      "\n",
      "              ORG: Precision: 86.09%, Recall: 80.03%, Fscore: 82.95\n",
      "\n",
      "              PER: Precision: 93.61%, Recall: 90.92%, Fscore: 92.25\n",
      "\n",
      "Best f1 score: 87.780\n",
      "Epoch: 6 Step: 30 of 974, Model Loss:  2.580483\n",
      "Epoch: 6 Step: 130 of 974, Model Loss:  0.608942\n",
      "Epoch: 6 Step: 230 of 974, Model Loss:  0.702284\n",
      "Epoch: 6 Step: 330 of 974, Model Loss:  0.833905\n",
      "Epoch: 6 Step: 430 of 974, Model Loss:  0.930428\n",
      "Epoch: 6 Step: 530 of 974, Model Loss:  1.052216\n",
      "Epoch: 6 Step: 630 of 974, Model Loss:  1.221814\n",
      "Epoch: 6 Step: 730 of 974, Model Loss:  1.481170\n",
      "Epoch: 6 Step: 830 of 974, Model Loss:  1.612719\n",
      "Epoch: 6 Step: 930 of 974, Model Loss:  2.171858\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 2147 phrases, correct: 1918.\n",
      "\n",
      "Accuracy: 98.55%, Precision: 89.33%, Recall: 88.84%, Fscore: 89.08\n",
      "\n",
      "              LOC: Precision: 88.97%, Recall: 90.33%, Fscore: 89.64\n",
      "\n",
      "              ORG: Precision: 86.09%, Recall: 81.01%, Fscore: 83.47\n",
      "\n",
      "              PER: Precision: 92.79%, Recall: 94.27%, Fscore: 93.52\n",
      "\n",
      "Best f1 score: 89.080\n",
      "Epoch: 7 Step: 56 of 974, Model Loss:  1.721512\n",
      "Epoch: 7 Step: 156 of 974, Model Loss:  0.549070\n",
      "Epoch: 7 Step: 256 of 974, Model Loss:  0.706024\n",
      "Epoch: 7 Step: 356 of 974, Model Loss:  0.700736\n",
      "Epoch: 7 Step: 456 of 974, Model Loss:  0.929933\n",
      "Epoch: 7 Step: 556 of 974, Model Loss:  1.033862\n",
      "Epoch: 7 Step: 656 of 974, Model Loss:  1.141643\n",
      "Epoch: 7 Step: 756 of 974, Model Loss:  1.386631\n",
      "Epoch: 7 Step: 856 of 974, Model Loss:  1.479736\n",
      "Epoch: 7 Step: 956 of 974, Model Loss:  2.125987\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 2125 phrases, correct: 1915.\n",
      "\n",
      "Accuracy: 98.56%, Precision: 90.12%, Recall: 88.70%, Fscore: 89.40\n",
      "\n",
      "              LOC: Precision: 91.05%, Recall: 87.39%, Fscore: 89.18\n",
      "\n",
      "              ORG: Precision: 84.89%, Recall: 84.62%, Fscore: 84.75\n",
      "\n",
      "              PER: Precision: 93.84%, Recall: 94.59%, Fscore: 94.21\n",
      "\n",
      "Best f1 score: 89.400\n",
      "Epoch: 8 Step: 82 of 974, Model Loss:  1.103739\n",
      "Epoch: 8 Step: 182 of 974, Model Loss:  0.537205\n",
      "Epoch: 8 Step: 282 of 974, Model Loss:  0.641993\n",
      "Epoch: 8 Step: 382 of 974, Model Loss:  0.663882\n",
      "Epoch: 8 Step: 482 of 974, Model Loss:  0.810042\n",
      "Epoch: 8 Step: 582 of 974, Model Loss:  0.970357\n",
      "Epoch: 8 Step: 682 of 974, Model Loss:  1.150368\n",
      "Epoch: 8 Step: 782 of 974, Model Loss:  1.299039\n",
      "Epoch: 8 Step: 882 of 974, Model Loss:  1.504785\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 2061 phrases, correct: 1913.\n",
      "\n",
      "Accuracy: 98.79%, Precision: 92.82%, Recall: 88.61%, Fscore: 90.66\n",
      "\n",
      "              LOC: Precision: 90.89%, Recall: 91.09%, Fscore: 90.99\n",
      "\n",
      "              ORG: Precision: 94.16%, Recall: 79.21%, Fscore: 86.04\n",
      "\n",
      "              PER: Precision: 94.56%, Recall: 94.11%, Fscore: 94.33\n",
      "\n",
      "Best f1 score: 90.660\n",
      "Epoch: 9 Step: 8 of 974, Model Loss:  2.236667\n",
      "Epoch: 9 Step: 108 of 974, Model Loss:  0.470572\n",
      "Epoch: 9 Step: 208 of 974, Model Loss:  0.487877\n",
      "Epoch: 9 Step: 308 of 974, Model Loss:  0.606984\n",
      "Epoch: 9 Step: 408 of 974, Model Loss:  0.688738\n",
      "Epoch: 9 Step: 508 of 974, Model Loss:  0.826125\n",
      "Epoch: 9 Step: 608 of 974, Model Loss:  0.935260\n",
      "Epoch: 9 Step: 708 of 974, Model Loss:  1.109550\n",
      "Epoch: 9 Step: 808 of 974, Model Loss:  1.185143\n",
      "Epoch: 9 Step: 908 of 974, Model Loss:  1.526200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 2044 phrases, correct: 1926.\n",
      "\n",
      "Accuracy: 98.96%, Precision: 94.23%, Recall: 89.21%, Fscore: 91.65\n",
      "\n",
      "              LOC: Precision: 92.13%, Recall: 91.63%, Fscore: 91.88\n",
      "\n",
      "              ORG: Precision: 94.42%, Recall: 80.36%, Fscore: 86.83\n",
      "\n",
      "              PER: Precision: 97.21%, Recall: 94.27%, Fscore: 95.72\n",
      "\n",
      "Best f1 score: 91.650\n",
      "Epoch: 10 Step: 34 of 974, Model Loss:  1.595829\n",
      "Epoch: 10 Step: 134 of 974, Model Loss:  0.405083\n",
      "Epoch: 10 Step: 234 of 974, Model Loss:  0.498820\n",
      "Epoch: 10 Step: 334 of 974, Model Loss:  0.569709\n",
      "Epoch: 10 Step: 434 of 974, Model Loss:  0.669950\n",
      "Epoch: 10 Step: 534 of 974, Model Loss:  0.765068\n",
      "Epoch: 10 Step: 634 of 974, Model Loss:  0.891274\n",
      "Epoch: 10 Step: 734 of 974, Model Loss:  1.030428\n",
      "Epoch: 10 Step: 834 of 974, Model Loss:  1.190792\n",
      "Epoch: 10 Step: 934 of 974, Model Loss:  1.592483\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 2060 phrases, correct: 1945.\n",
      "\n",
      "Accuracy: 99.04%, Precision: 94.42%, Recall: 90.09%, Fscore: 92.20\n",
      "\n",
      "              LOC: Precision: 93.48%, Recall: 90.33%, Fscore: 91.87\n",
      "\n",
      "              ORG: Precision: 94.11%, Recall: 83.63%, Fscore: 88.56\n",
      "\n",
      "              PER: Precision: 96.02%, Recall: 96.02%, Fscore: 96.02\n",
      "\n",
      "Best f1 score: 92.200\n",
      "Time taken for training ID-CNNs model in 10 epochs is: 649.9255409240723 s\n"
     ]
    }
   ],
   "source": [
    "# Iterated Dilated Convolutions model\n",
    "steps_check = 100\n",
    "total_epoch = 10 \n",
    "\n",
    "if selected_model == 'id-cnn':\n",
    "    if restart_flag == False:\n",
    "        restart_flag = True\n",
    "        start_time = time.time()\n",
    "        with tf.Session() as sess:\n",
    "            model = Model()\n",
    "            loss_holder = []\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(total_epoch):\n",
    "                for batch in train_batch_data:\n",
    "                    step, temp_loss = model.each_step(sess, True, batch)\n",
    "                    loss_holder.append(temp_loss)\n",
    "                    if step % steps_check == 0:\n",
    "                        Epoch = step // epoch_iterations + 1\n",
    "                        print(\"Epoch: {} Step: {} of {}, \"\"Model Loss: {:>9.6f}\".format(Epoch, step % epoch_iterations, epoch_iterations, np.mean(loss_holder)))\n",
    "                        loss_holder = []\n",
    "\n",
    "                evaluate(sess, model, dev_batch_data, id_to_tag)\n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken for training ID-CNNs model in {total_epoch} epochs is: {end_time - start_time} s\")\n",
    "    else:\n",
    "        print('Do not train both models in one time. The GPU memory has been filled up by the last model.  Please restart the kernel to clean the GPU and re-run the codes to train the second model!' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kefan/.conda/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kefan/.conda/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kefan/.conda/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2974: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/kefan/.conda/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kefan/.conda/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-CN-Char (InputLayer)      (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-CN-Phrase (InputLayer)    (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 256, 128)     547456      Input-CN-Char[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 256, 20)      80          Input-CN-Phrase[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256, 148)     0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256, 200)     199200      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256, 200)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, 256, 13)      2808        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 749,544\n",
      "Trainable params: 202,008\n",
      "Non-trainable params: 547,536\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /home/kefan/.conda/envs/nlp/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Bi-lstm model training from beginning...\n",
      "WARNING:tensorflow:From /home/kefan/.conda/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kefan/.conda/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kefan/.conda/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "WARNING:tensorflow:From /home/kefan/.conda/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kefan/.conda/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kefan/.conda/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kefan/.conda/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/kefan/.conda/envs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "19472/19472 [==============================] - 70s 4ms/step - loss: 23.6105 - crf_accuracy: 0.7613 - val_loss: 23.2453 - val_crf_accuracy: 0.8681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kefan/.conda/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 0 phrases, correct: 0.\n",
      "\n",
      "Accuracy: 81.10%, Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "              LOC: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "              ORG: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 0.000\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 66s 3ms/step - loss: 23.1684 - crf_accuracy: 0.8572 - val_loss: 23.1896 - val_crf_accuracy: 0.8682\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 0 phrases, correct: 0.\n",
      "\n",
      "Accuracy: 81.10%, Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "              LOC: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "              ORG: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 0.000\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 66s 3ms/step - loss: 23.1047 - crf_accuracy: 0.8572 - val_loss: 23.1243 - val_crf_accuracy: 0.8683\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 5 phrases, correct: 5.\n",
      "\n",
      "Accuracy: 81.10%, Precision:100.00%, Recall:  0.79%, Fscore:  1.57\n",
      "\n",
      "              LOC: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "              ORG: Precision:100.00%, Recall:  2.04%, Fscore:  4.00\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 1.570\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 66s 3ms/step - loss: 23.0333 - crf_accuracy: 0.8580 - val_loss: 23.0682 - val_crf_accuracy: 0.8697\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 27 phrases, correct: 25.\n",
      "\n",
      "Accuracy: 81.40%, Precision: 92.59%, Recall:  3.96%, Fscore:  7.60\n",
      "\n",
      "              LOC: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "              ORG: Precision: 92.59%, Recall: 10.20%, Fscore: 18.38\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 7.600\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 67s 3ms/step - loss: 22.9799 - crf_accuracy: 0.8602 - val_loss: 23.0329 - val_crf_accuracy: 0.8743\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 114 phrases, correct: 81.\n",
      "\n",
      "Accuracy: 83.41%, Precision: 71.05%, Recall: 12.84%, Fscore: 21.74\n",
      "\n",
      "              LOC: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "              ORG: Precision: 71.05%, Recall: 33.06%, Fscore: 45.13\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 21.740\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 66s 3ms/step - loss: 22.9389 - crf_accuracy: 0.8626 - val_loss: 22.9885 - val_crf_accuracy: 0.8752\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 114 phrases, correct: 83.\n",
      "\n",
      "Accuracy: 83.47%, Precision: 72.81%, Recall: 13.15%, Fscore: 22.28\n",
      "\n",
      "              LOC: Precision: 75.00%, Recall:  1.64%, Fscore:  3.21\n",
      "\n",
      "              ORG: Precision: 72.73%, Recall: 32.65%, Fscore: 45.07\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 22.280\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 67s 3ms/step - loss: 22.8980 - crf_accuracy: 0.8656 - val_loss: 22.9549 - val_crf_accuracy: 0.8778\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 246 phrases, correct: 137.\n",
      "\n",
      "Accuracy: 84.07%, Precision: 55.69%, Recall: 21.71%, Fscore: 31.24\n",
      "\n",
      "              LOC: Precision: 83.33%, Recall:  5.46%, Fscore: 10.26\n",
      "\n",
      "              ORG: Precision: 54.27%, Recall: 51.84%, Fscore: 53.03\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 31.240\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 67s 3ms/step - loss: 22.8612 - crf_accuracy: 0.8682 - val_loss: 22.9164 - val_crf_accuracy: 0.8808\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 247 phrases, correct: 149.\n",
      "\n",
      "Accuracy: 84.55%, Precision: 60.32%, Recall: 23.61%, Fscore: 33.94\n",
      "\n",
      "              LOC: Precision: 72.50%, Recall: 15.85%, Fscore: 26.01\n",
      "\n",
      "              ORG: Precision: 57.97%, Recall: 48.98%, Fscore: 53.10\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 33.940\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 67s 3ms/step - loss: 22.8234 - crf_accuracy: 0.8720 - val_loss: 22.8802 - val_crf_accuracy: 0.8844\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 221 phrases, correct: 152.\n",
      "\n",
      "Accuracy: 85.03%, Precision: 68.78%, Recall: 24.09%, Fscore: 35.68\n",
      "\n",
      "              LOC: Precision: 62.50%, Recall: 27.32%, Fscore: 38.02\n",
      "\n",
      "              ORG: Precision: 72.34%, Recall: 41.63%, Fscore: 52.85\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 35.680\n",
      "Train on 19472 samples, validate on 3339 samples\n",
      "Epoch 1/1\n",
      "19472/19472 [==============================] - 66s 3ms/step - loss: 22.7884 - crf_accuracy: 0.8761 - val_loss: 22.8515 - val_crf_accuracy: 0.8869\n",
      "Total tokens is 3339 and total is phrases 631\n",
      "Found: 262 phrases, correct: 169.\n",
      "\n",
      "Accuracy: 85.15%, Precision: 64.50%, Recall: 26.78%, Fscore: 37.85\n",
      "\n",
      "              LOC: Precision: 68.92%, Recall: 27.87%, Fscore: 39.69\n",
      "\n",
      "              ORG: Precision: 62.77%, Recall: 48.16%, Fscore: 54.50\n",
      "\n",
      "              PER: Precision:  0.00%, Recall:  0.00%, Fscore:  0.00\n",
      "\n",
      "Best F1 score: 37.850\n",
      "Time taken for training BI-LSTM model in 10 epochs is: 802.4845011234283 s\n"
     ]
    }
   ],
   "source": [
    "# Bi-lstm model\n",
    "total_epoch = 10 \n",
    "if selected_model =='bi-lstm':\n",
    "    if restart_flag == False:\n",
    "        restart_flag = True\n",
    "        start_time = time.time()\n",
    "        model= Bilstm()\n",
    "        model.compile(loss=loss, optimizer='adam', metrics=[accuracy])\n",
    "        if os.path.exists('Bi-lstm_weights-100epoch.h5') == True: #load pre-trained model so that you don`t need train from scratch\n",
    "            print('Loading pre-trained weights obtained from the designed Bilstm model pre-trained 100 epoch on Cheaha...')\n",
    "            model.load_weights('Bi-lstm_weights-100epoch.h5')\n",
    "            print('Loading complete!')\n",
    "        else:\n",
    "            print('Bi-lstm model training from beginning...')\n",
    "            for i in range(total_epoch):\n",
    "                model.fit([train_data_lstm[0], train_data_lstm[1]], train_labels, epochs=1, shuffle=True, batch_size=256, validation_data=(evl_data_lstm, evl_labels))\n",
    "                y_pred = model.predict([evl_data_lstm[0], evl_data_lstm[1]], batch_size=128)\n",
    "                pred = string_tag(y_pred, id_to_tag, True)\n",
    "                ground_truth = string_tag(evl_labels, id_to_tag)\n",
    "                info = []\n",
    "                for i in range(len(pred)):\n",
    "                    temp = []\n",
    "                    padding = 'S' # padding information\n",
    "                    ground_Truth = convert_iobes_to_iob_tags(ground_truth[i]) # tag num to strings\n",
    "                    predictions = convert_iobes_to_iob_tags(pred[i]) # tag num to strings\n",
    "                    for padding_char, gT, predict in zip(padding, ground_Truth, predictions):\n",
    "                        temp.append(\" \".join([padding_char, gT, predict]))\n",
    "                    info.append(temp)\n",
    "                lines = evaluate_report(info, folder_path)\n",
    "                for line in lines:\n",
    "                    print(line)\n",
    "                F1 = float(lines[1].strip().split()[-1])\n",
    "                print(\"Best F1 score: {:>.3f}\".format(F1))\n",
    "        model.save_weights('Bi-lstm_weights-{}epoch.h5'.format(total_epoch)) # save the model after training\n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken for training BI-LSTM model in {total_epoch} epochs is: {end_time - start_time} s\")\n",
    "    else:\n",
    "        print('Do not train both models in one time. The GPU memory has been filled up by the last model.  Please restart the kernel to clean the GPU and re-run the codes to train the second model!' )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The result of training the Bi-LSTM Model with 101 epochs\n",
    "\n",
    "```\n",
    "total_epoch = 101\n",
    "BI-lstm model:\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "Input-CN-Char (InputLayer)      (None, 256)          0                                            \n",
    "__________________________________________________________________________________________________\n",
    "Input-CN-Phrase (InputLayer)    (None, 256)          0                                            \n",
    "__________________________________________________________________________________________________\n",
    "embedding_3 (Embedding)         (None, 256, 128)     547456      Input-CN-Char[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "embedding_4 (Embedding)         (None, 256, 20)      80          Input-CN-Phrase[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "concatenate_2 (Concatenate)     (None, 256, 148)     0           embedding_3[0][0]                \n",
    "                                                                 embedding_4[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "bidirectional_2 (Bidirectional) (None, 256, 200)     199200      concatenate_2[0][0]              \n",
    "__________________________________________________________________________________________________\n",
    "dropout_2 (Dropout)             (None, 256, 200)     0           bidirectional_2[0][0]            \n",
    "__________________________________________________________________________________________________\n",
    "crf_2 (CRF)                     (None, 256, 13)      2808        dropout_2[0][0]                  \n",
    "==================================================================================================\n",
    "Total params: 749,544\n",
    "Trainable params: 202,008\n",
    "Non-trainable params: 547,536\n",
    "__________________________________________________________________________________________________\n",
    "Loading pre-trained weights obtained from the designed Bilstm model pre-trained 100 epoch on Cheaha...\n",
    "Loading complete!\n",
    "Epoch 1/1\n",
    "Train on 19472 samples, validate on 3339 samples\n",
    "Epoch 1/1\n",
    "19472/19472 [==============================] - 76s 4ms/step - loss: 22.4698 - crf_accuracy: 0.9583 - val_loss: 22.5969 - val_crf_accuracy: 0.9522\n",
    "Total tokens is 3339 and total is phrases 631\n",
    "Found: 539 phrases, correct: 472.\n",
    "\n",
    "Accuracy: 94.46%, Precision: 87.57%, Recall: 74.80%, Fscore: 80.68\n",
    "\n",
    "              LOC: Precision: 82.74%, Recall: 75.96%, Fscore: 79.20\n",
    "\n",
    "              ORG: Precision: 88.68%, Recall: 76.73%, Fscore: 82.28\n",
    "\n",
    "              PER: Precision: 91.19%, Recall: 71.43%, Fscore: 80.11\n",
    "\n",
    "Best F1 score: 80.680\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-nlp]",
   "language": "python",
   "name": "conda-env-.conda-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
