{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /scratch/local/jieba.cache\n",
      "Loading model cost 0.761 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "/home/larry5/.conda/envs/2020/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/larry5/.conda/envs/2020/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/larry5/.conda/envs/2020/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/larry5/.conda/envs/2020/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/larry5/.conda/envs/2020/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/larry5/.conda/envs/2020/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import pickle\n",
    "import math\n",
    "import jieba\n",
    "jieba.initialize()\n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.crf import crf_log_likelihood\n",
    "from tensorflow.contrib.crf import viterbi_decode\n",
    "from collections import defaultdict, namedtuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Raw Data preprocessing\n",
    "\n",
    "    1) Load data from file. \n",
    "    2) Convert IOB tagging into IOBES tagging. \n",
    "    3) Split data into training data, testing data and evaluation data.\n",
    "    4) Creating item-to-sequence and sequence-to-item dictionaries.\n",
    "    5) Convert chinese characters and tags into sequence.\n",
    "    6) Divide data into batches with fixed length and padding samples with 0 to maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['因', 'O'], ['此', 'O'], ['，', 'O'], ['这', 'O'], ['次', 'O'], ['政', 'O'], ['府', 'O'], ['危', 'O'], ['机', 'O'], ['终', 'O'], ['于', 'O'], ['得', 'O'], ['到', 'O'], ['化', 'O'], ['解', 'O'], ['，', 'O'], ['对', 'O'], ['俄', 'B-LOC'], ['罗', 'I-LOC'], ['斯', 'I-LOC'], ['来', 'O'], ['说', 'O'], ['是', 'O'], ['值', 'O'], ['得', 'O'], ['庆', 'O'], ['幸', 'O'], ['的', 'O'], ['。', 'O']]\n",
      "[['因', 'O'], ['此', 'O'], ['，', 'O'], ['这', 'O'], ['次', 'O'], ['政', 'O'], ['府', 'O'], ['危', 'O'], ['机', 'O'], ['终', 'O'], ['于', 'O'], ['得', 'O'], ['到', 'O'], ['化', 'O'], ['解', 'O'], ['，', 'O'], ['对', 'O'], ['俄', 'B-LOC'], ['罗', 'I-LOC'], ['斯', 'E-LOC'], ['来', 'O'], ['说', 'O'], ['是', 'O'], ['值', 'O'], ['得', 'O'], ['庆', 'O'], ['幸', 'O'], ['的', 'O'], ['。', 'O']]\n",
      "The number of sentences of trainning data is 19472\n",
      "The number of sentences of testing data is 5007\n",
      "The number of sentences of development data is 3339\n",
      "The number of unique Chinese characters is: 4277\n",
      "The number of unique tag characters is: 13\n",
      "[['因', '此', '，', '这', '次', '政', '府', '危', '机', '终', '于', '得', '到', '化', '解', '，', '对', '俄', '罗', '斯', '来', '说', '是', '值', '得', '庆', '幸', '的', '。'], [209, 187, 2, 22, 133, 63, 247, 642, 117, 619, 57, 107, 42, 103, 229, 2, 38, 663, 415, 206, 43, 87, 11, 552, 107, 782, 1130, 3, 4], [1, 3, 0, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 0, 0, 1, 2, 3, 1, 3, 0, 1, 3, 1, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "The number of steps per epoch is 974\n"
     ]
    }
   ],
   "source": [
    "# read sentences from file\n",
    "def load_data(file_path):\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for line in codecs.open(file_path, 'r', 'utf8'):\n",
    "        line = line.rstrip() # Remove any white spaces at the end of the string\n",
    "        if not line:\n",
    "            if len(sent) > 0: # a line with \"\\n\" is used for spliting sentences\n",
    "                sentences.append(sent)\n",
    "                sent = []\n",
    "        else:\n",
    "            word_tag = line.split() # split word and tag\n",
    "            if len(word_tag) == 2:\n",
    "                sent.append(word_tag)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# convert IOB tags to IOBES tags\n",
    "def convert_to_iobes_tags(sentences):   \n",
    "    for index, sent in enumerate(sentences):\n",
    "        iob_tags = [word_tag[-1] for word_tag in sent] # obtain iob tags of a sentence\n",
    "        iobes_tags = [] # iobes tags\n",
    "        for i, tag in enumerate(iob_tags): \n",
    "            if tag == 'O': # O tag is unchanged\n",
    "                iobes_tags.append(tag)\n",
    "            elif tag.split('-')[0] == 'B':  # B to S if an entity only includes a single word\n",
    "                if i + 1 < len(iob_tags) and iob_tags[i + 1].split('-')[0] == 'I':\n",
    "                    iobes_tags.append(tag)\n",
    "                else:\n",
    "                    iobes_tags.append(tag.replace('B-', 'S-'))\n",
    "            elif tag.split('-')[0] == 'I':  # E is used for the last item of an entity (words > 2)\n",
    "                if i + 1 < len(iob_tags) and iob_tags[i + 1].split('-')[0] == 'I':\n",
    "                    iobes_tags.append(tag)\n",
    "                else:\n",
    "                    iobes_tags.append(tag.replace('I-', 'E-'))\n",
    "            else:\n",
    "                print('ERROR: INVALID IOB TAGGING!')  \n",
    "        for word, iobes_tag in zip(sent, iobes_tags): # replace tags\n",
    "            word[-1] = iobes_tag\n",
    "            \n",
    "            \n",
    "# split sentences into train, test, dev\n",
    "def split_data(sentences):\n",
    "    train_div = int(len(sentences) * 0.7);  # train set divide number\n",
    "    train_sentences = sentences[:train_div]\n",
    "    remaining_sentences = sentences[train_div:] \n",
    "    test_div = int(len(remaining_sentences)* 0.6)  # test set divide number\n",
    "    test_sentences = remaining_sentences[:test_div]\n",
    "    dev_sentences = remaining_sentences[test_div:]\n",
    "    return train_sentences, test_sentences, dev_sentences\n",
    "\n",
    "\n",
    "# creating dictionaries from unique chinese characters to unique id\n",
    "def create_char_id_convert_dict(sentences):\n",
    "    char_dict = {} # a dictionary of the frequency of unique chinese characters\n",
    "    chinese_chars = [[word[0] for word in sent] for sent in sentences] # get words from tupe word_tag\n",
    "    for chars in chinese_chars: # get frequency of unique chinese characters\n",
    "        for char in chars:\n",
    "            if char not in char_dict:\n",
    "                char_dict[char] = 1\n",
    "            else:\n",
    "                char_dict[char] += 1\n",
    "    char_dict[\"<PAD>\"] = 99999 # spacial word for padding, and intial a largest frequency\n",
    "    char_dict['<UNK>'] = 99998 # spacial word for unkonwn, and intial a second largest frequency\n",
    "    # sort characters by frequency (highest to samllest)\n",
    "    sorted_char_dict = sorted(char_dict.items(), key=lambda x: (-x[1], x[0])) \n",
    "    # create two dictionaries: find char by id, or find id by char\n",
    "    id_to_char = {index: value[0] for index, value in enumerate(sorted_char_dict)} \n",
    "    char_to_id = {value: key for key, value in id_to_char.items()}\n",
    "    return id_to_char, char_to_id\n",
    "\n",
    "\n",
    "# creating dictionaries from unique tag to unique id\n",
    "def create_tag_id_convert_dict(sentences):\n",
    "    tag_dict = {} # a dictionary of the frequency of tags\n",
    "    tags = [[word[1] for word in sent] for sent in sentences]\n",
    "    for tag in tags: # get frequency of unique chinese characters\n",
    "        for t in tag:\n",
    "            if t not in tag_dict:\n",
    "                tag_dict[t] = 1\n",
    "            else:\n",
    "                tag_dict[t] += 1\n",
    "    # sort characters by frequency (highest to samllest)\n",
    "    sorted_tag_dict = sorted(tag_dict.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # create two dictionaries: find tag by id, or find id by tag\n",
    "    id_to_tag = {index: value[0] for index, value in enumerate(sorted_tag_dict)} \n",
    "    tag_to_id = {value: key for key, value in id_to_tag.items()}\n",
    "    return id_to_tag, tag_to_id\n",
    "\n",
    "\n",
    "# Generated formated data for training\n",
    "def get_formated_data(sentences, char_to_id, tag_to_id):\n",
    "    formated_data = []\n",
    "    for sent in sentences:\n",
    "        sent_chars = [word[0] for word in sent] # get chinese chars\n",
    "        # convert chars to id\n",
    "        chars_id = [char_to_id[char if char in char_to_id else '<UNK>'] for char in sent_chars] \n",
    "        joined_sent = \"\".join(sent_chars) # joined all the chars into a sentence\n",
    "\n",
    "        # Tokenize sent with Jieba to get chinese phrase feature (the start, inside, and end of a phrase)\n",
    "        phrase_feature = []\n",
    "        for token in jieba.cut(joined_sent):\n",
    "            if len(token) == 1: # phrase_feature is 0 if a phase only has one Chinese character\n",
    "                phrase_feature.append(0)\n",
    "            else:\n",
    "                phrase_list = [2] * len(token) # phrase_feature of middle characters in a phase is 2\n",
    "                phrase_list[0] = 1 # phrase_feature of start character in a phase is 1\n",
    "                phrase_list[-1] = 3 # phrase_feature of end character in a phase is 3\n",
    "                phrase_feature.extend(phrase_list)\n",
    "\n",
    "        tags_id = [tag_to_id[word[-1]] for word in sent] # convert tags to id\n",
    "        formated_data.append([sent_chars, chars_id, phrase_feature, tags_id]) # formated data\n",
    "    return formated_data\n",
    "\n",
    "\n",
    "# Divide data into batches and padding each sample\n",
    "def generate_batch_data_with_padding(data, bcount):\n",
    "    batches = []\n",
    "    batch_count = int(math.ceil(len(data)/ bcount)) # calulate number of batches\n",
    "    # sorted list based on the length of sentences(short to long)\n",
    "    sorted_len_data = sorted(train_data, key=lambda x: len(x[0]))\n",
    "    for i in range(batch_count):\n",
    "        batch = sorted_len_data[(i * bcount) : ((i + 1) * bcount)] # divided data into batches with fixed length\n",
    "        pad_sentsents = [] # sentsents after padding\n",
    "        pad_chars = [] # chinese characters after padding\n",
    "        pad_phrases = [] # pahrase features after padding\n",
    "        pad_tags = [] # tags after padding\n",
    "        max_length = max([len(sample[0]) for sample in batch]) # find the max length of sentence in batch\n",
    "        for sample in batch:\n",
    "            sent, char, phrase, tag = sample \n",
    "            pad_array = [0] * (max_length - len(sent)) # padding with 0 based on the max length\n",
    "            pad_sentsents.append(sent + pad_array) \n",
    "            pad_chars.append(char + pad_array)\n",
    "            pad_phrases.append(phrase + pad_array)\n",
    "            pad_tags.append(tag + pad_array)    \n",
    "        batches.append([pad_sentsents, pad_chars, pad_phrases, pad_tags]) # get batch data\n",
    "    return batches\n",
    "\n",
    "\n",
    "\n",
    "# data processing\n",
    "folder_patch = \"./dataset/\"  # dataset folder\n",
    "data_path = folder_patch + \"data.txt\" # data path\n",
    "\n",
    "sentences = load_data(data_path) # load data\n",
    "print(sentences[0]) \n",
    "\n",
    "convert_to_iobes_tags(sentences) # convert to iobes tags\n",
    "print(sentences[0]) \n",
    "\n",
    "train_sentences, test_sentences, dev_sentences = split_data(sentences) # split data \n",
    "print(\"The number of sentences of trainning data is\", len(train_sentences))\n",
    "print(\"The number of sentences of testing data is\", len(test_sentences))\n",
    "print(\"The number of sentences of development data is\", len(dev_sentences))\n",
    "\n",
    "# creates chinese characters and senquence convertion dictionaries\n",
    "id_to_char, char_to_id = create_char_id_convert_dict(train_sentences) \n",
    "# creates tags and senquence convertion dictionaries\n",
    "id_to_tag, tag_to_id = create_tag_id_convert_dict(train_sentences)\n",
    "print(\"The number of unique Chinese characters is:\", len(char_to_id))\n",
    "print(\"The number of unique tag characters is:\", len(tag_to_id))\n",
    "\n",
    "train_data = get_formated_data(train_sentences, char_to_id, tag_to_id) # formated training data\n",
    "test_data = get_formated_data(test_sentences, char_to_id, tag_to_id) # formated testing data\n",
    "dev_data = get_formated_data(dev_sentences, char_to_id, tag_to_id) # formated edata\n",
    "print(train_data[0])\n",
    "\n",
    "with open(folder_patch + 'dict.pkl', \"wb\") as out_file:  # dump data for eveluation \n",
    "    pickle.dump([char_to_id, id_to_char, tag_to_id, id_to_tag], out_file)\n",
    "\n",
    "# generate batches with padding\n",
    "train_batch_data = generate_batch_data_with_padding(train_data, 20) \n",
    "dev_batch_data = generate_batch_data_with_padding(dev_data, 100)\n",
    "test_batch_data = generate_batch_data_with_padding(test_data, 100)\n",
    "\n",
    "epoch_iterations = len(train_batch_data) # set the iterations per epoch\n",
    "print(\"The number of steps per epoch is\", epoch_iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Build the model and set up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure with the model:\n",
    "# you can make modificiations on these hyperparameters\n",
    "learning_rate = 0.001\n",
    "channel_char = 100\n",
    "channel_seg =20\n",
    "channel_lstm = 100\n",
    "len_tags = len(tag_to_id)\n",
    "len_char = len(char_to_id)\n",
    "\n",
    "# build model\n",
    "class Model(object):\n",
    "    def __init__(self,mode):\n",
    "#         mode type, check whether the model is running for training or not\n",
    "        if mode == 'training':\n",
    "            self.model_training = True\n",
    "        else:\n",
    "            print('Mode Errors!Please Choose Correct Training Mode')\n",
    "        self.__main_setup() # model initializing\n",
    "        \n",
    "    def __main_setup(self):\n",
    "        self.__hyper() #set up hyperparameters\n",
    "        self.__placeholder() #build tensor holder\n",
    "        self.__parameters() #initializing\n",
    "        self.__layers() #create model\n",
    "        self.__opt() #optimizer\n",
    "        \n",
    "    def __layers(self):\n",
    "        self.__embedding() #embedding layers\n",
    "        self.__dilated() # iterated dilated cnn \n",
    "        self.__loss() \n",
    "        \n",
    "    def __hyper(self):\n",
    "        self.learningR = learning_rate #learning rate \n",
    "#         embedding dimention\n",
    "        self.channel_char = channel_char  \n",
    "        self.channel_seg = channel_seg\n",
    "        self.channel_lstm = channel_lstm\n",
    "        self.len_tags = len_tags # number of tags\n",
    "        self.len_chars = len_char #unique Chinese char\n",
    "        self.output_channel = 0\n",
    "        \n",
    "    def __placeholder(self):\n",
    "        self.gt = tf.placeholder(dtype=tf.int32) #GT\n",
    "        self.f1_evaluate = tf.Variable(dtype=tf.float32,initial_value=0.0, trainable=False) #best f1 score for evaluate data\n",
    "        self.f1_test = tf.Variable(dtype=tf.float32,initial_value=0.0, trainable=False) #for test data\n",
    "        self.whole_steps = tf.Variable(dtype=tf.int32,initial_value=0, trainable=False) #steps for training process\n",
    "        self.cn_char = tf.placeholder(dtype=tf.int32) #input sentence\n",
    "        self.cn_segment = tf.placeholder(dtype=tf.int32) #nput Chinese segmentation features\n",
    "        self.dropout = tf.placeholder(dtype=tf.float32) #dropout\n",
    "        \n",
    "    def __parameters(self):\n",
    "        self.len_segment = 4  #segement features 0,1,2,3\n",
    "        length = tf.reduce_sum(tf.sign(tf.abs(self.cn_char)), reduction_indices=1)\n",
    "        self.lengths = tf.cast(length, tf.int32)\n",
    "        self.batch_size = tf.shape(self.cn_char)[0] #batch_size\n",
    "        self.num_steps = tf.shape(self.cn_char)[-1] #num_steps: total chars in each sentenc\n",
    "        self.layers = [1,1,2] #dilation\n",
    "        self.flag_drop = 0.5\n",
    "        if self.model_training == False:\n",
    "            self.flag_drop = 1.0\n",
    "        self.num_filter = 3\n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        self.channel_embedding = self.channel_char + self.channel_seg  # char channels + segmentation channels\n",
    "        self.iterations = 4\n",
    "        \n",
    "    def __embedding(self):        \n",
    "        # initializing for two features\n",
    "        with tf.name_scope('Embedding_1'):  #1. the unique Chinese char \n",
    "            char_embeddings = tf.get_variable('Embeddings_1',shape=[self.len_chars, self.channel_char],\n",
    "                initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None),\n",
    "                dtype=tf.float32,trainable=True)\n",
    "            feature_1 = tf.nn.embedding_lookup(char_embeddings, self.cn_char)\n",
    "            with tf.name_scope('Embedding_2'): # 2. the word length and location \n",
    "                seg_embeddings = tf.get_variable('Embeddings_2',shape=[self.len_segment, self.channel_seg],\n",
    "                    initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None),\n",
    "                    dtype=tf.float32,trainable=True)\n",
    "                feature_2 = tf.nn.embedding_lookup(seg_embeddings, self.cn_segment)\n",
    "            self.embedding_cns = tf.concat([feature_1,feature_2], axis=-1)\n",
    "#         apply dropout \n",
    "        self.embedding_cns = tf.nn.dropout(self.embedding_cns, self.dropout)\n",
    "    \n",
    "    def __dilated(self):\n",
    "#         core networks\n",
    "        core_input = tf.expand_dims(self.embedding_cns, 1) # shape(?, ?, 120) ——> shape(?, 1, ?, 120)  \n",
    "        with tf.variable_scope(\"Core_nets\"):\n",
    "            initialed_weight = tf.get_variable(\"Idcnn_filter\",shape=[1, self.num_filter, self.channel_embedding,\n",
    "                       self.channel_lstm],initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "            core_Input = tf.nn.conv2d(core_input, initialed_weight, strides=[1, 1, 1, 1],  padding=\"SAME\",name=\"core_input\")\n",
    "            output = []\n",
    "            channels = 0\n",
    "            for j in range(self.iterations):  \n",
    "                for i in range(len(self.layers)):\n",
    "                    dilated_rate = self.layers[i]\n",
    "                    if i == (len(self.layers) - 1):\n",
    "                        last_layer = True\n",
    "                    else:\n",
    "                        last_layer = False\n",
    "                    with tf.variable_scope(\"Dilated-Conv-%d\" % i, reuse=tf.AUTO_REUSE):\n",
    "                        weights = tf.get_variable(name='Weights',shape=[1, self.num_filter, self.channel_lstm,self.channel_lstm],\n",
    "                            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "                        biases = tf.get_variable(name='Biases',shape=[self.channel_lstm])\n",
    "                        c = tf.nn.atrous_conv2d(core_Input,weights, rate=dilated_rate, padding=\"SAME\") # dilated convolution\n",
    "                        c = tf.nn.bias_add(c, biases)\n",
    "                        c = tf.nn.relu(c)\n",
    "                        if last_layer:\n",
    "                            channels += self.channel_lstm\n",
    "                            output.append(c)\n",
    "                        core_Input = c\n",
    "            output_last = tf.concat(values=output,axis=3) # 4 layers features\n",
    "            output_last = tf.nn.dropout(output_last, self.flag_drop)\n",
    "#             drop dimention: the dimention which contians only one data\n",
    "            output_last = tf.squeeze(output_last, [1])\n",
    "            output_last = tf.reshape(output_last, [-1, channels]) # final features done\n",
    "            self.output_channel = channels\n",
    "            with tf.variable_scope(\"Fully\"):\n",
    "                with tf.variable_scope(\"Unit\"):\n",
    "                    weight = tf.get_variable(\"Weight\", shape=[self.output_channel, self.len_tags],\n",
    "                                        dtype=tf.float32, initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "                    bias = tf.get_variable(\"Bias\",  initializer=tf.constant(0.0001, shape=[self.len_tags]))\n",
    "#                    matmul(x, w) + b.\n",
    "                    result = tf.nn.xw_plus_b(output_last, weight, bias)\n",
    "            self.result =  tf.reshape(result, [-1, self.num_steps, self.len_tags])  # num_steps: total chars in each sentenc, len_tags: number of tags\n",
    "\n",
    "    def __loss(self):\n",
    "        # loss \n",
    "        with tf.variable_scope(\"CRF\"):\n",
    "            small = -1000.0\n",
    "            # pad units \n",
    "            initial_units = tf.concat([small * tf.ones(shape=[self.batch_size, 1, self.len_tags]), tf.zeros(shape=[self.batch_size, 1, 1])], axis=-1)\n",
    "            pad_units = tf.cast(small * tf.ones([self.batch_size, self.num_steps, 1]), tf.float32)\n",
    "            temp = tf.concat([self.result, pad_units], axis=-1)\n",
    "            temp = tf.concat([initial_units, temp], axis=1)\n",
    "            gt = tf.concat([tf.cast(self.len_tags*tf.ones([self.batch_size, 1]), tf.int32), self.gt], axis=-1)\n",
    "            self.transition = tf.get_variable(\"transit\",shape=[self.len_tags + 1, self.len_tags + 1],\n",
    "                initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "            likelihood, self.transition = crf_log_likelihood(inputs=temp,tag_indices=gt,transition_params=self.transition,\n",
    "                sequence_lengths=self.lengths+1)\n",
    "            self.error = tf.reduce_mean(likelihood*(-1))\n",
    "            \n",
    "    def __opt(self):\n",
    "        with tf.variable_scope(\"optimizer\"):\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learningR)\n",
    "            # apply grad clip to avoid gradient explosion\n",
    "            gradients = self.optimizer.compute_gradients(self.error)\n",
    "            limited_gradients = [[tf.clip_by_value(gra, -4, 4), va] for gra, va in gradients]\n",
    "            self.optimize = self.optimizer.apply_gradients(limited_gradients, self.whole_steps)\n",
    "            \n",
    "    def evaluate(self, sess, batch_data, id_to_tag):\n",
    "        transition = self.transition.eval()\n",
    "        report = []\n",
    "        for batch in batch_data:\n",
    "            cn_sentences = batch[0]\n",
    "            tags = batch[-1] #true tag\n",
    "            lengths, scores = self.each_step(sess, False, batch)\n",
    "            batch_paths = self.viterbi(scores, lengths, transition)\n",
    "            for i in range(len(cn_sentences)):\n",
    "                output = []\n",
    "                sentence = cn_sentences[i][:lengths[i]]\n",
    "                gt = convert_iobes_to_iob_tags([id_to_tag[int(x)] for x in tags[i][:lengths[i]]])\n",
    "                predict = convert_iobes_to_iob_tags([id_to_tag[int(x)] for x in batch_paths[i][:lengths[i]]])\n",
    "                for cn_char, gt, predict in zip(sentence, gt, predict):\n",
    "                    output.append(\" \".join([cn_char, gt, predict]))\n",
    "                report.append(output)\n",
    "        return report\n",
    "    \n",
    "    def viterbi(self, units, lengths, array):\n",
    "        # viterbi Algorithm\n",
    "        paths = []\n",
    "        begin = np.asarray([[-1000.0]*self.len_tags +[0]])\n",
    "        for val, temp_len in zip(units, lengths):\n",
    "            val = val[:temp_len]\n",
    "            pad = np.ones([temp_len, 1]) * (-1000.0)\n",
    "            units = np.concatenate([val, pad], axis=1)\n",
    "            units = np.concatenate([begin, units], axis=0)\n",
    "            path, _ = viterbi_decode(units, array)\n",
    "            paths.append(path[1:])\n",
    "        return paths\n",
    "    \n",
    "    def each_step(self, sess, training, batch):\n",
    "        _, cn_char, cn_segment, tags = batch\n",
    "        temp_dict = {\n",
    "            self.cn_char: np.asarray(cn_char),# char id\n",
    "            self.cn_segment: np.asarray(cn_segment), #segmentation\n",
    "            self.dropout: 1.0, # val or test do not need to dropout\n",
    "        }\n",
    "        if training:\n",
    "            temp_dict[self.gt] = np.asarray(tags) #GT\n",
    "            temp_dict[self.dropout] = 0.6\n",
    "            whole_steps, error, _ = sess.run(\n",
    "                [self.whole_steps, self.error, self.optimize],\n",
    "                temp_dict)\n",
    "            return whole_steps, error\n",
    "        else:\n",
    "            lengths, units = sess.run([self.lengths, self.result], temp_dict)\n",
    "            return lengths, units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-2020]",
   "language": "python",
   "name": "conda-env-.conda-2020-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
