{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /scratch/local/jieba.cache\n",
      "Loading model cost 0.761 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "/home/larry5/.conda/envs/2020/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/larry5/.conda/envs/2020/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/larry5/.conda/envs/2020/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/larry5/.conda/envs/2020/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/larry5/.conda/envs/2020/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/larry5/.conda/envs/2020/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import pickle\n",
    "import math\n",
    "import jieba\n",
    "jieba.initialize()\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.crf import crf_log_likelihood\n",
    "from tensorflow.contrib.crf import viterbi_decode\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import concat, placeholder, reduce_sum, Variable, expand_dims, reduce_mean\n",
    "from tensorflow import abs, get_variable, variable_scope, sign, reshape, cast, squeeze, shape\n",
    "from tensorflow.nn import embedding_lookup as embed\n",
    "from tensorflow.nn import dropout, atrous_conv2d, conv2d, bias_add, relu, xw_plus_b\n",
    "from collections import defaultdict, namedtuple\n",
    "from keras.models import Model as Model_init\n",
    "from keras.layers import  LSTM, Bidirectional, Input, Embedding, Concatenate, Dropout\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss as loss\n",
    "from keras_contrib.metrics import crf_accuracy as accuracy\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Raw Data preprocessing\n",
    "\n",
    "    1) Load data from file. \n",
    "    2) Convert IOB tagging into IOBES tagging. \n",
    "    3) Split data into training data, testing data and evaluation data.\n",
    "    4) Creating item-to-sequence and sequence-to-item dictionaries.\n",
    "    5) Convert chinese characters and tags into sequence.\n",
    "    6) Divide data into batches with fixed length and padding samples with 0 to maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['因', 'O'], ['此', 'O'], ['，', 'O'], ['这', 'O'], ['次', 'O'], ['政', 'O'], ['府', 'O'], ['危', 'O'], ['机', 'O'], ['终', 'O'], ['于', 'O'], ['得', 'O'], ['到', 'O'], ['化', 'O'], ['解', 'O'], ['，', 'O'], ['对', 'O'], ['俄', 'B-LOC'], ['罗', 'I-LOC'], ['斯', 'I-LOC'], ['来', 'O'], ['说', 'O'], ['是', 'O'], ['值', 'O'], ['得', 'O'], ['庆', 'O'], ['幸', 'O'], ['的', 'O'], ['。', 'O']]\n",
      "[['因', 'O'], ['此', 'O'], ['，', 'O'], ['这', 'O'], ['次', 'O'], ['政', 'O'], ['府', 'O'], ['危', 'O'], ['机', 'O'], ['终', 'O'], ['于', 'O'], ['得', 'O'], ['到', 'O'], ['化', 'O'], ['解', 'O'], ['，', 'O'], ['对', 'O'], ['俄', 'B-LOC'], ['罗', 'I-LOC'], ['斯', 'E-LOC'], ['来', 'O'], ['说', 'O'], ['是', 'O'], ['值', 'O'], ['得', 'O'], ['庆', 'O'], ['幸', 'O'], ['的', 'O'], ['。', 'O']]\n",
      "The number of sentences of trainning data is 19472\n",
      "The number of sentences of testing data is 5007\n",
      "The number of sentences of development data is 3339\n",
      "The number of unique Chinese characters is: 4277\n",
      "The number of unique tag characters is: 13\n",
      "[['因', '此', '，', '这', '次', '政', '府', '危', '机', '终', '于', '得', '到', '化', '解', '，', '对', '俄', '罗', '斯', '来', '说', '是', '值', '得', '庆', '幸', '的', '。'], [209, 187, 2, 22, 133, 63, 247, 642, 117, 619, 57, 107, 42, 103, 229, 2, 38, 663, 415, 206, 43, 87, 11, 552, 107, 782, 1130, 3, 4], [1, 3, 0, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 0, 0, 1, 2, 3, 1, 3, 0, 1, 3, 1, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "The number of steps per epoch is 974\n"
     ]
    }
   ],
   "source": [
    "# read sentences from file\n",
    "def load_data(file_path):\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for line in codecs.open(file_path, 'r', 'utf8'):\n",
    "        line = line.rstrip() # Remove any white spaces at the end of the string\n",
    "        if not line:\n",
    "            if len(sent) > 0: # a line with \"\\n\" is used for spliting sentences\n",
    "                sentences.append(sent)\n",
    "                sent = []\n",
    "        else:\n",
    "            word_tag = line.split() # split word and tag\n",
    "            if len(word_tag) == 2:\n",
    "                sent.append(word_tag)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# convert IOB tags to IOBES tags\n",
    "def convert_to_iobes_tags(sentences):   \n",
    "    for index, sent in enumerate(sentences):\n",
    "        iob_tags = [word_tag[-1] for word_tag in sent] # obtain iob tags of a sentence\n",
    "        iobes_tags = [] # iobes tags\n",
    "        for i, tag in enumerate(iob_tags): \n",
    "            if tag == 'O': # O tag is unchanged\n",
    "                iobes_tags.append(tag)\n",
    "            elif tag.split('-')[0] == 'B':  # B to S if an entity only includes a single word\n",
    "                if i + 1 < len(iob_tags) and iob_tags[i + 1].split('-')[0] == 'I':\n",
    "                    iobes_tags.append(tag)\n",
    "                else:\n",
    "                    iobes_tags.append(tag.replace('B-', 'S-'))\n",
    "            elif tag.split('-')[0] == 'I':  # E is used for the last item of an entity (words > 2)\n",
    "                if i + 1 < len(iob_tags) and iob_tags[i + 1].split('-')[0] == 'I':\n",
    "                    iobes_tags.append(tag)\n",
    "                else:\n",
    "                    iobes_tags.append(tag.replace('I-', 'E-'))\n",
    "            else:\n",
    "                print('ERROR: INVALID IOB TAGGING!')  \n",
    "        for word, iobes_tag in zip(sent, iobes_tags): # replace tags\n",
    "            word[-1] = iobes_tag\n",
    "            \n",
    "            \n",
    "# split sentences into train, test, dev\n",
    "def split_data(sentences):\n",
    "    train_div = int(len(sentences) * 0.7);  # train set divide number\n",
    "    train_sentences = sentences[:train_div]\n",
    "    remaining_sentences = sentences[train_div:] \n",
    "    test_div = int(len(remaining_sentences)* 0.6)  # test set divide number\n",
    "    test_sentences = remaining_sentences[:test_div]\n",
    "    dev_sentences = remaining_sentences[test_div:]\n",
    "    return train_sentences, test_sentences, dev_sentences\n",
    "\n",
    "\n",
    "# creating dictionaries from unique chinese characters to unique id\n",
    "def create_char_id_convert_dict(sentences):\n",
    "    char_dict = {} # a dictionary of the frequency of unique chinese characters\n",
    "    chinese_chars = [[word[0] for word in sent] for sent in sentences] # get words from tupe word_tag\n",
    "    for chars in chinese_chars: # get frequency of unique chinese characters\n",
    "        for char in chars:\n",
    "            if char not in char_dict:\n",
    "                char_dict[char] = 1\n",
    "            else:\n",
    "                char_dict[char] += 1\n",
    "    char_dict[\"<PAD>\"] = 99999 # spacial word for padding, and intial a largest frequency\n",
    "    char_dict['<UNK>'] = 99998 # spacial word for unkonwn, and intial a second largest frequency\n",
    "    # sort characters by frequency (highest to samllest)\n",
    "    sorted_char_dict = sorted(char_dict.items(), key=lambda x: (-x[1], x[0])) \n",
    "    # create two dictionaries: find char by id, or find id by char\n",
    "    id_to_char = {index: value[0] for index, value in enumerate(sorted_char_dict)} \n",
    "    char_to_id = {value: key for key, value in id_to_char.items()}\n",
    "    return id_to_char, char_to_id\n",
    "\n",
    "\n",
    "# creating dictionaries from unique tag to unique id\n",
    "def create_tag_id_convert_dict(sentences):\n",
    "    tag_dict = {} # a dictionary of the frequency of tags\n",
    "    tags = [[word[1] for word in sent] for sent in sentences]\n",
    "    for tag in tags: # get frequency of unique chinese characters\n",
    "        for t in tag:\n",
    "            if t not in tag_dict:\n",
    "                tag_dict[t] = 1\n",
    "            else:\n",
    "                tag_dict[t] += 1\n",
    "    # sort characters by frequency (highest to samllest)\n",
    "    sorted_tag_dict = sorted(tag_dict.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # create two dictionaries: find tag by id, or find id by tag\n",
    "    id_to_tag = {index: value[0] for index, value in enumerate(sorted_tag_dict)} \n",
    "    tag_to_id = {value: key for key, value in id_to_tag.items()}\n",
    "    return id_to_tag, tag_to_id\n",
    "\n",
    "\n",
    "# Generated formated data for training\n",
    "def get_formated_data(sentences, char_to_id, tag_to_id):\n",
    "    formated_data = []\n",
    "    for sent in sentences:\n",
    "        sent_chars = [word[0] for word in sent] # get chinese chars\n",
    "        # convert chars to id\n",
    "        chars_id = [char_to_id[char if char in char_to_id else '<UNK>'] for char in sent_chars] \n",
    "        joined_sent = \"\".join(sent_chars) # joined all the chars into a sentence\n",
    "\n",
    "        # Tokenize sent with Jieba to get chinese phrase feature (the start, inside, and end of a phrase)\n",
    "        phrase_feature = []\n",
    "        for token in jieba.cut(joined_sent):\n",
    "            if len(token) == 1: # phrase_feature is 0 if a phase only has one Chinese character\n",
    "                phrase_feature.append(0)\n",
    "            else:\n",
    "                phrase_list = [2] * len(token) # phrase_feature of middle characters in a phase is 2\n",
    "                phrase_list[0] = 1 # phrase_feature of start character in a phase is 1\n",
    "                phrase_list[-1] = 3 # phrase_feature of end character in a phase is 3\n",
    "                phrase_feature.extend(phrase_list)\n",
    "\n",
    "        tags_id = [tag_to_id[word[-1]] for word in sent] # convert tags to id\n",
    "        formated_data.append([sent_chars, chars_id, phrase_feature, tags_id]) # formated data\n",
    "    return formated_data\n",
    "\n",
    "\n",
    "# Divide data into batches and padding each sample\n",
    "def generate_batch_data_with_padding(data, bcount):\n",
    "    batches = []\n",
    "    batch_count = int(math.ceil(len(data)/ bcount)) # calulate number of batches\n",
    "    # sorted list based on the length of sentences(short to long)\n",
    "    sorted_len_data = sorted(train_data, key=lambda x: len(x[0]))\n",
    "    for i in range(batch_count):\n",
    "        batch = sorted_len_data[(i * bcount) : ((i + 1) * bcount)] # divided data into batches with fixed length\n",
    "        pad_sentsents = [] # sentsents after padding\n",
    "        pad_chars = [] # chinese characters after padding\n",
    "        pad_phrases = [] # pahrase features after padding\n",
    "        pad_tags = [] # tags after padding\n",
    "        max_length = max([len(sample[0]) for sample in batch]) # find the max length of sentence in batch\n",
    "        for sample in batch:\n",
    "            sent, char, phrase, tag = sample \n",
    "            pad_array = [0] * (max_length - len(sent)) # padding with 0 based on the max length\n",
    "            pad_sentsents.append(sent + pad_array) \n",
    "            pad_chars.append(char + pad_array)\n",
    "            pad_phrases.append(phrase + pad_array)\n",
    "            pad_tags.append(tag + pad_array)    \n",
    "        batches.append([pad_sentsents, pad_chars, pad_phrases, pad_tags]) # get batch data\n",
    "    return batches\n",
    "\n",
    "\n",
    "\n",
    "# data processing\n",
    "folder_patch = \"./dataset/\"  # dataset folder\n",
    "data_path = folder_patch + \"data.txt\" # data path\n",
    "\n",
    "sentences = load_data(data_path) # load data\n",
    "print(sentences[0]) \n",
    "\n",
    "convert_to_iobes_tags(sentences) # convert to iobes tags\n",
    "print(sentences[0]) \n",
    "\n",
    "train_sentences, test_sentences, dev_sentences = split_data(sentences) # split data \n",
    "print(\"The number of sentences of trainning data is\", len(train_sentences))\n",
    "print(\"The number of sentences of testing data is\", len(test_sentences))\n",
    "print(\"The number of sentences of development data is\", len(dev_sentences))\n",
    "\n",
    "# creates chinese characters and senquence convertion dictionaries\n",
    "id_to_char, char_to_id = create_char_id_convert_dict(train_sentences) \n",
    "# creates tags and senquence convertion dictionaries\n",
    "id_to_tag, tag_to_id = create_tag_id_convert_dict(train_sentences)\n",
    "print(\"The number of unique Chinese characters is:\", len(char_to_id))\n",
    "print(\"The number of unique tag characters is:\", len(tag_to_id))\n",
    "\n",
    "train_data = get_formated_data(train_sentences, char_to_id, tag_to_id) # formated training data\n",
    "test_data = get_formated_data(test_sentences, char_to_id, tag_to_id) # formated testing data\n",
    "dev_data = get_formated_data(dev_sentences, char_to_id, tag_to_id) # formated edata\n",
    "print(train_data[0])\n",
    "\n",
    "with open(folder_patch + 'dict.pkl', \"wb\") as out_file:  # dump data for eveluation \n",
    "    pickle.dump([char_to_id, id_to_char, tag_to_id, id_to_tag], out_file)\n",
    "\n",
    "# Hyper Parameters:\n",
    "selected_model = 'Bi-lstm' # load the Bi-lstm model\n",
    "selected_model = False # uncommented this line will load the Iterated Dilated Convolutions model\n",
    "learning_rate = 0.001\n",
    "channel_char = 128 #embedding output dimention for char\n",
    "channel_phrase =20 #embedding output dimention for phrase\n",
    "channel_lstm = 256 #input dimention for Bi-lstm\n",
    "len_tags = len(tag_to_id)\n",
    "len_char = len(char_to_id)\n",
    "\n",
    "# generate batches with padding\n",
    "train_batch_data = generate_batch_data_with_padding(train_data, 20) \n",
    "dev_batch_data = generate_batch_data_with_padding(dev_data, 100)\n",
    "test_batch_data = generate_batch_data_with_padding(test_data, 100)\n",
    "\n",
    "epoch_iterations = len(train_batch_data) # set the iterations per epoch\n",
    "print(\"The number of steps per epoch is\", epoch_iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Build the model and set up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-lstm model\n",
    "# Because we take advantage of two types of features: cn_chars, cn_phrase, there will be two input layers\n",
    "def Bilstm():\n",
    "    crf = CRF(len(tag_to_id), sparse_target=True) # define the crf layer at first\n",
    "    char_input = Input(shape=(channel_lstm,), name=\"Input-CN-Char\") # the shape must be the max size of a sentence, it can be changed for long sentence. However, the too many neurons will slow down the model dramatically\n",
    "    char_emb = Embedding(len(char_to_id),output_dim=channel_char,trainable=False,mask_zero=True)(char_input) \n",
    "    phrase_input = Input(shape=(channel_lstm,), name=\"Input-CN-Phrase\")# the shape must be the max size of a sentence\n",
    "    phrase_emb = Embedding(input_dim=4,output_dim=20,trainable=False,mask_zero=True)(phrase_input) \n",
    "    # concatenate them to makes a single vector\n",
    "    merged = Concatenate(axis=-1)([char_emb, phrase_emb])\n",
    "#     dropout = Dropout(0.5)(merged) #prevent from overfitting; if encounter severe overfitting, uncomment this line and change the input layer of the next layer\n",
    "    lstm = Bidirectional(LSTM(100, return_sequences=True))(merged)\n",
    "    dropout = Dropout(0.5)(lstm) #prevent from overfitting\n",
    "    CRF_layer = crf(dropout)\n",
    "    model = Model_init(inputs=[char_input, phrase_input], outputs=[CRF_layer]) #explicate the input list.\n",
    "    model.summary() # plot the configure\n",
    "    plot_model(model, to_file='BI-lstm model.png') #output the figure of model structure\n",
    "    return model\n",
    "\n",
    "class Model(object): # <Fast and Accurate Entity Recognition with Iterated Dilated Convolutions>\n",
    "    def __init__(self):\n",
    "        self.__main_setup() # model initializing\n",
    "        \n",
    "    def __main_setup(self):\n",
    "        self.__hyper() #set up hyperparameters\n",
    "        self.__placeholder() #build tensor holder\n",
    "        self.__parameters() #initializing\n",
    "        self.__layers() #create model\n",
    "        self.__opt() #optimizer\n",
    "        \n",
    "    def __layers(self):\n",
    "        self.__embedding() #embedding layers\n",
    "        self.__dilated() # iterated dilated cnn \n",
    "        self.__loss() \n",
    "        \n",
    "    def __hyper(self):\n",
    "        self.learningR = learning_rate #learning rate \n",
    "        self.channel_char = channel_char  # char embedding dimention\n",
    "        self.channel_phrase = channel_phrase # phrase embedding dimention\n",
    "        self.len_tags = len_tags # number of tags\n",
    "        self.len_chars = len_char #unique Chinese char\n",
    "        self.output_channel = 0\n",
    "        \n",
    "    def __placeholder(self):\n",
    "        self.gt = placeholder(dtype=tf.int32) #GT\n",
    "        self.f1_evaluate = Variable(dtype=tf.float32,initial_value=0.0, trainable=False) #best f1 score for evaluate data\n",
    "        self.f1_test = Variable(dtype=tf.float32,initial_value=0.0, trainable=False) #for test data\n",
    "        self.whole_steps = Variable(dtype=tf.int32,initial_value=0, trainable=False) #steps for training process\n",
    "        self.cn_char = placeholder(dtype=tf.int32) #input sentence\n",
    "        self.cn_phrase = placeholder(dtype=tf.int32) #nput Chinese phrase features\n",
    "        self.dropout = placeholder(dtype=tf.float32) #dropout\n",
    "        \n",
    "    def __parameters(self):\n",
    "        self.output_channel = 0\n",
    "        self.len_phrase = 4  #phrase features 0,1,2,3\n",
    "        length = reduce_sum(sign(abs(self.cn_char)), reduction_indices=1)\n",
    "        self.lengths = cast(length, tf.int32)\n",
    "        self.batch_size = shape(self.cn_char)[0] #batch_size\n",
    "        self.num_steps = shape(self.cn_char)[-1] #num_steps: total chars in each sentenc\n",
    "        self.layers = [1,1,2] #based on the paper, there will be 2 types of dilated rates\n",
    "        self.flag_drop = 0.5  #prevent from overfitting\n",
    "        self.channel_cnn = 100 # cnn kernels numbers \n",
    "        self.minor = -1000.0\n",
    "        self.model_training = True\n",
    "        if self.model_training == False:\n",
    "            self.flag_drop = 1.0 \n",
    "        self.filters = 3 \n",
    "        self.iterations = 4 #iterated \n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        self.channel_embedding = self.channel_char + self.channel_phrase  # char channels + phrase channels\n",
    "        \n",
    "    def __embedding(self): # initializing for two features\n",
    "        char_embeddings = get_variable('Embeddings_1',shape=[self.len_chars, self.channel_char],initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None),\n",
    "            dtype=tf.float32,trainable=True)\n",
    "        feature_1 = embed(char_embeddings, self.cn_char)\n",
    "        phrase_embeddings = get_variable('Embeddings_2',shape=[self.len_phrase, self.channel_phrase],initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None),\n",
    "            dtype=tf.float32,trainable=True)\n",
    "        feature_2 = embed(phrase_embeddings, self.cn_phrase)\n",
    "        self.embedding_cns = concat([feature_1,feature_2], axis=-1)\n",
    "        self.embedding_cns = dropout(self.embedding_cns, self.dropout)#apply dropout \n",
    "\n",
    "    def __dilated(self):# Dilated Convolutions Networks\n",
    "        nets_input = expand_dims(self.embedding_cns, 1)   \n",
    "        initialed_weight = get_variable(\"kernel\",shape=[1, self.filters, self.channel_embedding,self.channel_cnn],initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "        nets_input = conv2d(nets_input, initialed_weight, strides=[1, 1, 1, 1],  padding=\"SAME\",name=\"nets_input\")\n",
    "        output = []\n",
    "        channels = 0\n",
    "        for j in range(self.iterations):  \n",
    "            for i in range(len(self.layers)):# many dilated cnns can cover almost all the features\n",
    "                dilated_rate = self.layers[i]\n",
    "                if i == (len(self.layers) - 1):\n",
    "                    last_layer = True\n",
    "                else:\n",
    "                    last_layer = False\n",
    "                with variable_scope(\"DilatedConv%d\" % i, reuse=tf.AUTO_REUSE):\n",
    "                    weights = get_variable(name='Weights',shape=[1, self.filters, self.channel_cnn,self.channel_cnn], initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "                    biases = get_variable(name='Biases',shape=[self.channel_cnn]) \n",
    "                    c = atrous_conv2d(nets_input,weights, rate=dilated_rate, padding=\"SAME\") # dilated convolution\n",
    "                    c = bias_add(c, biases)\n",
    "                    c = relu(c)\n",
    "                    if last_layer:\n",
    "                        channels += self.channel_cnn\n",
    "                        output.append(c)\n",
    "                    nets_input = c\n",
    "        output_last = concat(values=output,axis=3) # merge the output of 4 last layers\n",
    "        output_last = dropout(output_last, self.flag_drop) #add dropout layer \n",
    "#             drop dimention: the dimention which contians only one data\n",
    "        output_last = squeeze(output_last, [1])\n",
    "        output_last = reshape(output_last, [-1, channels]) # final features done\n",
    "        self.output_channel = channels\n",
    "        weight = get_variable(\"Weight\", shape=[self.output_channel, self.len_tags],dtype=tf.float32, initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "        bias = get_variable(\"Bias\",  initializer=tf.constant(0.0001, shape=[self.len_tags]))\n",
    "#                    matmul(x, w) + b.\n",
    "        result = xw_plus_b(output_last, weight, bias)\n",
    "        self.result =  reshape(result, [-1, self.num_steps, self.len_tags])  # num_steps: total chars in each sentenc, len_tags: number of tags\n",
    "\n",
    "    def __loss(self):\n",
    "        # pad units \n",
    "        initial_units = concat([self.minor*tf.ones(shape=[self.batch_size, 1, self.len_tags]), tf.zeros(shape=[self.batch_size, 1, 1])], axis=-1)\n",
    "        pad_units = cast(self.minor*tf.ones([self.batch_size, self.num_steps, 1]), tf.float32)\n",
    "        temp = concat([self.result, pad_units], axis=-1)\n",
    "        temp = concat([initial_units, temp], axis=1)\n",
    "        gt = concat([cast(self.len_tags*tf.ones([self.batch_size, 1]), tf.int32), self.gt], axis=-1)\n",
    "        self.transition = get_variable(\"transit\",shape=[self.len_tags + 1, self.len_tags + 1],initializer=tf.random_normal_initializer(mean=0.0, stddev=0.05, seed=None))\n",
    "        likelihood, self.transition = crf_log_likelihood(inputs=temp,tag_indices=gt,transition_params=self.transition,sequence_lengths=self.lengths+1)\n",
    "        self.error = reduce_mean(likelihood*(-1))\n",
    "            \n",
    "    def __opt(self):\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learningR)\n",
    "        gradients = self.optimizer.compute_gradients(self.error) \n",
    "        limited_gradients = [[tf.clip_by_value(gra, -4, 4), va] for gra, va in gradients] # avoid gradient explosion\n",
    "        self.optimize = self.optimizer.apply_gradients(limited_gradients, self.whole_steps)\n",
    "            \n",
    "    def evaluate(self, sess, batch_data, id_to_tag):\n",
    "        transition = self.transition.eval()\n",
    "        report = []\n",
    "        for batch in batch_data:\n",
    "            cn_sentences = batch[0]\n",
    "            tags = batch[-1] #true tag\n",
    "            lengths, scores = self.each_step(sess, False, batch)\n",
    "            batch_paths = self.viterbi(scores, lengths, transition)\n",
    "            for i in range(len(cn_sentences)):\n",
    "                output = []\n",
    "                sentence = cn_sentences[i][:lengths[i]]\n",
    "                gt = convert_iobes_to_iob_tags([id_to_tag[int(x)] for x in tags[i][:lengths[i]]])\n",
    "                predict = convert_iobes_to_iob_tags([id_to_tag[int(x)] for x in batch_paths[i][:lengths[i]]])\n",
    "                for cn_char, gt, predict in zip(sentence, gt, predict):\n",
    "                    output.append(\" \".join([cn_char, gt, predict]))\n",
    "                report.append(output)\n",
    "        return report\n",
    "    \n",
    "    def viterbi(self, units, lengths, array): # viterbi Algorithm\n",
    "        paths = []\n",
    "        begin = np.asarray([[self.minor]*self.len_tags +[0]])\n",
    "        for val, temp_len in zip(units, lengths):\n",
    "            val = val[:temp_len]\n",
    "            pad = np.ones([temp_len, 1])*(self.minor)\n",
    "            units = np.concatenate([val, pad], axis=1)\n",
    "            units = np.concatenate([begin, units], axis=0)\n",
    "            path, _ = viterbi_decode(units, array)\n",
    "            paths.append(path[1:])\n",
    "        return paths\n",
    "    \n",
    "    def each_step(self, sess, training, batch):\n",
    "        _, cn_char, cn_phrase, tags = batch\n",
    "        temp_dict = {self.cn_char: np.asarray(cn_char),self.cn_phrase: np.asarray(cn_phrase), self.dropout: 1.0}\n",
    "        if training:\n",
    "            temp_dict[self.gt] = np.asarray(tags) #GT\n",
    "            temp_dict[self.dropout] = 0.5\n",
    "            whole_steps, error, _ = sess.run([self.whole_steps, self.error, self.optimize], temp_dict)\n",
    "            return whole_steps, error\n",
    "        else:\n",
    "            lengths, units = sess.run([self.lengths, self.result], temp_dict)\n",
    "            return lengths, units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert IOBES tags to IOB tags\n",
    "def convert_iobes_to_iob_tags(tags):\n",
    "    iob_tags = []\n",
    "    for index, tag in enumerate(tags):\n",
    "        t = tag.split('-')[0]        \n",
    "        if t == 'S': iob_tags.append(tag.replace('S-', 'B-'))\n",
    "        elif t == 'E': iob_tags.append(tag.replace('E-', 'I-'))\n",
    "        else: iob_tags.append(tag)          \n",
    "    return iob_tags\n",
    "\n",
    "\n",
    "# check if a phrase ended between the previous and current character\n",
    "def check_phrase_end_tag(prev_tag, cur_tag, prev_type, cur_type):\n",
    "    is_end = False\n",
    "\n",
    "    if prev_tag == 'E': is_end = True\n",
    "    if prev_tag == 'S': is_end = True\n",
    "        \n",
    "    if prev_tag == 'B' and (cur_tag == 'B' or cur_tag == 'S' or cur_tag == 'O'): \n",
    "        is_end = True  \n",
    "    if prev_tag == 'I' and (cur_tag == 'B' or cur_tag == 'S' or cur_tag == 'O'): \n",
    "        is_end = True\n",
    "        \n",
    "    if prev_tag != 'O' and prev_tag != '.' and prev_type != cur_type: \n",
    "        is_end = True\n",
    "\n",
    "    return is_end\n",
    "\n",
    "\n",
    "# check if a phrase started between the previous and current character\n",
    "def check_phrase_start_tag(prev_tag, cur_tag, prev_type, cur_type):\n",
    "    is_start = False\n",
    "\n",
    "    if cur_tag == 'B': chunk_start = True\n",
    "    if cur_tag == 'S': chunk_start = True\n",
    "    \n",
    "    if prev_tag == 'E' and (cur_tag == 'E' or cur_tag == 'I'): \n",
    "        is_start = True\n",
    "    if prev_tag == 'S' and (cur_tag == 'E' or cur_tag == 'I'): \n",
    "        is_start = True\n",
    "    if prev_tag == 'O' and (cur_tag == 'E' or cur_tag == 'I'): \n",
    "        is_start = True\n",
    "    \n",
    "    if cur_tag != 'O' and cur_tag != '.' and prev_type != cur_type: \n",
    "        is_start = True\n",
    "\n",
    "    return is_start\n",
    "\n",
    "\n",
    "# calculate the precision, recall and f-score\n",
    "def get_metrics(correct_count, predict_count, total_count):\n",
    "    TP = correct_count \n",
    "    FP = predict_count - correct_count\n",
    "    FN = total_count - correct_count\n",
    "\n",
    "    prec = 0 if (TP + FP == 0) else (1. * TP) / (TP + FP)  # precision\n",
    "    recall = 0 if (TP + FN == 0) else (1. * TP) / (TP + FN)  # recall \n",
    "    fscore = 0 if (prec + recall == 0) else (2 * prec * recall / (prec + recall)) # f-score\n",
    "\n",
    "    Metrics = namedtuple('Metrics', 'tp fp fn prec rec fscore')\n",
    "    return Metrics(TP, FP, FN, prec, recall, fscore)\n",
    "\n",
    "\n",
    "# parse tag into IOBES tags and enetity type\n",
    "def parse_tages(tag):\n",
    "    matched = re.match(r'^([^-]*)-(.*)$', tag)\n",
    "    return matched.groups() if matched else (tag, '')\n",
    "\n",
    "\n",
    "# print precsion, recall and f-score with format\n",
    "def print_report(parse_results, correct_entities, found_c_entities, found_g_entities):    \n",
    "    parsed_report = []        \n",
    "    metrics = get_metrics(parse_results[0], parse_results[3], parse_results[2])\n",
    "    \n",
    "    # all the found entities\n",
    "    cg_entities = list(found_c_entities) + list(found_g_entities)    \n",
    "    uniq_tags = set([e for e in cg_entities])  # unique tags\n",
    "\n",
    "    # get metrics includes precsion, recall and f-score\n",
    "    entity_metrics = {}\n",
    "    for tag in uniq_tags:\n",
    "        entity_metrics[tag] = get_metrics(correct_entities[tag], found_g_entities[tag], found_c_entities[tag])\n",
    "     \n",
    "    # print total tokens and phrases count\n",
    "    result_line = []\n",
    "    result_line.append('Total tokens is %d and total is phrases %d\\n' % (parse_results[4], parse_results[2]))\n",
    "    result_line.append('Found: %d phrases, correct: %d.\\n' % (parse_results[3], parse_results[0]))\n",
    "    parsed_report.append(\"\".join(result_line))\n",
    "\n",
    "    # formated result lines \n",
    "    if parse_results[4] > 0:\n",
    "        result_line = []\n",
    "        result_line.append(\"Accuracy:%6.2f%%, \" % (100. * parse_results[1] / parse_results[4]))\n",
    "        result_line.append(\"Precision:%6.2f%%, \" % (100.* metrics.prec))\n",
    "        result_line.append(\"Recall:%6.2f%%, \" % (100. * metrics.rec))\n",
    "        result_line.append(\"Fscore:%6.2f\\n\" % (100. * metrics.fscore))\n",
    "        parsed_report.append(\"\".join(result_line))\n",
    "\n",
    "    for index, metric in sorted(entity_metrics.items()):\n",
    "        result_line = []\n",
    "        result_line.append('%17s: ' % index)\n",
    "        result_line.append('Precision:%6.2f%%, ' % (100. * metric.prec))\n",
    "        result_line.append('Recall:%6.2f%%, ' % (100. * metric.rec))\n",
    "        result_line.append('Fscore:%6.2f\\n' % (100. * metric.fscore))\n",
    "        parsed_report.append(\"\".join(result_line))\n",
    "    \n",
    "    return parsed_report\n",
    "\n",
    "\n",
    "# parsed the reports\n",
    "def parse_report(file_name):\n",
    "\n",
    "    is_correct = False        # if current chunk is correct\n",
    "    \n",
    "    prev_ctag = 'O'           # previous correct tag\n",
    "    prev_ctag_entity = ''     # previous correct entity (LOC, ORG, PER)\n",
    "    prev_gtag = 'O'           # previous guessed tag\n",
    "    prev_gtag_entity = ''     # previous guessed entity (LOC, ORG, PER)\n",
    "    \n",
    "    # 0: correct entity number, 1: correct tag number, 2: number of phrases  \n",
    "    # 3: number of guessed phrases 4: number of tokens\n",
    "    results = [0, 0, 0, 0, 0]\n",
    "\n",
    "    correct_entities = defaultdict(int)\n",
    "    found_c_entities = defaultdict(int)\n",
    "    found_g_entities = defaultdict(int)\n",
    "\n",
    "    with codecs.open(file_name, \"r\") as file:   # read file\n",
    "        for line in file:\n",
    "            features = line.split() # features list per line  \n",
    "            if len(features) == 0: \n",
    "                features = ['-X-', 'O', 'O'] # for white space\n",
    "\n",
    "            cur_gtag, cur_gtag_entity = parse_tages(features.pop())  # parse predicted tag\n",
    "            cur_ctag, cur_ctag_entity = parse_tages(features.pop())  # parse correct tag\n",
    "            chinese_char = features.pop(0)  # chinese character \n",
    "\n",
    "            # check if the phrase is ended between the previous and current character\n",
    "            is_end_correct = check_phrase_end_tag(prev_ctag, cur_ctag, prev_ctag_entity, cur_ctag_entity)\n",
    "            is_end_guessed = check_phrase_end_tag(prev_gtag, cur_gtag, prev_gtag_entity, cur_gtag_entity)\n",
    "\n",
    "            # check if the phrase is started between the previous and current character\n",
    "            is_start_correct = check_phrase_start_tag(prev_ctag, cur_ctag, prev_ctag_entity, cur_ctag_entity)\n",
    "            is_start_guessed = check_phrase_start_tag(prev_gtag, cur_gtag, prev_gtag_entity, cur_gtag_entity)\n",
    "\n",
    "            if is_correct:\n",
    "                if (is_end_correct and is_end_guessed and prev_gtag_entity == prev_ctag_entity):\n",
    "                    is_correct = False\n",
    "                    results[0] += 1\n",
    "                    correct_entities[prev_ctag_entity] += 1\n",
    "\n",
    "                elif (is_end_correct != is_end_guessed or cur_gtag_entity != cur_ctag_entity):\n",
    "                    is_correct = False\n",
    "\n",
    "            if is_start_correct and is_start_guessed and cur_gtag_entity == cur_ctag_entity:\n",
    "                is_correct = True\n",
    "\n",
    "            if is_start_correct:\n",
    "                results[2] += 1\n",
    "                found_c_entities[cur_ctag_entity] += 1\n",
    "            if is_start_guessed:\n",
    "                results[3] += 1\n",
    "                found_g_entities[cur_gtag_entity] += 1\n",
    "            \n",
    "            if chinese_char != '-X-':  # not empty character\n",
    "                if cur_ctag == cur_gtag and cur_gtag_entity == cur_ctag_entity:\n",
    "                    results[1] += 1\n",
    "                results[4] += 1\n",
    "            \n",
    "            # get previous tags \n",
    "            prev_gtag = cur_gtag\n",
    "            prev_ctag = cur_ctag\n",
    "            prev_gtag_entity = cur_gtag_entity\n",
    "            prev_ctag_entity = cur_ctag_entity\n",
    "\n",
    "        if is_correct:\n",
    "            results[0] += 1\n",
    "            correct_entities[prev_ctag_entity] += 1\n",
    "\n",
    "    # get parsed report,includes accuracy, precsion, recall and f-score\n",
    "    parsed_report = print_report(results, correct_entities, found_c_entities, found_g_entities)   \n",
    "    return parsed_report\n",
    "\n",
    "# write predict result and parse report\n",
    "def evaluate_report(train_results, file_path):\n",
    "    # file name\n",
    "    file_name = os.path.join(file_path, \"predict_result.txt\") \n",
    "    # write file\n",
    "    with open(file_name, \"w\") as outfile:\n",
    "        write_context = []\n",
    "        # write line by line\n",
    "        for chunk in train_results:\n",
    "            for line in chunk:\n",
    "                write_context.append(line + \"\\n\")\n",
    "            write_context.append(\"\\n\")\n",
    "        outfile.writelines(write_context)\n",
    "    # parse report\n",
    "    result_lines = parse_report(file_name)\n",
    "    return result_lines\n",
    "\n",
    "# evalute data\n",
    "def evaluate(tf_sess, model, data, id_to_tag):\n",
    "    predict_results = model.evaluate(tf_sess, data, id_to_tag)\n",
    "    parsed_lines = evaluate_report(predict_results, folder_patch)\n",
    "    for line in parsed_lines:\n",
    "        print(line)\n",
    "    f1 = float(parsed_lines[1].strip().split()[-1])\n",
    "    f1_test = model.f1_evaluate.eval()\n",
    "    if f1 > f1_test:\n",
    "        tf.assign(model.f1_evaluate, f1).eval()\n",
    "        print(\"Best f1 score: {:>.3f}\".format(f1))\n",
    "    return f1 > f1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larry5/.conda/envs/2020/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Num:1 Step Num:100 of 974, Model Loss:11.401937\n",
      "Iteration Num:1 Step Num:200 of 974, Model Loss: 8.194280\n",
      "Iteration Num:1 Step Num:300 of 974, Model Loss: 7.719868\n",
      "Iteration Num:1 Step Num:400 of 974, Model Loss: 7.033868\n",
      "Iteration Num:1 Step Num:500 of 974, Model Loss: 7.407226\n",
      "Iteration Num:1 Step Num:600 of 974, Model Loss: 7.303170\n",
      "Iteration Num:1 Step Num:700 of 974, Model Loss: 7.193491\n",
      "Iteration Num:1 Step Num:800 of 974, Model Loss: 7.431215\n",
      "Iteration Num:1 Step Num:900 of 974, Model Loss: 7.760583\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 1932 phrases, correct: 1249.\n",
      "\n",
      "Accuracy: 95.45%, Precision: 64.65%, Recall: 57.85%, Fscore: 61.06\n",
      "\n",
      "              LOC: Precision: 54.40%, Recall: 72.61%, Fscore: 62.20\n",
      "\n",
      "              ORG: Precision: 78.77%, Recall: 50.41%, Fscore: 61.48\n",
      "\n",
      "              PER: Precision: 87.22%, Recall: 43.47%, Fscore: 58.02\n",
      "\n",
      "Best f1 score: 61.060\n",
      "Iteration Num:2 Step Num:26 of 974, Model Loss:10.478935\n",
      "Iteration Num:2 Step Num:126 of 974, Model Loss: 2.079245\n",
      "Iteration Num:2 Step Num:226 of 974, Model Loss: 2.216519\n",
      "Iteration Num:2 Step Num:326 of 974, Model Loss: 2.465111\n",
      "Iteration Num:2 Step Num:426 of 974, Model Loss: 2.701747\n",
      "Iteration Num:2 Step Num:526 of 974, Model Loss: 2.893805\n",
      "Iteration Num:2 Step Num:626 of 974, Model Loss: 3.262674\n",
      "Iteration Num:2 Step Num:726 of 974, Model Loss: 3.733427\n",
      "Iteration Num:2 Step Num:826 of 974, Model Loss: 3.879964\n",
      "Iteration Num:2 Step Num:926 of 974, Model Loss: 4.844354\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 2045 phrases, correct: 1592.\n",
      "\n",
      "Accuracy: 97.08%, Precision: 77.85%, Recall: 73.74%, Fscore: 75.74\n",
      "\n",
      "              LOC: Precision: 70.82%, Recall: 80.98%, Fscore: 75.56\n",
      "\n",
      "              ORG: Precision: 80.90%, Recall: 73.49%, Fscore: 77.02\n",
      "\n",
      "              PER: Precision: 90.87%, Recall: 63.38%, Fscore: 74.67\n",
      "\n",
      "Best f1 score: 75.740\n",
      "Iteration Num:3 Step Num:52 of 974, Model Loss: 4.758164\n",
      "Iteration Num:3 Step Num:152 of 974, Model Loss: 1.241736\n",
      "Iteration Num:3 Step Num:252 of 974, Model Loss: 1.479801\n",
      "Iteration Num:3 Step Num:352 of 974, Model Loss: 1.565775\n",
      "Iteration Num:3 Step Num:452 of 974, Model Loss: 1.915848\n",
      "Iteration Num:3 Step Num:552 of 974, Model Loss: 2.033610\n",
      "Iteration Num:3 Step Num:652 of 974, Model Loss: 2.211455\n",
      "Iteration Num:3 Step Num:752 of 974, Model Loss: 2.613904\n",
      "Iteration Num:3 Step Num:852 of 974, Model Loss: 2.954391\n",
      "Iteration Num:3 Step Num:952 of 974, Model Loss: 3.857461\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 2173 phrases, correct: 1761.\n",
      "\n",
      "Accuracy: 97.36%, Precision: 81.04%, Recall: 81.57%, Fscore: 81.30\n",
      "\n",
      "              LOC: Precision: 81.62%, Recall: 83.04%, Fscore: 82.33\n",
      "\n",
      "              ORG: Precision: 74.64%, Recall: 77.09%, Fscore: 75.85\n",
      "\n",
      "              PER: Precision: 86.80%, Recall: 83.76%, Fscore: 85.25\n",
      "\n",
      "Best f1 score: 81.300\n",
      "Iteration Num:4 Step Num:78 of 974, Model Loss: 2.452975\n",
      "Iteration Num:4 Step Num:178 of 974, Model Loss: 0.882493\n",
      "Iteration Num:4 Step Num:278 of 974, Model Loss: 1.182438\n",
      "Iteration Num:4 Step Num:378 of 974, Model Loss: 1.226533\n",
      "Iteration Num:4 Step Num:478 of 974, Model Loss: 1.416960\n",
      "Iteration Num:4 Step Num:578 of 974, Model Loss: 1.616516\n",
      "Iteration Num:4 Step Num:678 of 974, Model Loss: 1.777136\n",
      "Iteration Num:4 Step Num:778 of 974, Model Loss: 2.034533\n",
      "Iteration Num:4 Step Num:878 of 974, Model Loss: 2.463821\n",
      "Total tokens is 66636 and total is phrases 2159\n",
      "Found: 2146 phrases, correct: 1800.\n",
      "\n",
      "Accuracy: 97.65%, Precision: 83.88%, Recall: 83.37%, Fscore: 83.62\n",
      "\n",
      "              LOC: Precision: 85.51%, Recall: 84.67%, Fscore: 85.09\n",
      "\n",
      "              ORG: Precision: 75.89%, Recall: 79.87%, Fscore: 77.83\n",
      "\n",
      "              PER: Precision: 90.03%, Recall: 84.87%, Fscore: 87.38\n",
      "\n",
      "Best f1 score: 83.620\n"
     ]
    }
   ],
   "source": [
    "steps_check = 100\n",
    "mode = 'training' # 'training' or 'testing'\n",
    "\n",
    "\n",
    "if mode == 'training':\n",
    "    with tf.Session() as sess:\n",
    "        model = Model(mode)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        loss_holder = []\n",
    "        for i in range(4):\n",
    "            for batch in train_batch_data:\n",
    "                step, temp_loss = model.each_step(sess, True, batch)\n",
    "                loss_holder.append(temp_loss)\n",
    "                if step % steps_check == 0:\n",
    "                    iteration = step // epoch_iterations + 1\n",
    "                    print(\"Iteration Num:{} Step Num:{} of {}, \"\"Model Loss:{:>9.6f}\".format(\n",
    "                        iteration, step % epoch_iterations, epoch_iterations, np.mean(loss_holder)))\n",
    "                    loss_holder = []\n",
    "\n",
    "            evaluate(sess, model, dev_batch_data, id_to_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-2020]",
   "language": "python",
   "name": "conda-env-.conda-2020-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
