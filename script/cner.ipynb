{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /scratch/local/jieba.cache\n",
      "Loading model cost 0.673 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import pickle\n",
    "import math\n",
    "import jieba\n",
    "jieba.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing\n",
    "\n",
    "    1) Load data from file. \n",
    "    2) Convert IOB tagging into IOBES tagging. \n",
    "    3) Split data into training data, testing data and evaluation data.\n",
    "    4) Creating item-to-sequence and sequence-to-item dictionaries.\n",
    "    5) Convert chinese characters and tags into sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['因', 'O'], ['此', 'O'], ['，', 'O'], ['这', 'O'], ['次', 'O'], ['政', 'O'], ['府', 'O'], ['危', 'O'], ['机', 'O'], ['终', 'O'], ['于', 'O'], ['得', 'O'], ['到', 'O'], ['化', 'O'], ['解', 'O'], ['，', 'O'], ['对', 'O'], ['俄', 'B-LOC'], ['罗', 'I-LOC'], ['斯', 'I-LOC'], ['来', 'O'], ['说', 'O'], ['是', 'O'], ['值', 'O'], ['得', 'O'], ['庆', 'O'], ['幸', 'O'], ['的', 'O'], ['。', 'O']]\n",
      "[['因', 'O'], ['此', 'O'], ['，', 'O'], ['这', 'O'], ['次', 'O'], ['政', 'O'], ['府', 'O'], ['危', 'O'], ['机', 'O'], ['终', 'O'], ['于', 'O'], ['得', 'O'], ['到', 'O'], ['化', 'O'], ['解', 'O'], ['，', 'O'], ['对', 'O'], ['俄', 'B-LOC'], ['罗', 'I-LOC'], ['斯', 'E-LOC'], ['来', 'O'], ['说', 'O'], ['是', 'O'], ['值', 'O'], ['得', 'O'], ['庆', 'O'], ['幸', 'O'], ['的', 'O'], ['。', 'O']]\n",
      "The number of sentences of trainning data is 19472\n",
      "The number of sentences of testing data is 5007\n",
      "The number of sentences of development data is 3339\n",
      "The number of unique Chinese characters is: 4277\n",
      "The number of unique tag characters is: 13\n",
      "[['因', '此', '，', '这', '次', '政', '府', '危', '机', '终', '于', '得', '到', '化', '解', '，', '对', '俄', '罗', '斯', '来', '说', '是', '值', '得', '庆', '幸', '的', '。'], [209, 187, 2, 22, 133, 63, 247, 642, 117, 619, 57, 107, 42, 103, 229, 2, 38, 663, 415, 206, 43, 87, 11, 552, 107, 782, 1130, 3, 4], [1, 3, 0, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 0, 0, 1, 2, 3, 1, 3, 0, 1, 3, 1, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# read sentences from file\n",
    "def load_data(file_path):\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for line in codecs.open(file_path, 'r', 'utf8'):\n",
    "        line = line.rstrip() # Remove any white spaces at the end of the string\n",
    "        if not line:\n",
    "            if len(sent) > 0: # a line with \"\\n\" is used for spliting sentences\n",
    "                sentences.append(sent)\n",
    "                sent = []\n",
    "        else:\n",
    "            word_tag = line.split() # split word and tag\n",
    "            if len(word_tag) == 2:\n",
    "                sent.append(word_tag)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# convert IOB tags to IOBES tags\n",
    "def convert_to_iobes_tags(sentences):   \n",
    "    for index, sent in enumerate(sentences):\n",
    "        iob_tags = [word_tag[-1] for word_tag in sent] # obtain iob tags of a sentence\n",
    "        iobes_tags = [] # iobes tags\n",
    "        for i, tag in enumerate(iob_tags): \n",
    "            if tag == 'O': # O tag is unchanged\n",
    "                iobes_tags.append(tag)\n",
    "            elif tag.split('-')[0] == 'B':  # B to S if an entity only includes a single word\n",
    "                if i + 1 < len(iob_tags) and iob_tags[i + 1].split('-')[0] == 'I':\n",
    "                    iobes_tags.append(tag)\n",
    "                else:\n",
    "                    iobes_tags.append(tag.replace('B-', 'S-'))\n",
    "            elif tag.split('-')[0] == 'I':  # E is used for the last item of an entity (words > 2)\n",
    "                if i + 1 < len(iob_tags) and iob_tags[i + 1].split('-')[0] == 'I':\n",
    "                    iobes_tags.append(tag)\n",
    "                else:\n",
    "                    iobes_tags.append(tag.replace('I-', 'E-'))\n",
    "            else:\n",
    "                print('ERROR: INVALID IOB TAGGING!')  \n",
    "        for word, iobes_tag in zip(sent, iobes_tags): # replace tags\n",
    "            word[-1] = iobes_tag\n",
    "            \n",
    "            \n",
    "# split sentences into train, test, dev\n",
    "def split_data(sentences):\n",
    "    train_div = int(len(sentences) * 0.7);  # train set divide number\n",
    "    train_sentences = sentences[:train_div]\n",
    "    remaining_sentences = sentences[train_div:] \n",
    "    test_div = int(len(remaining_sentences)* 0.6)  # test set divide number\n",
    "    test_sentences = remaining_sentences[:test_div]\n",
    "    dev_sentences = remaining_sentences[test_div:]\n",
    "    return train_sentences, test_sentences, dev_sentences\n",
    "\n",
    "\n",
    "# creating dictionaries from unique chinese characters to unique id\n",
    "def create_char_id_convert_dict(sentences):\n",
    "    char_dict = {} # a dictionary of the frequency of unique chinese characters\n",
    "    chinese_chars = [[word[0] for word in sent] for sent in sentences] # get words from tupe word_tag\n",
    "    for chars in chinese_chars: # get frequency of unique chinese characters\n",
    "        for char in chars:\n",
    "            if char not in char_dict:\n",
    "                char_dict[char] = 1\n",
    "            else:\n",
    "                char_dict[char] += 1\n",
    "    char_dict[\"<PAD>\"] = 99999 # spacial word for padding, and intial a largest frequency\n",
    "    char_dict['<UNK>'] = 99998 # spacial word for unkonwn, and intial a second largest frequency\n",
    "    # sort characters by frequency (highest to samllest)\n",
    "    sorted_char_dict = sorted(char_dict.items(), key=lambda x: (-x[1], x[0])) \n",
    "    # create two dictionaries: find char by id, or find id by char\n",
    "    id_to_char = {index: value[0] for index, value in enumerate(sorted_char_dict)} \n",
    "    char_to_id = {value: key for key, value in id_to_char.items()}\n",
    "    return id_to_char, char_to_id\n",
    "\n",
    "\n",
    "# creating dictionaries from unique tag to unique id\n",
    "def create_tag_id_convert_dict(sentences):\n",
    "    tag_dict = {} # a dictionary of the frequency of tags\n",
    "    tags = [[word[1] for word in sent] for sent in sentences]\n",
    "    for tag in tags: # get frequency of unique chinese characters\n",
    "        for t in tag:\n",
    "            if t not in tag_dict:\n",
    "                tag_dict[t] = 1\n",
    "            else:\n",
    "                tag_dict[t] += 1\n",
    "    # sort characters by frequency (highest to samllest)\n",
    "    sorted_tag_dict = sorted(tag_dict.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # create two dictionaries: find tag by id, or find id by tag\n",
    "    id_to_tag = {index: value[0] for index, value in enumerate(sorted_tag_dict)} \n",
    "    tag_to_id = {value: key for key, value in id_to_tag.items()}\n",
    "    return id_to_tag, tag_to_id\n",
    "\n",
    "\n",
    "# Generated formated data for training\n",
    "def get_formated_data(sentences, char_to_id, tag_to_id):\n",
    "    formated_data = []\n",
    "    for sent in sentences:\n",
    "        sent_chars = [word[0] for word in sent] # get chinese chars\n",
    "        # convert chars to id\n",
    "        chars_id = [char_to_id[char if char in char_to_id else '<UNK>'] for char in sent_chars] \n",
    "        joined_sent = \"\".join(sent_chars) # joined all the chars into a sentence\n",
    "\n",
    "        # Tokenize sent with Jieba to get chinese phrase feature (the start, inside, and end of a phrase)\n",
    "        phrase_feature = []\n",
    "        for token in jieba.cut(joined_sent):\n",
    "            if len(token) == 1: # phrase_feature is 0 if a phase only has one Chinese character\n",
    "                phrase_feature.append(0)\n",
    "            else:\n",
    "                phrase_list = [2] * len(token) # phrase_feature of middle characters in a phase is 2\n",
    "                phrase_list[0] = 1 # phrase_feature of start character in a phase is 1\n",
    "                phrase_list[-1] = 3 # phrase_feature of end character in a phase is 3\n",
    "                phrase_feature.extend(phrase_list)\n",
    "\n",
    "        tags_id = [tag_to_id[word[-1]] for word in sent] # convert tags to id\n",
    "        formated_data.append([sent_chars, chars_id, phrase_feature, tags_id]) # formated data\n",
    "    return formated_data\n",
    "\n",
    "\n",
    "\n",
    "# data processing\n",
    "folder_patch = \"./dataset/\"  # dataset folder\n",
    "data_path = folder_patch + \"data.txt\" # data path\n",
    "\n",
    "sentences = load_data(data_path) # load data\n",
    "print(sentences[0]) \n",
    "\n",
    "convert_to_iobes_tags(sentences) # convert to iobes tags\n",
    "print(sentences[0]) \n",
    "\n",
    "train_sentences, test_sentences, dev_sentences = split_data(sentences) # split data \n",
    "print(\"The number of sentences of trainning data is\", len(train_sentences))\n",
    "print(\"The number of sentences of testing data is\", len(test_sentences))\n",
    "print(\"The number of sentences of development data is\", len(dev_sentences))\n",
    "\n",
    "# creates chinese characters and senquence convertion dictionaries\n",
    "id_to_char, char_to_id = create_char_id_convert_dict(train_sentences) \n",
    "# creates tags and senquence convertion dictionaries\n",
    "id_to_tag, tag_to_id = create_tag_id_convert_dict(train_sentences)\n",
    "print(\"The number of unique Chinese characters is:\", len(char_to_id))\n",
    "print(\"The number of unique tag characters is:\", len(tag_to_id))\n",
    "\n",
    "train_data = get_formated_data(train_sentences, char_to_id, tag_to_id) # formated training data\n",
    "test_data = get_formated_data(test_sentences, char_to_id, tag_to_id) # formated testing data\n",
    "dev_data = get_formated_data(dev_sentences, char_to_id, tag_to_id) # formated evaluation data\n",
    "print(train_data[0])\n",
    "\n",
    "with open(folder_patch + 'dict.pkl', \"wb\") as out_file:\n",
    "    pickle.dump([char_to_id, id_to_char, tag_to_id, id_to_tag], out_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
